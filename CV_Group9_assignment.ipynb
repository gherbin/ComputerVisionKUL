{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CV_Group9_assignment.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gherbin/ComputerVisionKUL/blob/master/CV_Group9_assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SnkjeOT3GuBy",
        "colab_type": "text"
      },
      "source": [
        "Import all packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KH_DPxxjTeoI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import cv2\n",
        "import tarfile\n",
        "import zipfile\n",
        "import shutil\n",
        "\n",
        "import numpy as np\n",
        "import random\n",
        "import logging\n",
        "\n",
        "from urllib import request\n",
        "from socket import timeout\n",
        "from urllib.error import HTTPError, URLError\n",
        "\n",
        "from google.colab import drive\n",
        "from google.colab.patches import cv2_imshow\n",
        "from distutils.dir_util import copy_tree\n",
        "\n",
        "# added for HOG\n",
        "from skimage.feature import hog\n",
        "from skimage import exposure\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "logging.basicConfig(level=logging.DEBUG)\n",
        "\n",
        "mpl_logger = logging.getLogger(\"matplotlib\")\n",
        "mpl_logger.setLevel(logging.WARNING)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "83XvyfDBSLQX",
        "colab_type": "text"
      },
      "source": [
        "Several utils functions\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "05JhwSLCSPyI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def pretty_return_dict_size(my_dict):\n",
        "    output_list = [\"\\n\"]\n",
        "    for k in my_dict.keys():\n",
        "        output_list.append(str(k))\n",
        "        output_list.append(\":\")\n",
        "        output_list.append(str(len(my_dict[k])))\n",
        "        output_list.append(\"\\n\")\n",
        "    return ''.join(output_list)\n",
        "\n",
        "def show_images_from_dict(my_dict, show_index = False):\n",
        "    for k in my_dict.keys():\n",
        "        logging.debug(\"@------------------- Images of \" + str(k) + \" -------------------@\")\n",
        "        index = 0\n",
        "        for img in my_dict[k]:\n",
        "            if show_index:\n",
        "                logging.debug(\"Image index: \" + index)\n",
        "                index+=1\n",
        "            cv2_imshow(img)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BGBkRUir9f7g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "base_path = \"/content/sample_data/CV__Group_assignment\"\n",
        "\n",
        "if not os.path.isdir(base_path):\n",
        "  os.makedirs(base_path)\n",
        "\n",
        "vgg_face_dataset_url = \"http://www.robots.ox.ac.uk/~vgg/data/vgg_face/vgg_face_dataset.tar.gz\"\n",
        "\n",
        "with request.urlopen(vgg_face_dataset_url) as r, open(os.path.join(base_path, \"vgg_face_dataset.tar.gz\"), 'wb') as f:\n",
        "  f.write(r.read())\n",
        "\n",
        "with tarfile.open(os.path.join(base_path, \"vgg_face_dataset.tar.gz\")) as f:\n",
        "  f.extractall(os.path.join(base_path))\n",
        "\n",
        "trained_haarcascade_url = \"https://raw.githubusercontent.com/opencv/opencv/master/data/haarcascades/haarcascade_frontalface_default.xml\"\n",
        "\n",
        "with request.urlopen(trained_haarcascade_url) as r, open(os.path.join(base_path, \"haarcascade_frontalface_default.xml\"), 'wb') as f:\n",
        "    f.write(r.read())\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MOH3DL2jKk3U",
        "colab_type": "text"
      },
      "source": [
        "# New Section"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lQPukQmEpoRC",
        "colab_type": "text"
      },
      "source": [
        "How to generate datasets\n",
        "A = Emma Stone\n",
        "\n",
        "1.   A --> Emma Stone\n",
        "2.   B --> Bradley Cooper\n",
        "3.   C --> Jane Levy\n",
        "4.   D --> Marc Blucas\n",
        "\n",
        "\n",
        "#Idea to get the images dataset\n",
        "## For A and B\n",
        "1. define a seed for A, and a seed for B\n",
        "2. generate a number based on this seed\n",
        "3. using this number, select 50 images from the list of 1000 images provided in the database\n",
        "4. Select 30 out of the 50 images obtained. This constitutes the original dataset (Training and Test) for A and B.\n",
        "\n",
        "## For C and D\n",
        "1. define a seed for C, and a seed for D\n",
        "2. generator a number based on this seed\n",
        "3. using this number, select 20 images from the lsit of 1000 images provided in the database\n",
        "4. Select 10 out of the 20 images obtained. This constutes the original dataset for C and D.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YsyV36nO-SxH",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cFh6oMU6phLs",
        "colab_type": "text"
      },
      "source": [
        "Start from clean sheet\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NAMpaW-Ym0oG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "path_datasets = r\"/content/datasets/\"\n",
        "path_discard = r\"/content/discard/\"\n",
        "path_database = r\"/content/DATABASE/\"\n",
        "\n",
        "try:\n",
        "    shutil.rmtree(path_database)\n",
        "    shutil.rmtree(path_datasets)\n",
        "    shutil.rmtree(path_discard)\n",
        "except:\n",
        "    pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I4Rj5OSS3fUx",
        "colab_type": "text"
      },
      "source": [
        "Create required folders"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hQw39iLK36m1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "file_info = path_database+ r\"info_retrieved.txt\"\n",
        "\n",
        "try: \n",
        "    os.mkdir(path_database)\n",
        "    os.mkdir(path_datasets) \n",
        "    os.mkdir(path_discard)\n",
        "except OSError as error: \n",
        "    logging.error(error) \n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JuEfFZE42YeO",
        "colab_type": "text"
      },
      "source": [
        "Instead of randomly download from web, take images from \"clean\" and controled repository containing the 180 images downloaded once and for all. \n",
        "\n",
        "For that purpose, we use a github public repository created specifically for that matter. The address of this public repository is hardcoded."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TgaFPa0C2YJH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# drive.mount('/content/drive')\n",
        "load_from_local_drive = True\n",
        "if load_from_local_drive:\n",
        "    \n",
        "    !wget https://raw.githubusercontent.com/gherbin/cv_group9_database_replica/master/DATABASE-20200318T142918Z-001.zip\n",
        "\n",
        "    with zipfile.ZipFile(\"DATABASE-20200318T142918Z-001.zip\", 'r') as zip_ref:\n",
        "        zip_ref.extractall()\n",
        "    !rm -r \"DATABASE-20200318T142918Z-001.zip\"\n",
        "\n",
        "path_, dirs_, files = next(os.walk(path_database))\n",
        "if len(files) == 180+1:\n",
        "    logging.info(\"Successful database retrieval\")\n",
        "elif load_from_local_drive:\n",
        "    logging.error(\"Most Likely problem with database retrieval, number of files = \" + str(len(files)))\n",
        "else:\n",
        "    logging.info(\"No database images retrieved yet\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M09QQcnf38xN",
        "colab_type": "text"
      },
      "source": [
        "XXXXXXXXXXXXXXXXXXXXX\n",
        "XXXXXXXXXXXXXXXXXXXXX\n",
        "XXXXXXXXXXXXXXXXXXXXX"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UEGMZ43b4iaa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "personA = \"Emma_Stone.txt\"\n",
        "personC = \"Jane_Levy.txt\"\n",
        "personB = \"Bradley_Cooper.txt\"\n",
        "personD = \"Marc_Blucas.txt\"\n",
        "persons = [personA, personB, personC, personD]\n",
        "datasets_dict = {}\n",
        "images_size = {}\n",
        "images_size[personA] = 60\n",
        "images_size[personB] = 60\n",
        "images_size[personC] = 30\n",
        "images_size[personD] = 30\n",
        "\n",
        "total_images_size = sum(images_size.values())\n",
        "\n",
        "# Dictionary containing the ids of the pictures downloaded from internet\n",
        "vgg_ids = {}\n",
        "for p in persons:\n",
        "    vgg_ids[p] = []\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8qdfeFWLAL6C",
        "colab_type": "text"
      },
      "source": [
        "Populating DB - Run only if database folder is empty"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I5VsGBAJ3inK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "confirmation = False\n",
        "if confirmation:\n",
        "    try:\n",
        "        shutil.rmtree(path_database)\n",
        "    except:\n",
        "        pass \n",
        "    try:\n",
        "        os.mkdir(path_database)\n",
        "    except:\n",
        "        pass \n",
        "\n",
        "    fo = open(file_info, \"w+\")\n",
        "\n",
        "    # images = {}\n",
        "    # images_nominal_indices = {}\n",
        "    for person in persons:\n",
        "        logging.debug(\"Taking care of: \" + str(person))\n",
        "        random.seed(person)\n",
        "        # print(hash(person))\n",
        "        images_ = []\n",
        "        # images_nominal_indices_ = []\n",
        "        prev_index = []\n",
        "\n",
        "\n",
        "        with open(os.path.join(base_path, \"vgg_face_dataset\", \"files\", person), 'r') as f:\n",
        "            lines = f.readlines()       \n",
        "        \n",
        "\n",
        "        while len(images_) < images_size[person]:\n",
        "            index = random.randrange(0, 1000)\n",
        "            logging.debug(\"Index = \" + str(index))\n",
        "            if index in prev_index:\n",
        "                logging.debug(\"Index = \" + str(index) + \" => already there\")\n",
        "                continue\n",
        "            else:\n",
        "                prev_index.append(index)\n",
        "                line = lines[index]\n",
        "                # only curated data\n",
        "                if int(line.split(\" \")[8]) == 1:\n",
        "                    url = line[line.find(\"http://\"): line.find(\".jpg\") + 4]\n",
        "                    logging.debug(\"URL > \\\"\" + str(url))\n",
        "                    try:\n",
        "                        res = request.urlopen(url, timeout = 1)\n",
        "                        img = np.asarray(bytearray(res.read()), dtype=\"uint8\")\n",
        "                        img = cv2.imdecode(img, cv2.IMREAD_COLOR)\n",
        "\n",
        "                        h, w = img.shape[:2]\n",
        "                        cv2_imshow(cv2.resize(img, (w//4, h//4)))\n",
        "                        # images_nominal_indices_.append(index)\n",
        "\n",
        "                        filename = path_database +  str(index) + \"_\" + str(person.split(\".\")[0]) + \".jpg\"\n",
        "\n",
        "                        value = cv2.imwrite(filename, img) \n",
        "                        # logging.debug(\"saved in DB: \" + str(filename))\n",
        "                        images_.append(img)\n",
        "                        fo.write(line)\n",
        "                    except ValueError as e:\n",
        "                            logging.error(\"Value Error >\" + str(e))\n",
        "                    except (HTTPError, URLError) as e:\n",
        "                            logging.error('ERROR RETRIEVING URL >' + str(e))\n",
        "                    except timeout:\n",
        "                            logging.error('socket timed out - URL %s', str(url))\n",
        "                    except cv2.error as e: \n",
        "                            logging.error(\"ERROR WRITING FILE IN DB  >\" + str(e))\n",
        "                    except:\n",
        "                        logging.error(\"Weird exception : \" + str(line))\n",
        "                else:\n",
        "                    logging.debug(\"File not curated => rejected (id = \" + str(index) + \" )\")    \n",
        "                \n",
        "                # images[person] = images_\n",
        "                # images_nominal_indices[person] = images_nominal_indices_\n",
        "\n",
        "    fo.close()\n",
        "else:\n",
        "    logging.warning(\"If you really want to erase and renew the database, please change first the \\\"confirmation\\\" boolean variable, at the beginning of this cell\")\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P3eNIMAdCLVu",
        "colab_type": "text"
      },
      "source": [
        "From local database (not the web, not the drive), building lists of images for all persons"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AdFihucjCO6e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open(file_info, 'r') as f: \n",
        "    lines = f.readlines()\n",
        "\n",
        "assert len(lines)==total_images_size, \"amount of lines in file incompatible\" \n",
        "\n",
        "images = {}\n",
        "\n",
        "for p in persons:\n",
        "    images[p] = []\n",
        "\n",
        "\n",
        "images_index = {}\n",
        "for running_index in range(len(lines)):\n",
        "    if running_index in range(0,images_size[personA]):\n",
        "        p = personA\n",
        "    elif running_index in range(images_size[personA],images_size[personA]+images_size[personB]):\n",
        "        p = personB\n",
        "    elif running_index in range(images_size[personA]+images_size[personB],images_size[personA]+images_size[personB]+images_size[personC]):\n",
        "        p = personC\n",
        "    elif running_index in range(images_size[personA]+images_size[personB]+images_size[personC],total_images_size):\n",
        "        p = personD\n",
        "    ind = str(int(lines[running_index].split(\" \")[0])-1)\n",
        "    vgg_ids[p].append(ind)\n",
        "    filename = ind + \"_\" + str(p.split(\".\")[0]) + \".jpg\"\n",
        "    images[p].append(cv2.imread(path_database+filename, cv2.IMREAD_COLOR))\n",
        "    \n",
        "# print(len(images.keys()))\n",
        "# print(len(images[personA]))\n",
        "# print(len(images[personB]))\n",
        "# print(len(images[personC]))\n",
        "# print(len(images[personD]))\n",
        "\n",
        "# for person in persons:\n",
        "#     counter = 0\n",
        "#     for img in images[person]:\n",
        "#         logging.debug(\"------------------------------------------------------\")\n",
        "#         logging.debug(\"Photo ID = \" + str(counter))\n",
        "#         cv2_imshow(img)\n",
        "#         counter += 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4d-u5s3sHWp-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9v4tuRVO67u1",
        "colab_type": "text"
      },
      "source": [
        "To remove:\n",
        "From DB loaded from the internet, several images are to be removed. The main reasons are:\n",
        "\n",
        "\n",
        "*   Too much make up\n",
        "*   too different hair with usual representation\n",
        "*   really poor image quality\n",
        "*   relevance and error in dataset\n",
        "*   same image as already in dataset\n",
        "*   cropped image\n",
        "\n",
        "considering the tight selection of images to train our model, and the relative global amount of image candidates, it is acceptable to first sort the images according to visual insights. \n",
        "\n",
        "From the initial retrieved images, we then remove the undesired images, that we copy in discard images folder, for tracking purposes. We may want to use them later on to assess the training, for an academical purpose.\n",
        "\n",
        "From the remaing imaging, we can apply the same strategy of selecting the required datasets, that we finally load in separate folders, and save in drive.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZPKRTSB5r8pg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Dictionary of the size required (see section 3)\n",
        "datasets_size = {}\n",
        "datasets_size[personA] = 30\n",
        "datasets_size[personB] = 30\n",
        "datasets_size[personC] = 10\n",
        "datasets_size[personD] = 10\n",
        "\n",
        "\n",
        "# manually remove images that are not relevant or considered not good enough to be part of the dataset\n",
        "to_remove = {}\n",
        "to_remove[personA] = [0,1,4,8,12,13,16,23,28,34,36,42,44,47,48,49,54]\n",
        "to_remove[personB] = [4,7,11,12,13,16,21,22,23,25,26,27,32,36,39,41,46,49,53,55,58]\n",
        "to_remove[personC] = [0,1,4,6,7,11,14,16,17,19,20]\n",
        "to_remove[personD] = [0,3,5,6,8,10,15,16,17,24]\n",
        "\n",
        "# goal is to sort in descending to remove elements from lists without modifying the indexes\n",
        "for p in persons:\n",
        "    to_remove[p].sort(reverse = True)\n",
        "\n",
        "\n",
        "# retrieve images candidates\n",
        "# --------------------------\n",
        "if len(os.listdir(path_datasets) ) == 0 or True:\n",
        "    logging.debug(\"datasets empty - need to retrieve all !\")\n",
        "    # removing images to discard\n",
        "    for person in persons:\n",
        "        for index in to_remove[person]:\n",
        "            img = images[person].pop(index)\n",
        "            logging.debug(\"Removing item \" + str(index) + \" from list \" + str(person))\n",
        "            try:\n",
        "                filename = path_discard +  str(index) +\"_discarded_\" + str(person.split(\".\")[0]) + \".jpg\"\n",
        "                cv2.imwrite(filename, img) \n",
        "            except:\n",
        "                logging.error(\"Error while writing discarded image \" + str(filename))\n",
        "\n",
        "    # randomly select among remaining images\n",
        "    for person in persons:\n",
        "        # build list of indices from remaining images\n",
        "        logging.debug(\"Phase 2 -> random selection for \" + str(person))\n",
        "\n",
        "        images_ = []\n",
        "        indices = []\n",
        "        new_ids = []\n",
        "        # prev_index = []\n",
        "        random.seed(person)\n",
        "\n",
        "        while len(indices) < datasets_size[person]:       \n",
        "            index = random.randrange(0, len(images[person]))\n",
        "            if index in indices:\n",
        "                logging.debug(\"Index among remaining = \" + str(index) + \" => already there\")\n",
        "                continue\n",
        "            else:\n",
        "                # prev_index.append(index)\n",
        "                indices.append(index)\n",
        "\n",
        "        logging.debug(\"Phase 2 -> random selection idx:  \" + str(indices))\n",
        "\n",
        "        for index in indices:\n",
        "            img = images[person][index]\n",
        "            images_.append(img)\n",
        "            filename = path_datasets +  str(vgg_ids[person][index]) + \"_\" + str(person.split(\".\")[0]) + \".jpg\"\n",
        "            logging.debug(\"saved: \" + str(filename))\n",
        "            cv2.imwrite(filename, img) \n",
        "            new_ids.append(vgg_ids[person][index])\n",
        "        images[person] = images_\n",
        "        vgg_ids[person] = new_ids\n",
        "else:\n",
        "    logging.debug(\"folders not empty => can build directly images dictionnary\")\n",
        "\n",
        "# logging.debug(\"Number of images keys=\" + len(images.keys))\n",
        "# logging.debug(\"Number of images values=\" + len(images.values))\n",
        "\n",
        "logging.info(pretty_return_dict_size(images))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XwJjZ1oaHEV9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Save to drive folders\n",
        "to_db_confirmation = False\n",
        "\n",
        "path_drive_DB = r\"/content/drive/My Drive/ComputerVision/DATABASE\"\n",
        "path_drive_Datasets = r\"/content/drive/My Drive/ComputerVision/DATASETS\"\n",
        "\n",
        "\n",
        "\n",
        "# drive folders should be properly set up\n",
        "\n",
        "if to_db_confirmation:\n",
        "    try:\n",
        "        drive.mount('/content/drive')\n",
        "    except:\n",
        "        pass\n",
        "    try:\n",
        "        shutil.rmtree(path_drive_DB)\n",
        "        shutil.rmtree(path_drive_Datasets)\n",
        "    except:\n",
        "        logging.error(\"Error in rmtree\") \n",
        "\n",
        "\n",
        "    try: \n",
        "        os.mkdir(path_drive_DB) \n",
        "        os.mkdir(path_drive_Datasets)\n",
        "    except OSError as error: \n",
        "        logging.error(error) \n",
        "    \n",
        "    logging.debug(\"Saving database in drive : start\")\n",
        "\n",
        "    fromDirectory = path_database\n",
        "    toDirectory = path_drive_DB\n",
        "    copy_tree(fromDirectory, toDirectory)\n",
        "\n",
        "    logging.debug(\"Saving datasets in drive : start\")\n",
        "\n",
        "    fromDirectory = path_datasets\n",
        "    toDirectory = path_drive_Datasets\n",
        "    copy_tree(fromDirectory, toDirectory)\n",
        "\n",
        "    logging.debug(\"Saving: done !\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GAdqr8emBM0J",
        "colab_type": "text"
      },
      "source": [
        "<!-- ## New Section title\n",
        "New section texte.\n",
        "\n",
        "[square]\n",
        "- List item1\n",
        "** * * *   List item2\n",
        "*   List item3\n",
        "    * list item 31\n",
        "    ** list item 32 -->\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5AF-XMBgbGRg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def draw_box(information_file_lines, index, image, person):\n",
        "    \"\"\"\n",
        "    information_file_lines is a list of lines of all images in the database\n",
        "    index is the vgg_idx\n",
        "    image is the image of interest, as retrieved from website\n",
        "    \"\"\"\n",
        "    raise RuntimeError(\"Probably not the smartest way...\")\n",
        "    logging.debug(\"VGG INDEX = \" + str(index))\n",
        "\n",
        "    if person == personA:\n",
        "        lines = information_file_lines[0:59]\n",
        "    elif person == personB:\n",
        "        lines = information_file_lines[59:119]\n",
        "    elif person == personC:\n",
        "        lines = information_file_lines[119:149]\n",
        "    elif person == personD:\n",
        "        lines = information_file_lines[150:179]\n",
        "    \n",
        "    for line in lines:\n",
        "        if int(line.split(\" \")[0]) == index:\n",
        "            h, w = img_.shape[:2]\n",
        "            \n",
        "            left = int(round(float(line.split(\" \")[2])))\n",
        "            # left = w - left\n",
        "            top = int(round(float(line.split(\" \")[3])))\n",
        "            # top = h - top\n",
        "            right = int(round(float(line.split(\" \")[4])))\n",
        "            # right = w - right\n",
        "            bottom = int(round(float(line.split(\" \")[5])))\n",
        "            # bottom = h - bottom\n",
        "            image = cv2.rectangle(image, (left,top), (right,bottom), [0,0,255], 10)\n",
        "\n",
        "        else:\n",
        "            continue\n",
        "\n",
        "    return image"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9HilePrqbDWC",
        "colab_type": "text"
      },
      "source": [
        "recognize faces based on haarCascade\n",
        "\n",
        "tutorial in: [How to detect faces using Haar Cascade](https://www.digitalocean.com/community/tutorials/how-to-detect-and-extract-faces-from-an-image-with-opencv-and-python)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G582IgL-fvi1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "faceCascade = cv2.CascadeClassifier(os.path.join(base_path, \"haarcascade_frontalface_default.xml\"))\n",
        "faces_cropped = {}\n",
        "\n",
        "with open(file_info, 'r') as f: \n",
        "    lines = f.readlines()\n",
        "\n",
        "for person in persons:\n",
        "\n",
        "    faces_cropped[person] = []\n",
        "\n",
        "    for img in images[person]:\n",
        "        img_ = img.copy()\n",
        "        img_gray = cv2.cvtColor(img_, cv2.COLOR_BGR2GRAY)\n",
        "        faces = faceCascade.detectMultiScale(\n",
        "            img_gray,\n",
        "            scaleFactor=1.13,\n",
        "            minNeighbors=10,\n",
        "            minSize=(30, 30),\n",
        "            flags=cv2.CASCADE_SCALE_IMAGE\n",
        "        )\n",
        "        for (x,y,w,h) in faces:\n",
        "            faces_cropped[person].append(img[y:y+h, x:x+w])\n",
        "            cv2.rectangle(img_, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
        "\n",
        "        # h, w = img_.shape[:2]\n",
        "        # draw_box(lines, int(vgg_ids[person][running_index])+1, img_, person)\n",
        "        # cv2_imshow(cv2.resize(img_, (w // 5, h // 5)))\n",
        "logging.info(\"Faces extracted and saved in dictionnary\")\n",
        "logging.info(pretty_return_dict_size(faces_cropped))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zVrtCWGgRFKX",
        "colab_type": "text"
      },
      "source": [
        "From Datasets of images to Trainng set and validation sets;\n",
        "\n",
        "```\n",
        "images[person]\n",
        "```\n",
        "is a dictionary of all the images in the database that are dedicated to a specific person. In order to obtain a :\n",
        "* training set\n",
        "* test set\n",
        "\n",
        "for the person A and B, one can randomly select 20 images for validation sets, and 10 images for test sets. \n",
        "\n",
        "For the sake of reproducibility, the details are logged"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ylnkm0G5SCcR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "training_sets_size = {}\n",
        "training_sets_size[personA] = 20\n",
        "training_sets_size[personB] = 20\n",
        "training_sets_size[personC] = 0\n",
        "training_sets_size[personD] = 0\n",
        "\n",
        "test_sets_size = {}\n",
        "test_sets_size[personA] = 10\n",
        "test_sets_size[personB] = 10\n",
        "test_sets_size[personC] = 10\n",
        "test_sets_size[personD] = 10\n",
        "\n",
        "training_set = {}\n",
        "test_set = {}\n",
        "for person in persons:\n",
        "    image_ = faces_cropped[person]\n",
        "    training_set_ = []\n",
        "    random.seed(person)\n",
        "    init_set = set(range(0, len(image_)))\n",
        "\n",
        "    indices_training = random.sample(init_set, training_sets_size[person])\n",
        "    indices_test = list(init_set - set(indices_training))\n",
        "\n",
        "    training_set[person] = [faces_cropped[person][i] for i in indices_training] \n",
        "    test_set[person] = [faces_cropped[person][i] for i in indices_test]\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZEo9lXETYXWR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# show_images_from_dict(training_set)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-pOSPIokbVSV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# show_images_from_dict(test_set)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j73x0LDhZX9k",
        "colab_type": "text"
      },
      "source": [
        "***AJOUTER UNE VISUALISATION*** "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cPlaQzrHZncq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#  Visualisation des images prises\n",
        "\n",
        "logging.error(\"NO visualisation found !\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fq0JMIBgOhih",
        "colab_type": "text"
      },
      "source": [
        "Feature Descriptor - HOG"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PrC1L60wOadz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "src = faces_cropped[personA][0]\n",
        "cv2_imshow(src)\n",
        "\n",
        "#1 resizing\n",
        "resized_img = cv2.resize(src, (128,128))\n",
        "\n",
        "fd, hog_image = hog(resized_img, \n",
        "                    orientations=9, \n",
        "                    pixels_per_cell=(8,8), \n",
        "                    cells_per_block=(2, 2), \n",
        "                    block_norm = \"L2\",\n",
        "                    visualize=True, \n",
        "                    transform_sqrt = True,\n",
        "                    multichannel=True)\n",
        "\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 8), sharex=True, sharey=True) \n",
        "\n",
        "ax1.imshow(cv2.cvtColor(resized_img, cv2.COLOR_BGR2RGB))\n",
        "ax1.set_title('Input image') \n",
        "\n",
        "# Rescale histogram for better display \n",
        "hog_image_rescaled = exposure.rescale_intensity(hog_image, in_range=(0, 10)) \n",
        "\n",
        "ax2.imshow(hog_image_rescaled, cmap=plt.cm.gray) \n",
        "ax2.set_title('Histogram of Oriented Gradients - rescaled')\n",
        "print(\"HOG Rescaled: \" + str(hog_image_rescaled.min()) + \" -> \" + str(hog_image_rescaled.max()) )\n",
        "\n",
        "# ax3.imshow(hog_image, cmap=plt.cm.gray) \n",
        "# ax3.set_title('Histogram of Oriented Gradients')\n",
        "# print(\"HOG: \" + str(hog_image.min()) + \" -> \" + str(hog_image.max()) )\n",
        "plt.show()\n",
        "\n",
        "print(hog_image_rescaled.shape)\n",
        "print(fd.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b_b7lnLnmuDk",
        "colab_type": "text"
      },
      "source": [
        "## Detail procedure for HOG\n",
        "# computation of the gradients\n",
        "* each pixel\n",
        "* magnitude and orientation\n",
        "    * unsigned gradient : On the right, we see the raw numbers representing the gradients in the 8×8 cells with one minor difference — the angles are between 0 and 180 degrees instead of 0 to 360 degrees. These are called “unsigned” gradients because a gradient and it’s negative are represented by the same numbers. In other words, a gradient arrow and the one 180 degrees opposite to it are considered the same. But, why not use the 0 – 360 degrees ? Empirically it has been shown that unsigned gradients work better than signed gradients for pedestrian detection. Some implementations of HOG will allow you to specify if you want to use signed gradients\n",
        "    \n",
        "* plot on a cell\n",
        "* size of the cell\n",
        "    * design choice\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a_muPjY3ScMI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "img = faces_cropped[personA][0].copy()\n",
        "img = cv2.imread(\"hog-preprocessing.jpg\")\n",
        "logging.debug(\"Toy Example\")\n",
        "cv2_imshow(img)\n",
        "\n",
        "y=95\n",
        "x=200\n",
        "h = 172\n",
        "w = 84\n",
        "crop_img = img[y:y+h, x:x+w]\n",
        "crop_img = cv2.resize(crop_img, (64,128))\n",
        "cv2_imshow(crop_img)\n",
        "\n",
        "img = np.float64(crop_img)\n",
        "\n",
        "gx = cv2.Sobel(img, cv2.CV_64F, 1, 0, ksize = 1)\n",
        "gy = cv2.Sobel(img, cv2.CV_64F, 0, 1, ksize = 1)\n",
        "\n",
        "mag, angle = cv2.cartToPolar(gx, gy, angleInDegrees=True)\n",
        "orn = angle.copy()\n",
        "\n",
        "# Normatlization to have a magnitude between 0 and 255\n",
        "mag_normalized = np.array(((mag - mag.min()) / (mag.max() - mag.min()) * 255 + 0)).astype(np.uint8)\n",
        "# orn_normalized = np.array(((orn - orn.min()) / (orn.max() - orn.min()) * 360 + 0)).astype(np.float64)\n",
        "orn_normalized = orn\n",
        "\n",
        "# mag_normalized = np.array(((mag) / (mag.max() - mag.min()) * 255 + 0)).astype(np.uint8)\n",
        "# orn_normalized = np.array(((orn) / (orn.max() - orn.min()) * 180 + 0)).astype(np.uint8)\n",
        "\n",
        "# mag_normalized = ((mag) * 255).astype(np.uint8)\n",
        "# orn_normalized = orn \n",
        "\n",
        "# mag_normalized = mag_normalized / np.sqrt(np.sum(mag_normalized**2))\n",
        "# orn_normalized = orn_normalized / np.sqrt(np.sum(orn_normalized**2))\n",
        "\n",
        "k = 8\n",
        "y_ = 10\n",
        "x_ = 24\n",
        "crop_img_cell = crop_img[y_:y_+k, x_:x_+k]\n",
        "\n",
        "\n",
        "\n",
        "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(16, 8), sharex=True, sharey=True) \n",
        "\n",
        "ax1.imshow(cv2.cvtColor(crop_img_cell, cv2.COLOR_BGR2RGB))\n",
        "ax1.set_title('Input image') \n",
        "\n",
        "\n",
        "# construction of the magnitude and orn matrices\n",
        "orn_matrix = np.zeros((k,k))\n",
        "for i in range(k):\n",
        "    for j in range(k):\n",
        "        idx = np.argmax(mag_normalized[y_+j,x_+i])\n",
        "        angle_ = round(orn[y_+j,x_+i, idx])\n",
        "        orn_matrix[i,j] = angle_\n",
        "\n",
        "orn_matrix_clipped = orn_matrix.copy() \n",
        "\n",
        "orn_matrix_clipped = ((orn_matrix_clipped)+ 90 )%360\n",
        "for i in range(k):\n",
        "    for j in range(k):\n",
        "        if 0 <= orn_matrix_clipped[i,j] < 180:\n",
        "            orn_matrix_clipped[i,j] = 180 - orn_matrix_clipped[i,j]\n",
        "        elif 180 <= orn_matrix_clipped[i,j] <=359.9:\n",
        "            orn_matrix_clipped[i,j] = 180 - orn_matrix_clipped[i,j] % 180\n",
        "\n",
        "orn_matrix_clipped.astype(np.uint8)\n",
        "# ax1 => just to visually represent the arrows\n",
        "for i in range(k):\n",
        "    for j in range(k):\n",
        "        r = round(mag_normalized[y_+j,x_+i].max())/(mag_normalized.max() - mag_normalized.min())\n",
        "        angle_ = orn_matrix[i,j]\n",
        "        ax1.arrow(i, j, r*np.cos(np.deg2rad(angle_)), r*np.sin(np.deg2rad(angle_)))\n",
        "\n",
        "for i in range(k):\n",
        "    for j in range(k):\n",
        "        c = orn_matrix_clipped[i,j]\n",
        "        ax2.text(i, j, str(c), fontsize=10,va='center', ha='center')\n",
        "\n",
        "ax2.matshow(orn_matrix_clipped, alpha=0)\n",
        "ax2.set_title('Orn values')\n",
        "\n",
        "\n",
        "# ax2.imshow(hog_image_rescaled, cmap=plt.cm.gray) \n",
        "ax3.set_title('Magnitude values')\n",
        "# print(\"HOG Rescaled: \" + str(hog_image_rescaled.min()) + \" -> \" + str(hog_image_rescaled.max()) )\n",
        "\n",
        "intersection_matrix = np.zeros(crop_img_cell.shape)\n",
        "\n",
        "for i in range(k):\n",
        "    for j in range(k):\n",
        "        c = round(mag_normalized[y_+j,x_+i].max())\n",
        "        ax3.text(i, j, str(c), fontsize=10,va='center', ha='center')\n",
        "        intersection_matrix[j,i] = c\n",
        "ax3.matshow(intersection_matrix)\n",
        "# ax2.imshow(cv2.cvtColor(crop_img_cell, cv2.COLOR_BGR2RGB))\n",
        "\n",
        "plt.show()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GuWXw_8xuTWb",
        "colab_type": "text"
      },
      "source": [
        "histogram construction is based on the gradient computed - both magnitude and orientation (as defined)\n",
        "\n",
        "- the bin is selected according to the orientation (direction) of the gradient; \n",
        "- the value that goes in the bin is based on the magnitude\n",
        "\n",
        "For instance on the toy example:\n",
        "first pixel has:\n",
        "* mag = 6; orn = 45°. So, the vote of this pixel goes for 75% in the bin of 40°, and 25% in the bin of 60°, as closer to 40°. As a result, we add 4.5 to bin nb 3, and 1.5 to bin nb 4. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RFSq-4SXoDZi",
        "colab_type": "text"
      },
      "source": [
        "Computations of the bins - pile up on our cell"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "smMfwDmsZAcL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "N_BUCKETS = 9\n",
        "bin_list = np.zeros(N_BUCKETS)\n",
        "\n",
        "def fill_bins_one_pixel(mag, orn, bin_list):\n",
        "    size_bin = 20.\n",
        "    if orn >= 160:\n",
        "        left_bin = 8\n",
        "        right_bin = 9\n",
        "        left_val= mag * (right_bin * 20 - orn) / 20\n",
        "        right_val = mag * (orn - left_bin * 20) / 20\n",
        "        left_bin = 8\n",
        "        right_bin = 0\n",
        "    else:\n",
        "        left_bin = int(orn / size_bin)\n",
        "        right_bin = (int(orn / size_bin) + 1) % N_BUCKETS\n",
        "        left_val= mag * (right_bin * 20 - orn) / 20\n",
        "        right_val = mag * (orn - left_bin * 20) / 20\n",
        "\n",
        "    bin_list[left_bin] += left_val\n",
        "    bin_list[right_bin] += right_val\n",
        "\n",
        "print(bin_list)\n",
        "\n",
        "toy_dir = np.array([[80,36,5,10,0,64,90,73],\n",
        "                    [37,9,9,179,78,27,169,166],\n",
        "                    [87,136,173,39,102,163,152,176],\n",
        "                    [76,13,1,168,159,22,125,143],\n",
        "                    [120,70,14,150,145,144,145,143],\n",
        "                    [58,86,119,98,100,101,133,113],\n",
        "                    [30,65,157,75,78,165,145,124],\n",
        "                    [11,170,91,4,110,17,133,110]])\n",
        "toy_mag = np.array([[2,3,4,4,3,4,2,2],\n",
        "                    [5,11,17,13,7,9,3,4],\n",
        "                    [11,21,23,27,22,17,4,6],\n",
        "                    [23,99,165,135,85,32,26,2],\n",
        "                    [91,155,133,136,144,152,57,28],\n",
        "                    [98,196,76,38,26,60,170,51],\n",
        "                    [165,60,60,27,77,85,43,136],\n",
        "                    [71,13,34,23,108,27,48,110]])\n",
        "print(mag_normalized.shape)\n",
        "\n",
        "for i in range(8):\n",
        "    for j in range(8):\n",
        "        m = mag_normalized[i,j].max()\n",
        "        d = orn_matrix_clipped[i,j]\n",
        "        fill_bins_one_pixel(m,d,bin_list)\n",
        "\n",
        "print(bin_list)\n",
        "\n",
        "n = np.linalg.norm(bin_list)\n",
        "bin_norms = bin_list/n\n",
        "plt.bar( range(9), bin_norms)\n",
        "plt.show()\n",
        "print(bin_norms)\n",
        "\n",
        "fd, hog_image = hog(crop_img_2, \n",
        "                    orientations=9, \n",
        "                    pixels_per_cell=(8,8), \n",
        "                    cells_per_block=(1, 1), \n",
        "                    block_norm = \"L2\",\n",
        "                    visualize=True, \n",
        "                    transform_sqrt = False,\n",
        "                    multichannel=True)\n",
        "print(fd)\n",
        "plt.bar( range(9), fd)\n",
        "\n",
        "\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(8, 8), sharex=True, sharey=True) \n",
        "\n",
        "ax1.imshow(cv2.cvtColor(crop_img_cell, cv2.COLOR_BGR2RGB))\n",
        "ax1.set_title('Input image') \n",
        "\n",
        "# Rescale histogram for better display \n",
        "hog_image_rescaled = exposure.rescale_intensity(hog_image, in_range=(0, 10)) \n",
        "\n",
        "ax2.imshow(hog_image_rescaled, cmap=plt.cm.gray) \n",
        "ax2.set_title('Histogram of Oriented Gradients - rescaled')\n",
        "\n",
        "print(hog_image_rescaled)\n",
        "plt.show()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "erQwBkq35RQ-",
        "colab_type": "text"
      },
      "source": [
        "Gradient is sensitive to light variation. We do not want that => normalization the histogram so that they are not affected by lighting variations\n",
        "\n",
        "> $v = [x, y, z]$, \n",
        "\n",
        "> $\\|v\\| = \\sqrt{x^2 + y^2 + z^2}$\n",
        "\n",
        "> $v_{normalized} =  \\frac{v}{\\|v\\|} = \\Big[ \\frac{x}{\\|v\\|}, \\frac{y}{\\|v\\|}, \\frac{z}{\\|v\\|} \\Big]$\n",
        "\n",
        "The normalization has a stronger helpful effect if applied on a larger scale than the cell only. AS a result, an often good choice is to apply on 4 cells, or a 16x16 block.  Four histograms are taken into account. \n",
        "* one cell (8 x 8) histogram is a vector of size 9 (as 9 bins)\n",
        "* one block (16 x 16) histogram is the concatenation of the 4 histograms, each representing one cell of the block, hence represented by a (36 x 1) vector.\n",
        "* the final HOG feature vector is based on the concatenation of all blocks.\n",
        "If the image as a width of size w*8 pixels, and a height of h*8 pixels, \n",
        "the image dimension in (8*h x 8*w). In such an image, they are :\n",
        "    -   h cells vertically, and w cells horizontally,\n",
        "    -  (h-1) blocks vertically and (w-1) blocks horizontally.\n",
        "\n",
        "For an image of 128 x 128, we then have:\n",
        "* w = 128/8 = 16\n",
        "* h = 128/8 = 16\n",
        "* #cells = 16x16\n",
        "* #Blocks = 15 * 15 = 225 blocks\n",
        "\n",
        "each block having a representative vector (36x1), the resulting vector has dimension (225*36 x 1), or (8100x1)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eXffH7oz8GEJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tfMDFGqsZvzC",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    }
  ]
}