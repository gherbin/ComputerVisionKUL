{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CV_Group9_assignment.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gherbin/ComputerVisionKUL/blob/master/CV_Group9_assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZsHsvp6wb1n2",
        "colab_type": "text"
      },
      "source": [
        "# Hi there! \n",
        "\n",
        "> *\\[14 Apr 2020] A notebook written by Geoffroy Herbin, group9, r0426473, in the context of the Computer Vision course [H02A5](https://p.cygnus.cc.kuleuven.be/webapps/blackboard/execute/announcement?method=search&context=course_entry&course_id=_891702_1&handle=announcements_entry&mode=view), Master of Artificial Intelligence, KULeuven.* \n",
        "\n",
        "\n",
        "Welcome to this Colab where we'll dig into some Computer Vision fancy stuff!\n",
        "\n",
        "![face recognition image](https://i.ibb.co/KKmkZYJ/emma-stone.jpg)\n",
        "\n",
        "The goals of this notebook is to perform faces classification and identification. To reach that goals we will first:\n",
        "- retrieve training and test images\n",
        "- build two \"features\".\n",
        "At this point, we simply say that a \"feature\" is another way to represent the input data (images, in our case). \n",
        "    1. handcrafted feature: Histogram of Oriented Gradients\n",
        "    2. feature learnt from the data: Principal Component Analysis\n",
        "\n",
        "Then, we will train different models based on the two features and compare the classification and identification results.\n",
        "\n",
        "---\n",
        "\n",
        "* Several optimized libraries (ex: `sklearn`) will be extensively used. However, at first, some of the key functionalities will be coded as to provide a better view of what really happens behind the calls to library functions.\n",
        "\n",
        "* The notebook is compatible with grayscale and colormode, depending on a parameter defined a bit later. The text, static content, is written based on analysis made in `color = False` mode. Results may vary a little.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SnkjeOT3GuBy",
        "colab_type": "text"
      },
      "source": [
        "## Import (most of) the required packages\n",
        "\n",
        "Almost all the required packages are imported first.\n",
        "> a few, used very locally, will be imported on the code snippet. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KH_DPxxjTeoI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import cv2\n",
        "import tarfile\n",
        "import zipfile\n",
        "import shutil\n",
        "\n",
        "import numpy as np\n",
        "from numpy.testing import assert_array_almost_equal\n",
        "\n",
        "import random\n",
        "import logging\n",
        "\n",
        "from urllib import request\n",
        "from socket import timeout\n",
        "from urllib.error import HTTPError, URLError\n",
        "\n",
        "from google.colab import drive\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "from distutils.dir_util import copy_tree\n",
        "\n",
        "from skimage.feature import hog as skimage_feature_hog\n",
        "from skimage import exposure\n",
        "\n",
        "from sklearn.decomposition import PCA as sklearn_decomposition_PCA\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn import svm \n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.model_selection import cross_val_predict\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "import sklearn.manifold\n",
        "\n",
        "\n",
        "from math import sqrt\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "from matplotlib.gridspec import GridSpec\n",
        "from matplotlib.offsetbox import OffsetImage, AnnotationBbox\n",
        "%matplotlib inline\n",
        "\n",
        "import seaborn as sns\n",
        "\n",
        "from scipy.interpolate import RectBivariateSpline\n",
        "from scipy.linalg import svd as scipy_linalg_svd\n",
        "from scipy import ndimage, misc\n",
        "\n",
        "\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "\n",
        "mpl_logger = logging.getLogger(\"matplotlib\")\n",
        "mpl_logger.setLevel(logging.WARNING)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XQ204pRs2P2i",
        "colab_type": "text"
      },
      "source": [
        "## Parameters\n",
        "\n",
        "As all computerized systems, several parameters help in defining how the system should react. \n",
        "Those parameters are centralized here."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ueU9Z-iMU63f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "base_path = \"/content/sample_data/CV__Group_assignment\"\n",
        "path_datasets = r\"/content/datasets/\"\n",
        "path_discard = r\"/content/discard/\"\n",
        "path_database = r\"/content/DATABASE/\"\n",
        "\n",
        "'''\n",
        "Parameters handling the database build up\n",
        "'''\n",
        "need_vgg_download = False\n",
        "confirmation_db_renewal = False\n",
        "to_drive_confirmation = False\n",
        "to_drive_confirmation_vgg = False\n",
        "\n",
        "load_from_local_drive = True # allows downloading the source images directly from the archive in github repository (see \"important note\")\n",
        "\n",
        "show = True # similar as global verbose parameter, for images (when custom functions allows it)\n",
        "\n",
        "sq_size = 64 # square size used -> shall be smaller than the output of get_min_size(faces_cropped) # assert sq_size <= min(get_min_size(faces_cropped))\n",
        "\n",
        "color = False # if False, tasks are run in Grayscale. if True, tasls are run in full colormode\n",
        "\n",
        "if color:\n",
        "    my_color_map = plt.cm.viridis\n",
        "else:\n",
        "    my_color_map = plt.cm.gray\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "83XvyfDBSLQX",
        "colab_type": "text"
      },
      "source": [
        "## Several utils functions\n",
        "\n",
        "Several *utils* functions are used to:\n",
        "- pretty plot a dictionnary content, \n",
        "- retrieve minimal size of a batch of images, \n",
        "- reshape in the appropriate way an input, considering the `color` and `sq_size` parameters defined, \n",
        "- retrieve the data in the appropriate format from the datasets initially built.\n",
        "\n",
        "\n",
        "You can find more info on the functions and the codes below, or when we'll use them later in the tutorial."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "05JhwSLCSPyI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def pretty_return_dict_size(my_dict):\n",
        "    '''\n",
        "    returns a string containing the different size of the elements of a dict\n",
        "    '''\n",
        "    output_list = [\"\\n\"]\n",
        "    for k in my_dict.keys():\n",
        "        output_list.append(str(k))\n",
        "        output_list.append(\":\")\n",
        "        output_list.append(str(len(my_dict[k])))\n",
        "        output_list.append(\"\\n\")\n",
        "    return ''.join(output_list)\n",
        "\n",
        "def show_images_from_dict(my_dict, show_index = False):\n",
        "    '''\n",
        "    shows the images contained in a dictionary, going through all keys\n",
        "    '''\n",
        "    for k in my_dict.keys():\n",
        "        logging.debug(\"@------------------- Images of \" + str(k) + \" -------------------@\")\n",
        "        index = 0\n",
        "        for img in my_dict[k]:\n",
        "            if show_index:\n",
        "                logging.debug(\"Image index: \" + str(index))\n",
        "                index+=1\n",
        "            cv2_imshow(img)\n",
        "            logging.debug(\"Shape = \" + str(img.shape))\n",
        "def get_min_size(images_dict):\n",
        "    '''\n",
        "    returns the minimum size of images contained in the images_dict input\n",
        "    '''\n",
        "    min_rows, min_cols = float(\"inf\"), float(\"inf\")\n",
        "    max_rows, max_cols = 0, 0\n",
        "    for person in persons:\n",
        "        for src in images_dict[person]:\n",
        "            r, c = src.shape[0], src.shape[1]    \n",
        "            min_rows = min(min_rows, r)\n",
        "            max_rows = max(max_rows, r)\n",
        "            min_cols = min(min_cols, c)\n",
        "            max_cols = max(max_cols, c)\n",
        "    # logging.info(\"smallest px numbers (row, cols) = \" + str((min_rows,min_cols)))\n",
        "    return min_rows, min_cols\n",
        "        \n",
        "def my_reshape(image_vector, sq_size, color):\n",
        "    '''\n",
        "    returns a reshape version of an image represented as an image array, depending of the color parameter.\n",
        "    If color is True, it returns a colored RGB format image of size (sq_size x sq_size) (useable as is by matplotlib)\n",
        "    If color is False, it returns a grayscale image (sq_size x sq_size)\n",
        "    '''\n",
        "    flattened = image_vector.ndim == 1\n",
        "    if flattened:\n",
        "        if color:\n",
        "            img_reshaped = (np.reshape(image_vector, (sq_size, sq_size, 3))).astype('uint8')\n",
        "            return cv2.cvtColor(img_reshaped, cv2.COLOR_BGR2RGB)\n",
        "        else:\n",
        "            return np.reshape(image_vector, (sq_size, sq_size))\n",
        "    else:\n",
        "        if color:\n",
        "            img_reshaped = (np.reshape(image_vector, (sq_size, sq_size, 3))).astype('uint8')\n",
        "            return cv2.cvtColor(img_reshaped, cv2.COLOR_BGR2RGB)\n",
        "        else:\n",
        "            return image_vector\n",
        "\n",
        "def get_matrix_from_set(images_set, color, sq_size = 64, flatten = True):\n",
        "    '''\n",
        "    from images_set (training_set or test_set), create and fill in matrix so that it contains the input data.\n",
        "    if flatten, then the matrix contains images represented in 1D\n",
        "    '''\n",
        "\n",
        "    # init output\n",
        "    matrix = None\n",
        "    nb_faces = sum([len(images_set[x]) for x in images_set if isinstance(images_set[x], list)])\n",
        "    # depending on mode, select appropriate size items. N\n",
        "    \n",
        "    if color and flatten:\n",
        "        matrix = np.empty((nb_faces, sq_size*sq_size*3)) # *3 => color images\n",
        "    elif color and (not flatten):\n",
        "        matrix = np.empty((nb_faces, sq_size, sq_size, 3))\n",
        "    elif (not color) and flatten:\n",
        "        matrix = np.empty((nb_faces, sq_size*sq_size))\n",
        "    elif (not color) and (not flatten):\n",
        "        matrix = np.empty((nb_faces, sq_size,sq_size ))\n",
        "    else:\n",
        "        raise RuntimeError\n",
        "\n",
        "    i = 0\n",
        "    for person in persons:\n",
        "        for src in images_set[person]:\n",
        "            src_rescaled = cv2.resize(src, (sq_size,sq_size))\n",
        "            if color and flatten:\n",
        "                matrix[i,:] = src_rescaled.flatten()\n",
        "            elif color and (not flatten):\n",
        "                matrix[i,:,:,:] = src_rescaled\n",
        "            elif (not color) and flatten:\n",
        "                matrix[i,:] = cv2.cvtColor(src_rescaled, cv2.COLOR_BGR2GRAY).flatten()\n",
        "            elif (not color) and (not flatten):\n",
        "                matrix[i,:,:] = cv2.cvtColor(src_rescaled, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "            i +=1\n",
        "    return matrix\n",
        "\n",
        "def plot_matrix(images_matrix, color, my_color_map, h=8, w=5, transpose = False, return_figure = False):\n",
        "    '''\n",
        "    plots the images contained in a matrix of data, reshaping and coloring them\n",
        "    '''\n",
        "    fig = plt.figure(figsize=(w,h)) \n",
        "    fig.subplots_adjust(left=0, right=1, bottom=0, top=1, hspace=0.05, wspace=0.05) \n",
        "    # plot the faces, each image is 64 by 64 pixels \n",
        "\n",
        "    if transpose:\n",
        "        images_matrix_used = images_matrix.T.copy()\n",
        "    else:\n",
        "        images_matrix_used = images_matrix.copy()\n",
        "\n",
        "    i=0\n",
        "    for img_vector in images_matrix_used: \n",
        "        ax = fig.add_subplot(h, w, i+1, xticks=[], yticks=[]) \n",
        "        ax.imshow(my_reshape(img_vector, sq_size, color), cmap = my_color_map, interpolation='nearest') \n",
        "        i+=1\n",
        "    plt.show()\n",
        "\n",
        "    if return_figure:\n",
        "        return fig"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ebQpRYW50fS2",
        "colab_type": "text"
      },
      "source": [
        "## Inputs\n",
        "\n",
        "* The very first input of the system is an archive containing several text files containing each 1000 weblinks to images. This archive is downloaded from [here](http://www.robots.ox.ac.uk/~vgg/data/vgg_face) and extracted locally in `/content/sample_data/CV__Group_assignment` folder. We download and extract it only if needed.\n",
        "\n",
        "* To extract faces in the images, we download the Haarcascade model\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BGBkRUir9f7g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if not os.path.isdir(base_path):\n",
        "  os.makedirs(base_path)\n",
        "\n",
        "if need_vgg_download:\n",
        "    vgg_face_dataset_url = \"http://www.robots.ox.ac.uk/~vgg/data/vgg_face/vgg_face_dataset.tar.gz\"\n",
        "    with request.urlopen(vgg_face_dataset_url) as r, open(os.path.join(base_path, \"vgg_face_dataset.tar.gz\"), 'wb') as f:\n",
        "        f.write(r.read())\n",
        "\n",
        "    with tarfile.open(os.path.join(base_path, \"vgg_face_dataset.tar.gz\")) as f:\n",
        "        f.extractall(os.path.join(base_path))\n",
        "\n",
        "\n",
        "trained_haarcascade_url = \"https://raw.githubusercontent.com/opencv/opencv/master/data/haarcascades/haarcascade_frontalface_default.xml\"\n",
        "with request.urlopen(trained_haarcascade_url) as r, open(os.path.join(base_path, \"haarcascade_frontalface_default.xml\"), 'wb') as f:\n",
        "    f.write(r.read())\n",
        "logging.info(\"Downloaded haarcascade_frontalface_default\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MOH3DL2jKk3U",
        "colab_type": "text"
      },
      "source": [
        "# Data Retrieval\n",
        "\n",
        "This tutorial will extensively use images from four different actors. The images are selected (pseudo-)randomly.\n",
        "\n",
        "The movie stars are (chosen quite randomly as well):\n",
        "1.   personA: Emma Stone\n",
        "2.   personB: Bradley Cooper\n",
        "3.   personC: Jane Levy\n",
        "4.   personD: Marc Blucas\n",
        "\n",
        "\n",
        "Process to get datasets images:\n",
        "1. Randomly pick N images (60 for persons A and B, and 30 for persons C and D) images from the list of 1000 images provided in the textfile. \n",
        "3. Reject some \"i\" images that are not appropriated (see rejection step later) for each person. it may of course be different \"i\" for all actors.\n",
        "2. Select M images randomly out of the (N-i) images obtained for each persons:\n",
        "    - M=30 for personA and personB (Training and Test),\n",
        "    - M=10 for personC and personD (Test only)\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "**[IMPORTANT NOTE]**\n",
        "\n",
        "If nothing else is set up, getting the N images require to perform an url request on websites we do not control. This is risky, as for any reason, the target website could be modified, not responding, responding too slowly, have removed the picture of interest, ...\n",
        "To prevent such issue, this tutorial provide the code to do things differently.\n",
        "- the first time (with some parameters below properly set), the source images are downloaded from the website (retrieving errors, skipping too slow website, etc.)\n",
        "- the images, downloaded, are then saved and zipped with a logfile\n",
        "- this zip archive is then uploaded on my personal Github account, as a public file\n",
        "\n",
        "It leads to a controlled database containing the source images, and ensure reproducibility during the different test run.\n",
        "\n",
        "*Three remarks*\n",
        "1. only the original files are stored in the archive in the ZIP. Those files were selected randomly, using a random number generator.\n",
        "2. the curation of the source files, the face cropping, and selection between training and test sets is still done at every run of this notebook.\n",
        "3. the code dedicated to the archiving and saving part will not be detailed (while yet provided) in this notebook, but surely, you are welcome to contact me for more details using geoffroy.herbin@student.kuleuven.be.\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cFh6oMU6phLs",
        "colab_type": "text"
      },
      "source": [
        "Start from clean sheet\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NAMpaW-Ym0oG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "try:\n",
        "    shutil.rmtree(path_database)\n",
        "    shutil.rmtree(path_datasets)\n",
        "    shutil.rmtree(path_discard)\n",
        "except:\n",
        "    pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I4Rj5OSS3fUx",
        "colab_type": "text"
      },
      "source": [
        "Create required folders"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hQw39iLK36m1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "file_info = path_database+ r\"info_retrieved.txt\"\n",
        "\n",
        "try: \n",
        "    os.mkdir(path_database)\n",
        "    os.mkdir(path_datasets) \n",
        "    os.mkdir(path_discard)\n",
        "except OSError as error: \n",
        "    logging.error(error) \n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JuEfFZE42YeO",
        "colab_type": "text"
      },
      "source": [
        "Instead of randomly download from web, download images from a \"clean\" and controlled repository (in [github](https://raw.githubusercontent.com/gherbin/cv_group9_database_replica/master/DATABASE-20200318T142918Z-001.zip) ), dedicated for this notebook. It ensures reproducibility and accessibility to the 180 input images.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TgaFPa0C2YJH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if load_from_local_drive:\n",
        "    \n",
        "    !wget https://raw.githubusercontent.com/gherbin/cv_group9_database_replica/master/DATABASE-20200318T142918Z-001.zip\n",
        "\n",
        "    with zipfile.ZipFile(\"DATABASE-20200318T142918Z-001.zip\", 'r') as zip_ref:\n",
        "        zip_ref.extractall()\n",
        "    !rm -r \"DATABASE-20200318T142918Z-001.zip\"\n",
        "\n",
        "path_, dirs_, files = next(os.walk(path_database))\n",
        "if len(files) == 180+1:\n",
        "    logging.info(\"Successful database retrieval\")\n",
        "elif load_from_local_drive:\n",
        "    logging.error(\"Most Likely problem with database retrieval, number of files = \" + str(len(files)))\n",
        "else:\n",
        "    logging.info(\"No database images retrieved yet\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M09QQcnf38xN",
        "colab_type": "text"
      },
      "source": [
        "###Definition of several data structures\n",
        "\n",
        "`images_size` : dictionary containing the number of images to first get from the web\n",
        "\n",
        "`persons` : list containing the names of the four persons (the names actually are the name of the text file in original database)\n",
        "\n",
        "`images`: dictionary containing the source images. The keys of the dictionary are the names of the four persons of interest"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UEGMZ43b4iaa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "personA = \"Emma_Stone.txt\"\n",
        "personC = \"Jane_Levy.txt\"\n",
        "personB = \"Bradley_Cooper.txt\"\n",
        "personD = \"Marc_Blucas.txt\"\n",
        "persons = [personA, personB, personC, personD]\n",
        "datasets_dict = {}\n",
        "images_size = {}\n",
        "images_size[personA] = 60\n",
        "images_size[personB] = 60\n",
        "images_size[personC] = 30\n",
        "images_size[personD] = 30\n",
        "\n",
        "total_images_size = sum(images_size.values())\n",
        "\n",
        "# Dictionary containing the ids of the pictures downloaded from internet\n",
        "vgg_ids = {}\n",
        "for p in persons:\n",
        "    vgg_ids[p] = []\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8qdfeFWLAL6C",
        "colab_type": "text"
      },
      "source": [
        "If `confirmation_db_renewal` is `True`, the following code picks randomly (based on a seed being the name of the person) the images from the web.\n",
        "\n",
        "For a normal run, if the user does not want to change the original sourced data, `confirmation_db_renewal` should remain `False` (aka *change at your own risk* ;-) )\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I5VsGBAJ3inK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if confirmation_db_renewal:\n",
        "    try:\n",
        "        shutil.rmtree(path_database)\n",
        "    except:\n",
        "        pass \n",
        "    try:\n",
        "        os.mkdir(path_database)\n",
        "    except:\n",
        "        pass \n",
        "\n",
        "    fo = open(file_info, \"w+\")\n",
        "\n",
        "    # images = {}\n",
        "    # images_nominal_indices = {}\n",
        "    for person in persons:\n",
        "        logging.debug(\"Taking care of: \" + str(person))\n",
        "        random.seed(person)\n",
        "        # print(hash(person))\n",
        "        images_ = []\n",
        "        # images_nominal_indices_ = []\n",
        "        prev_index = []\n",
        "\n",
        "\n",
        "        with open(os.path.join(base_path, \"vgg_face_dataset\", \"files\", person), 'r') as f:\n",
        "            lines = f.readlines()       \n",
        "        \n",
        "\n",
        "        while len(images_) < images_size[person]:\n",
        "            index = random.randrange(0, 1000)\n",
        "            logging.debug(\"Index = \" + str(index))\n",
        "            if index in prev_index:\n",
        "                logging.debug(\"Index = \" + str(index) + \" => already there\")\n",
        "                continue\n",
        "            else:\n",
        "                prev_index.append(index)\n",
        "                line = lines[index]\n",
        "                # only curated data\n",
        "                if int(line.split(\" \")[8]) == 1:\n",
        "                    url = line[line.find(\"http://\"): line.find(\".jpg\") + 4]\n",
        "                    logging.debug(\"URL > \\\"\" + str(url))\n",
        "                    try:\n",
        "                        res = request.urlopen(url, timeout = 1)\n",
        "                        img = np.asarray(bytearray(res.read()), dtype=\"uint8\")\n",
        "                        img = cv2.imdecode(img, cv2.IMREAD_COLOR)\n",
        "\n",
        "                        h, w = img.shape[:2]\n",
        "                        cv2_imshow(cv2.resize(img, (w//4, h//4)))\n",
        "                        # images_nominal_indices_.append(index)\n",
        "\n",
        "                        filename = path_database +  str(index) + \"_\" + str(person.split(\".\")[0]) + \".jpg\"\n",
        "\n",
        "                        value = cv2.imwrite(filename, img) \n",
        "                        # logging.debug(\"saved in DB: \" + str(filename))\n",
        "                        images_.append(img)\n",
        "                        fo.write(line)\n",
        "                    except ValueError as e:\n",
        "                            logging.error(\"Value Error >\" + str(e))\n",
        "                    except (HTTPError, URLError) as e:\n",
        "                            logging.error('ERROR RETRIEVING URL >' + str(e))\n",
        "                    except timeout:\n",
        "                            logging.error('socket timed out - URL %s', str(url))\n",
        "                    except cv2.error as e: \n",
        "                            logging.error(\"ERROR WRITING FILE IN DB  >\" + str(e))\n",
        "                    except:\n",
        "                        logging.error(\"Weird exception : \" + str(line))\n",
        "                else:\n",
        "                    logging.debug(\"File not curated => rejected (id = \" + str(index) + \" )\")    \n",
        "                \n",
        "                # images[person] = images_\n",
        "                # images_nominal_indices[person] = images_nominal_indices_\n",
        "\n",
        "    fo.close()\n",
        "else:\n",
        "    logging.warning(\"If you really want to erase and renew the database, please change first the \\\"confirmation\\\" boolean variable, at the beginning of this cell\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P3eNIMAdCLVu",
        "colab_type": "text"
      },
      "source": [
        "From the logfile in the archive, extract the information and fill the dictionary containing all the images `images`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AdFihucjCO6e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open(file_info, 'r') as f: \n",
        "    lines = f.readlines()\n",
        "\n",
        "assert len(lines)==total_images_size, \"amount of lines in file incompatible\" \n",
        "\n",
        "images = {}\n",
        "\n",
        "for p in persons:\n",
        "    images[p] = []\n",
        "\n",
        "\n",
        "images_index = {}\n",
        "for running_index in range(len(lines)):\n",
        "    if running_index in range(0,images_size[personA]):\n",
        "        p = personA\n",
        "    elif running_index in range(images_size[personA],images_size[personA]+images_size[personB]):\n",
        "        p = personB\n",
        "    elif running_index in range(images_size[personA]+images_size[personB],images_size[personA]+images_size[personB]+images_size[personC]):\n",
        "        p = personC\n",
        "    elif running_index in range(images_size[personA]+images_size[personB]+images_size[personC],total_images_size):\n",
        "        p = personD\n",
        "    ind = str(int(lines[running_index].split(\" \")[0])-1)\n",
        "    vgg_ids[p].append(ind)\n",
        "    filename = ind + \"_\" + str(p.split(\".\")[0]) + \".jpg\"\n",
        "    images[p].append(cv2.imread(path_database+filename, cv2.IMREAD_COLOR))\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9v4tuRVO67u1",
        "colab_type": "text"
      },
      "source": [
        "###Rejection step\n",
        "From the sources files, although the images downloaded were part of a curated data, several images need to be removed to be used in the context of this *educative* tutorial. The main reasons are:\n",
        "\n",
        "*   too different from usual representation (make up, masks, ...)\n",
        "*   really poor image quality\n",
        "*   irrelevance and/or error in dataset\n",
        "*   same image already in dataset\n",
        "*   cropped image\n",
        "\n",
        "Considering the tight selection of images to train our model (20 from personA and 20 from personB), and the relatively large global amount of image candidates (1000 for each person), it is acceptable to reject the images we know won't help.\n",
        "\n",
        "From the initial retrieved images, we then remove the undesired images, that we copy in discard images folder, for tracking purposes. We/you may want to use them later.\n",
        "\n",
        "---\n",
        "`datasets_size`: dictionary of the size required per persons ( keys = person names)\n",
        "\n",
        "`to_remove`: dictionary containing the indices to remove, per persons ( keys = person names)\n",
        "\n",
        "`print_images = False` indicates that the remaining images in `images` dictionary will not be printed. \n",
        "\n",
        "---\n",
        "\n",
        "From the remaining images (after rejection), we can select randomly the images that are part of the final sets (training and test sets are not split yet)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZPKRTSB5r8pg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Dictionary of the size required (see section 3)\n",
        "datasets_size = {}\n",
        "datasets_size[personA] = 30\n",
        "datasets_size[personB] = 30\n",
        "datasets_size[personC] = 10\n",
        "datasets_size[personD] = 10\n",
        "\n",
        "\n",
        "# manually remove images that are not relevant or considered not good enough to be part of the dataset\n",
        "to_remove = {}\n",
        "to_remove[personA] = [0,1,4,8,12,13,16,23,28,34,36,42,44,47,48,49,54]\n",
        "to_remove[personB] = [4,7,11,12,13,16,21,22,23,24,25,26,27,32,36,39,41,46,49,53,55,58]\n",
        "to_remove[personC] = [0,1,6,7,8,11,14,16,17,19,20,21,24]\n",
        "to_remove[personD] = [0,3,5,6,8,10,12,15,16,17,24]\n",
        "\n",
        "# goal is to sort in descending to remove elements from lists without modifying the indexes\n",
        "for p in persons:\n",
        "    to_remove[p].sort(reverse = True)\n",
        "\n",
        "\n",
        "# retrieve images candidates\n",
        "# --------------------------\n",
        "if len(os.listdir(path_datasets) ) == 0 or True:\n",
        "    logging.debug(\"datasets empty - need to retrieve all !\")\n",
        "    # removing images to discard\n",
        "    for person in persons:\n",
        "        for index in to_remove[person]:\n",
        "            img = images[person].pop(index)\n",
        "            logging.debug(\"Removing item \" + str(index) + \" from list \" + str(person))\n",
        "            try:\n",
        "                filename = path_discard +  str(index) +\"_discarded_\" + str(person.split(\".\")[0]) + \".jpg\"\n",
        "                cv2.imwrite(filename, img) \n",
        "            except:\n",
        "                logging.error(\"Error while writing discarded image \" + str(filename))\n",
        "\n",
        "    # randomly select among remaining images\n",
        "    for person in persons:\n",
        "        # build list of indices from remaining images\n",
        "        logging.debug(\"Phase 2 (part 1) -> random indices selection for \" + str(person))\n",
        "\n",
        "        images_ = []\n",
        "        indices = []\n",
        "        new_ids = []\n",
        "        # prev_index = []\n",
        "        random.seed(person)\n",
        "\n",
        "        while len(indices) < datasets_size[person]:       \n",
        "            index = random.randrange(0, len(images[person]))\n",
        "            if index in indices:\n",
        "                logging.debug(\"Index among remaining = \" + str(index) + \" => already there\")\n",
        "                continue\n",
        "            else:\n",
        "                # prev_index.append(index)\n",
        "                indices.append(index)\n",
        "\n",
        "        logging.debug(\"Phase 2 (part 2) -> image selection based on indices\")\n",
        "\n",
        "        for index in indices:\n",
        "            img = images[person][index]\n",
        "            images_.append(img)\n",
        "            filename = path_datasets +  str(vgg_ids[person][index]) + \"_\" + str(person.split(\".\")[0]) + \".jpg\"\n",
        "            logging.debug(\"saved: \" + str(filename))\n",
        "            cv2.imwrite(filename, img) \n",
        "            new_ids.append(vgg_ids[person][index])\n",
        "        images[person] = images_\n",
        "        vgg_ids[person] = new_ids\n",
        "else:\n",
        "    logging.debug(\"folders not empty => can build directly images dictionnary\")\n",
        "\n",
        "# logging.debug(\"Number of images keys=\" + len(images.keys))\n",
        "# logging.debug(\"Number of images values=\" + len(images.values))\n",
        "\n",
        "logging.info(pretty_return_dict_size(images))\n",
        "''' \n",
        "print images to get to_remove indices\n",
        "'''\n",
        "print_images = False\n",
        "if print_images:\n",
        "    for person in persons:\n",
        "        counter = 0\n",
        "        for img in images[person]:\n",
        "            h = 0\n",
        "            w = 0\n",
        "            img_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "            faces = faceCascade.detectMultiScale(\n",
        "                img_gray,\n",
        "                scaleFactor=1.13,\n",
        "                minNeighbors=10,\n",
        "                minSize=(30, 30),\n",
        "                flags=cv2.CASCADE_SCALE_IMAGE\n",
        "            )\n",
        "            for (x,y,w,h) in faces:\n",
        "                # faces_cropped[person].append(img[y:y+h, x:x+w])\n",
        "                cv2.rectangle(img, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
        "            logging.debug(\"------------------------------------------------------\")\n",
        "            logging.debug(\"Photo ID = \" + str(counter))\n",
        "            logging.debug(\"size = \" + str((h,w)))\n",
        "            cv2_imshow(img)\n",
        "            counter += 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jvExNNUtK5AU",
        "colab_type": "text"
      },
      "source": [
        "###Save images on drive (only if required)\n",
        "Mount drive and save images, according to the parameter `to_drive_confirmation` value. *Change at your own risk ;-)*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XwJjZ1oaHEV9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Save to drive folders\n",
        "path_drive_DB = r\"/content/drive/My Drive/ComputerVision/DATABASE\"\n",
        "path_drive_Datasets = r\"/content/drive/My Drive/ComputerVision/DATASETS\"\n",
        "\n",
        "# drive folders should be properly set up\n",
        "\n",
        "if to_drive_confirmation:\n",
        "    \n",
        "    logging.warning(\"You need to have a drive mounted for this snippet to run successfully\")\n",
        "\n",
        "    try:\n",
        "        drive.mount('/content/drive')\n",
        "    except:\n",
        "        pass\n",
        "    try:\n",
        "        shutil.rmtree(path_drive_DB)\n",
        "        shutil.rmtree(path_drive_Datasets)\n",
        "    except:\n",
        "        logging.error(\"Error in rmtree\") \n",
        "\n",
        "\n",
        "    try: \n",
        "        os.mkdir(path_drive_DB) \n",
        "        os.mkdir(path_drive_Datasets)\n",
        "    except OSError as error: \n",
        "        logging.error(error) \n",
        "    \n",
        "    logging.debug(\"Saving database in drive : start\")\n",
        "\n",
        "    fromDirectory = path_database\n",
        "    toDirectory = path_drive_DB\n",
        "    copy_tree(fromDirectory, toDirectory)\n",
        "\n",
        "    logging.debug(\"Saving datasets in drive : start\")\n",
        "\n",
        "    fromDirectory = path_datasets\n",
        "    toDirectory = path_drive_Datasets\n",
        "    copy_tree(fromDirectory, toDirectory)\n",
        "\n",
        "    logging.debug(\"Saving: done !\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z5K7VyKZupB3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "path_vgg = r\"/content/sample_data/CV__Group_assignment\"\n",
        "path_drive_vgg = r\"/content/drive/My Drive/ComputerVision/CV__Group_assignment\"\n",
        "\n",
        "# drive folders should be properly set up\n",
        "\n",
        "if to_drive_confirmation_vgg:\n",
        "    \n",
        "    logging.warning(\"You need to have a drive mounted for this snippet to run successfully\")\n",
        "\n",
        "    try:\n",
        "        drive.mount('/content/drive')\n",
        "    except:\n",
        "        pass\n",
        "    try:\n",
        "        shutil.rmtree(path_drive_vgg)\n",
        "    except:\n",
        "        logging.error(\"Error in rmtree\") \n",
        "\n",
        "    try: \n",
        "        os.mkdir(path_drive_vgg) \n",
        "    except OSError as error: \n",
        "        logging.error(error) \n",
        "    \n",
        "    logging.debug(\"Saving database in drive : start\")\n",
        "\n",
        "    fromDirectory = path_vgg\n",
        "    toDirectory = path_drive_vgg\n",
        "    copy_tree(fromDirectory, toDirectory)\n",
        "\n",
        "    logging.debug(\"Saving: done !\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9HilePrqbDWC",
        "colab_type": "text"
      },
      "source": [
        "##Face detection using Haar Cascade\n",
        "From the raw images saved in the `images` dictionary, the faces are extracted using the *HaarCascade* method.\n",
        "\n",
        "The following code is based on the tutorial: [How to detect faces using Haar Cascade](https://www.digitalocean.com/community/tutorials/how-to-detect-and-extract-faces-from-an-image-with-opencv-and-python)\n",
        "\n",
        "The faces are saved in a new dictionary: `faces_cropped`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G582IgL-fvi1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "faceCascade = cv2.CascadeClassifier(os.path.join(base_path, \"haarcascade_frontalface_default.xml\"))\n",
        "faces_cropped = {}\n",
        "\n",
        "with open(file_info, 'r') as f: \n",
        "    lines = f.readlines()\n",
        "\n",
        "for person in persons:\n",
        "\n",
        "    faces_cropped[person] = []\n",
        "\n",
        "    for img in images[person]:\n",
        "        img_ = img.copy()\n",
        "        img_gray = cv2.cvtColor(img_, cv2.COLOR_BGR2GRAY)\n",
        "        faces = faceCascade.detectMultiScale(\n",
        "            img_gray,\n",
        "            scaleFactor=1.13,\n",
        "            minNeighbors=10,\n",
        "            minSize=(30, 30),\n",
        "            flags=cv2.CASCADE_SCALE_IMAGE\n",
        "        )\n",
        "        for (x,y,w,h) in faces:\n",
        "            faces_cropped[person].append(img[y:y+h, x:x+w])\n",
        "            cv2.rectangle(img_, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
        "\n",
        "        # h, w = img_.shape[:2]\n",
        "        # draw_box(lines, int(vgg_ids[person][running_index])+1, img_, person)\n",
        "        # cv2_imshow(cv2.resize(img_, (w // 2, h // 2)))\n",
        "\n",
        "logging.info(\"Faces extracted and saved in dictionnary faces_cropped\")\n",
        "logging.info(pretty_return_dict_size(faces_cropped))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zVrtCWGgRFKX",
        "colab_type": "text"
      },
      "source": [
        "At this point, `faces_cropped` dictionary contains 30 cropped faces for personA and B, and 10 images for personC and personD.\n",
        "\n",
        "The following code selects randomly (based on a seed) the 20 images part of the training set for personA and personB. The other faces (10 for each person) are then part of the test set.\n",
        "\n",
        "---\n",
        "\n",
        "* `training_set`: dictionary containing faces cropped (original size) part of the training set\n",
        "* `test_set`: dictionary containing faces cropped (original size) part of the test set\n",
        "---\n",
        "\n",
        "At this point, there is not (yet) dedicated validation sets. It is discussed later on.\n",
        "\n",
        "All the training will be done on the training set faces, without any tailoring or dedicated fitting on the test set images. Indeed, metrics on the test set faces indicate how well our model will generalize. It's therefore important to not influence our model with the data of the test set.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ylnkm0G5SCcR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "training_sets_size = {}\n",
        "training_sets_size[personA] = 20\n",
        "training_sets_size[personB] = 20\n",
        "training_sets_size[personC] = 0\n",
        "training_sets_size[personD] = 0\n",
        "\n",
        "test_sets_size = {}\n",
        "test_sets_size[personA] = 10\n",
        "test_sets_size[personB] = 10\n",
        "test_sets_size[personC] = 10\n",
        "test_sets_size[personD] = 10\n",
        "\n",
        "training_set = {}\n",
        "test_set = {}\n",
        "for person in persons:\n",
        "    image_ = faces_cropped[person]\n",
        "    training_set_ = []\n",
        "    random.seed(person)\n",
        "    init_set = set(range(0, len(image_)))\n",
        "\n",
        "    indices_training = random.sample(init_set, training_sets_size[person])\n",
        "    indices_test = list(init_set - set(indices_training))\n",
        "\n",
        "    training_set[person] = [faces_cropped[person][i] for i in indices_training] \n",
        "    test_set[person] = [faces_cropped[person][i] for i in indices_test]\n",
        "\n",
        "logging.info(\"Faces saved in dictionnary training_set: \")\n",
        "logging.info(pretty_return_dict_size(training_set))\n",
        "logging.info(\"Faces saved in dictionnary test_set: \")\n",
        "logging.info(pretty_return_dict_size(test_set))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "pAvlX6WBio8R",
        "colab": {}
      },
      "source": [
        "# show_images_from_dict(training_set)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-pOSPIokbVSV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# show_images_from_dict(test_set)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j73x0LDhZX9k",
        "colab_type": "text"
      },
      "source": [
        "### Faces of the training set\n",
        "- 20 faces of Emma Stone, personA\n",
        "- 20 faces of Bradley Cooper, personB\n",
        "\n",
        "PersonA and PersonB are quite different, A being a female, and B a male. Furthermore, the images within a class are somehow dissimilar as well\n",
        "- different viewpoints (front, left, right)\n",
        "- different lighting conditions\n",
        "- not same hair color\n",
        "- beard/no beard (personB)\n",
        "- not same (limited) background\n",
        "\n",
        "However, a similar characteristic is that both of the actors are most of the time smiling on the faces extracted. Sometimes showing their teeth, sometimes not.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cPlaQzrHZncq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#  Visualization \n",
        "training_set_matrix = get_matrix_from_set(training_set, color = True, sq_size = sq_size,flatten = True)\n",
        "plot_matrix(training_set_matrix, color = True, my_color_map = plt.cm.viridis, h=4, w=10, transpose = False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T84lBuROhHe3",
        "colab_type": "text"
      },
      "source": [
        "### Faces of the test set\n",
        "Test images are needed for four persons:\n",
        "- 10 faces of Emma Stone, personA\n",
        "- 10 faces of Bradley Cooper, personB\n",
        "- 10 faces of Jane Levy, personC\n",
        "- 10 faces of Marc Blucas, personD\n",
        "\n",
        "(A - C) and (B - D) respectively share some characteristics:\n",
        "* both female / male\n",
        "* same kind of skin tone\n",
        "* visually quite similar (especially for A and C)\n",
        "\n",
        "Within each groups, as for the training set, the faces are taken from different viewpoints, lightening conditions, ...\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kpceMYvAhH_Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_set_matrix = get_matrix_from_set(test_set, color = True, sq_size=sq_size, flatten = True)\n",
        "plot_matrix(test_set_matrix, color = True, my_color_map = plt.cm.viridis, h=4, w=10, transpose = False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fq0JMIBgOhih",
        "colab_type": "text"
      },
      "source": [
        "# Feature Representations\n",
        "\n",
        "A feature representation of an object is intuitively a piece of *information*, of a reduced dimension with respect to the object, and that captures the object.\n",
        "It tells what defines the object, and allows differentiating different objects.\n",
        "\n",
        "In the context of an image, a good feature needs to be:\n",
        "- **robust**: the same feature extracted from the same object on an image should be *close*, even if the lightening condition change, the view point change, ...\n",
        "- **discriminative**: different images, representing different object, should lead to different representation in the feature space. As a toy example, the size of an image is not a good feature to detect a person, as several person can be represented in images of the same size.\n",
        "\n",
        "We will look at two features:\n",
        "1. **HOG**: Histogram of Oriented Gradients. This is a handcrafted feature, extracted using a specific algorithm \n",
        "2. **PCA**: Principal Component Analysis. This is a feature learnt from the data.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yTaT6m7T4lF6",
        "colab_type": "text"
      },
      "source": [
        "## Histogram of Oriented Gradients - HOG\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kFIgA2EaDhSQ",
        "colab_type": "text"
      },
      "source": [
        "As this is a tutorial of Computer Vision, let's look first at what is visually / intuitively the HOG on a real image - one of the Emma Stone (personA) faces. The execution of the following code snippet shows on the left the input face, and on the right, the results.\n",
        "\n",
        "Then, we'll see the details of the algorithm, and its specificities (parameters)\n",
        "\n",
        "This section is extensively inspired by [this course](https://www.learnopencv.com/histogram-of-oriented-gradients/), while the technique has been introduced by [this paper](http://lear.inrialpes.fr/people/triggs/pubs/Dalal-cvpr05.pdf), which I strongly advise to read!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PrC1L60wOadz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "First example - Emma stone first image\n",
        "'''\n",
        "src = faces_cropped[personA][0]\n",
        "\n",
        "#1 resizing\n",
        "resized_img = cv2.resize(src, (sq_size, sq_size))\n",
        "#2 computing HOG\n",
        "fd, hog_image = skimage_feature_hog(resized_img, \n",
        "                    orientations=9, \n",
        "                    pixels_per_cell=(8,8), \n",
        "                    cells_per_block=(2, 2), \n",
        "                    block_norm = \"L2\",\n",
        "                    visualize=True, \n",
        "                    transform_sqrt = True,\n",
        "                    multichannel=True)\n",
        "\n",
        "'''\n",
        "Plotting results\n",
        "'''\n",
        "logging.info(\"Resized image and its Histogram of Oriented Gradients (its visual representation)\")\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 8), sharex=True, sharey=True) \n",
        "ax1.imshow(cv2.cvtColor(resized_img, cv2.COLOR_BGR2RGB))\n",
        "ax1.set_title('Input image') \n",
        "\n",
        "# Rescale histogram for better display \n",
        "hog_image_rescaled = exposure.rescale_intensity(hog_image, in_range=(0, 10)) \n",
        "\n",
        "ax2.imshow(hog_image_rescaled, cmap=plt.cm.gray) \n",
        "ax2.set_title('Histogram of Oriented Gradients - rescaled')\n",
        "logging.debug(\"HOG Rescaled: \" + str(hog_image_rescaled.min()) + \" -> \" + str(hog_image_rescaled.max()) )\n",
        "\n",
        "# ax3.imshow(hog_image, cmap=plt.cm.gray) \n",
        "# ax3.set_title('Histogram of Oriented Gradients')\n",
        "# logging.debug(\"HOG: \" + str(hog_image.min()) + \" -> \" + str(hog_image.max()) )\n",
        "\n",
        "plt.show()\n",
        "\n",
        "# logging.debug(hog_image_rescaled.shape)\n",
        "logging.debug(\"fd shape = \" + str(fd.shape))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b_b7lnLnmuDk",
        "colab_type": "text"
      },
      "source": [
        "### HOG - What is that ?\n",
        "\n",
        "HOG is a feature descriptor that extracts information from an image (or more precisely, a patch) based on the gradients in this image. More specifically, it builds a vector representing the weighted distribution of the gradients orientation across the images.\n",
        "\n",
        "#### Why is it interesting ?\n",
        "Let's remember that our goal is to perform image classification and identification. \n",
        "A face can be recognized through the inherent shapes: circular of face, shapes of the eyes, the nose, potentially the glasses, etc. The *edge* information is therefore useful! It is even more useful than the colors... Intuitively, you can think about recognizing someone familiar with only some contours of one face.\n",
        "\n",
        "![Drawing Obama](http://www.drawingskill.com/wp-content/uploads/2/Barack-Obama-Drawing-Pics.jpg)\n",
        "\n",
        "It is easy to recognize the former US President, while no color information is given. Intuitively, HOG gives the same information:\n",
        "- magnitude of gradient is large around the edges and corners\n",
        "- orientation gives the shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "upmrGJDTJfRt",
        "colab_type": "text"
      },
      "source": [
        "#### How to compute the HOG of an image ?\n",
        "\n",
        "In a nutshell:\n",
        "- The gradients are first computed on each pixel. \n",
        "- The gradients orientation and magnitude are used to build an histogram for a cell. The size of a cell is typically 8 x 8 pixels. \n",
        "- Those histogram are normalized \n",
        "- All the histograms computed on the images are then concatenated in a *long* vector, yet much smaller than original image. \n",
        "\n",
        "In the upcoming sections, we will detail the process, as well as the code and parameters required at each step."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kotEMzlaJdvw",
        "colab_type": "text"
      },
      "source": [
        "The following code snippet is a homemade class required to compute the HOG. The results obtained with this code will be compared with the infamous skimage library optimized for the HOG descriptor. \n",
        "\n",
        "You can simply run the snippet and come back later on to see the details of the implementation.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "stIpfAn9FIIz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MyHog():\n",
        "    \n",
        "    def __init__(self, img):\n",
        "        self.img = img # image of the size 64x64; 64x128; 128x128; ... => resized image of the original\n",
        "        self.mag_max, self.orn_max = self.compute_gradients()\n",
        "\n",
        "    def compute_gradients(self):\n",
        "        gx = cv2.Sobel(self.img, cv2.CV_32F, 1, 0, ksize = 1)\n",
        "        gy = cv2.Sobel(self.img, cv2.CV_32F, 0, 1, ksize = 1)\n",
        "\n",
        "        mag, angle = cv2.cartToPolar(gx, gy, angleInDegrees=True)\n",
        "        orn = angle.copy()\n",
        "\n",
        "        logging.debug(\"mag shape :\" + str(mag.shape))\n",
        "        logging.debug(\"orn shape :\" + str(orn.shape))\n",
        "\n",
        "        # constructing matrices of max dimension\n",
        "        mag_max = np.zeros((mag.shape[0], mag.shape[1]))\n",
        "        orn_max = np.zeros((orn.shape[0], orn.shape[1]))\n",
        "        for i in range(mag.shape[0]):\n",
        "            for j in range(mag.shape[1]):\n",
        "                mag_max[i,j] = mag[i,j].max()\n",
        "                idx = np.argmax(mag[i,j])\n",
        "                orn_max[i,j] = orn[i,j,idx] \n",
        "\n",
        "        # mag_max = mag_max.T    \n",
        "        # orn_max = orn_max.T \n",
        "\n",
        "        return mag_max, orn_max\n",
        "\n",
        "    def get_cells_mag_orn(self, y_start, x_start, cell_h, cell_w):\n",
        "        '''\n",
        "        returns the cell magnitude, orientation and \"clipped\" orientation \n",
        "        ( where 0 -> 360 is mapped into 0 -> 180)\n",
        "        '''\n",
        "        cell_mag = np.zeros((cell_h,cell_w))\n",
        "        cell_orn = np.zeros((cell_h,cell_w))\n",
        "        for i in range(cell_h):\n",
        "            for j in range(cell_w):\n",
        "                cell_mag[i,j] = self.mag_max[y_start+i, x_start+j]\n",
        "                cell_orn[i,j] = round(self.orn_max[y_start+i,x_start+j])\n",
        "        \n",
        "        cell_orn_clipped = cell_orn.copy() \n",
        "        cell_orn_clipped = ((cell_orn_clipped) + 90 ) % 360\n",
        "        for i in range(cell_h):\n",
        "            for j in range(cell_w):\n",
        "                if 0 <= cell_orn_clipped[i,j] < 180:\n",
        "                    cell_orn_clipped[i,j] = 180 - cell_orn_clipped[i,j]\n",
        "                elif 180 <= cell_orn_clipped[i,j] <=360:\n",
        "                    cell_orn_clipped[i,j] = 180 - cell_orn_clipped[i,j] % 180\n",
        "\n",
        "        return cell_mag.T, cell_orn.T, cell_orn_clipped.T\n",
        "    \n",
        "    def fill_bins_one_pixel(self, mag, orn, bin_list, implementation_type = \"skimage\"):\n",
        "        '''\n",
        "        # mag: magnitude of the gradient of 1 px\n",
        "        # orn: orientation of the gradient of 1 px\n",
        "        bin_list: reference, list of bins that is incremented\n",
        "        '''\n",
        "        N_BUCKETS = len(bin_list)\n",
        "        assert N_BUCKETS == 9, \"N_BUCKETS is not 9!!!\"\n",
        "        size_bin = 20.\n",
        "        if implementation_type == \"learnopencv\":\n",
        "            if orn >= 160:\n",
        "                left_bin = 8\n",
        "                right_bin = 9\n",
        "                left_val= mag * (right_bin * 20 - orn) / 20\n",
        "                right_val = mag * (orn - left_bin * 20) / 20\n",
        "                left_bin = 8\n",
        "                right_bin = 0\n",
        "            else:\n",
        "                left_bin = int(orn / size_bin)\n",
        "                right_bin = (int(orn / size_bin) + 1) % N_BUCKETS\n",
        "                left_val= mag * (right_bin * 20 - orn) / 20\n",
        "                right_val = mag * (orn - left_bin * 20) / 20\n",
        "            \n",
        "            assert left_val >= 0, \"leftval = \" + str(left_val) + \", \" + str(\"mag = \") + str(mag) + \" & orn = \" + str(orn)\n",
        "            assert right_val >= 0, \"rightval = \" + str(right_val) + \", \" + str(\"mag = \") + str(mag) + \" & orn = \" + str(orn)\n",
        "\n",
        "            # print(left_val)\n",
        "            # print(right_val)\n",
        "\n",
        "            bin_list[left_bin] += left_val\n",
        "            bin_list[right_bin] += right_val\n",
        "\n",
        "        elif implementation_type == \"skimage\":\n",
        "            # easiest \n",
        "            '''\n",
        "            this implementation mimics the one from skimage\n",
        "            '''\n",
        "\n",
        "            if 0 <= orn <= 10:\n",
        "                bin_list[4] += mag\n",
        "            elif 10 < orn <= 30:\n",
        "                bin_list[3] += mag\n",
        "            elif 30 < orn <= 50:\n",
        "                bin_list[2] += mag\n",
        "            elif 50 < orn <= 70:\n",
        "                bin_list[1] += mag\n",
        "            elif 70 < orn <= 90:\n",
        "                bin_list[0] += mag\n",
        "            elif 90 < orn <= 110:\n",
        "                bin_list[8] += mag \n",
        "            elif 110 < orn <= 130:\n",
        "                bin_list[7] += mag\n",
        "            elif 130 < orn <= 150:\n",
        "                bin_list[6] += mag\n",
        "            elif 150 < orn <= 170:\n",
        "                bin_list[5] += mag \n",
        "            elif 170 < orn <= 180:\n",
        "                bin_list[4] += mag \n",
        "            else:\n",
        "                raise RuntimeError(\"Impossible ! > \" + str(orn))\n",
        "        else:\n",
        "            raise NotImplementedError\n",
        "\n",
        "\n",
        "\n",
        "    def compute_hog_bins(self, y_start, x_start, cell_h, cell_w, show_src = True, show_results=True, figsize = (12,4)):\n",
        "        '''\n",
        "        y_start: y value of the top left pixel\n",
        "        x_start: x value of the top left pixel\n",
        "        cell_h : height of the cell in which HOG is computed\n",
        "        cell_w : width of the cell in which HOG is computed\n",
        "        '''\n",
        "        cell_img = self.img[y_start:y_start + cell_h, x_start:x_start+cell_w]\n",
        "\n",
        "        if show_src:\n",
        "            tmp = self.img.copy()\n",
        "            cv2.rectangle(tmp, (x_start-1, y_start-1), (x_start+cell_w+1, y_start+cell_h+1), (0,255,0))\n",
        "\n",
        "            fig, ax = plt.subplots(1,1, figsize = (figsize[1],figsize[1]))\n",
        "            ax.imshow(cv2.cvtColor(tmp, cv2.COLOR_BGR2RGB))\n",
        "            ax.set_title(\"Selection of a cell\")\n",
        "\n",
        "            plt.show()\n",
        "\n",
        "        # construction of the magnitude and orn matrices\n",
        "        cell_mag, cell_orn, cell_orn_clipped = self.get_cells_mag_orn(y_start, x_start, cell_h, cell_w)\n",
        "\n",
        "        number_of_bins = 9\n",
        "        bin_list = np.zeros(number_of_bins)\n",
        "        for i in range(cell_h):\n",
        "            for j in range(cell_w):\n",
        "                # m = round(mag_normalized[y_start+j,x_start+i].max())\n",
        "                m = cell_mag[i,j]\n",
        "                d = cell_orn_clipped[i,j]\n",
        "                # print(\"m,d =\" + str((m,d)))\n",
        "                self.fill_bins_one_pixel(m,d,bin_list)\n",
        "        \n",
        "        # logging.debug(\"Bins computed:\" + str(bin_list))\n",
        "        n = np.linalg.norm(bin_list)\n",
        "        bin_norms = bin_list/n\n",
        "        # logging.debug(\"Bins normalized:\" + str(bin_norms))\n",
        "             \n",
        "        if show_results:\n",
        "            fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=figsize, sharex=True, sharey=True) \n",
        "\n",
        "            # ax1 => just to visually represent the arrows\n",
        "            for i in range(cell_h):\n",
        "                for j in range(cell_w):\n",
        "\n",
        "                    radius = cell_mag[i,j] / (cell_mag.max() - cell_mag.min())\n",
        "                    angle_ = cell_orn[i,j]\n",
        "                    \n",
        "                    orn_value_clipped = cell_orn_clipped[i,j]\n",
        "\n",
        "                    mag_value = round(cell_mag[i,j])\n",
        "                    \n",
        "                    ax1.arrow(i, j, radius*np.cos(np.deg2rad(angle_)), radius*np.sin(np.deg2rad(angle_)), head_width=0.15, head_length=0.15, fc='b', ec='b')\n",
        "                    ax2.text(i, j, str(orn_value_clipped.astype(np.int64)), fontsize=10,va='center', ha='center')\n",
        "                    ax3.text(i, j, str(mag_value.astype(np.int64)), fontsize=10,va='center', ha='center')\n",
        "\n",
        "            ax1.imshow(cv2.cvtColor(cell_img, cv2.COLOR_BGR2RGB))\n",
        "            ax1.set_title('Input image') \n",
        "\n",
        "            ax2.matshow(cell_orn_clipped, alpha=0)\n",
        "            ax2.set_title('Orientation values')\n",
        "\n",
        "            ax3.set_title('Magnitude values')\n",
        "            intersection_matrix = np.ones(cell_mag.shape)\n",
        "            ax3.matshow(cell_mag, alpha = 0)\n",
        "           \n",
        "            plt.show()\n",
        "        return bin_list, bin_norms"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oMB6GhIcLi2C",
        "colab_type": "text"
      },
      "source": [
        "###HOG How-to, Step1: Preprocessing the image\n",
        "\n",
        "Usually, the size of an image is not appropriate to perform the HOG computation. The easiest thing is just to resize the image to an appropriate size. In this tutorial, we use a multiple of 8 and a square image. Considering the smallest face cropped, we select 64 x 64 pixels.\n",
        "\n",
        "Furthermore, as often in image processing, the largest the image, the more resource consuming it is. Keeping a reasonably small image helps in having a decent computation time.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o42F21fRN7Fe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "logging.info(\"Toy Example\")\n",
        "\n",
        "img = faces_cropped[personA][0].copy()\n",
        "resized_img = cv2.resize(img, (64,64))\n",
        "logging.info(\"Shape of source  face: \" + str(img.shape))\n",
        "logging.info(\"Shape of resized face: \" + str(resized_img.shape))\n",
        "\n",
        "\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(8, 4), sharex=False, sharey=False) \n",
        "\n",
        "ax1.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
        "ax1.set_title('Input image') \n",
        "\n",
        "ax2.imshow(cv2.cvtColor(resized_img, cv2.COLOR_BGR2RGB))\n",
        "ax2.set_title('Resized image') \n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ufx19W7JePys",
        "colab_type": "text"
      },
      "source": [
        "###HOG How-to, Step2: Computing the gradient for all pixels\n",
        "\n",
        "Computing the gradient in horizontal ($x$) and vertical ($y$) directions can be done using a pass of the Sobel Filter, part of the *openCV* library. \n",
        "This is implemented in the `MyHog.compute_gradients()` function (see implementation of `MyHog` class, above). From these gradients in $x$ and $y$ we can derive the magnitudes and orientations in all pixels using the formulas:\n",
        "> $\n",
        "\\begin{align} \n",
        "mag &= \\sqrt{g_x^2 + g_y^2} \\\\ \n",
        "orn &= atan(\\frac{g_y}{g_x}) \n",
        "\\end{align}\n",
        "$\n",
        "\n",
        "This is implemented using *openCV* library with `cartToPolar`.\n",
        "\n",
        "#####*Grayscale or Colored image*\n",
        "> For grayscale image, every pixel has one value so that this computation is straightforward. For colored image, a pixel has three values (one for Red, one for Green, one for Blue). In the HOG algorithm, the gradients are computed for all three channels, and the final magnitude is the maximum of the three, and the orientation is the one corresponding to the magnitude.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hQiWXisWJItc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "Create an object MyHog, which takes a resized image as input, and compute its \n",
        "gradients.\n",
        "'''\n",
        "myhog = MyHog(resized_img)\n",
        "\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(8, 4), sharex=True, sharey=True) \n",
        "\n",
        "ax1.imshow(myhog.mag_max, cmap = plt.cm.jet)\n",
        "ax1.set_title('Max of magnitude') \n",
        "\n",
        "ax2.imshow(myhog.orn_max, cmap = plt.cm.jet)\n",
        "ax2.set_title('Max of Orientation') \n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IQxPKrGhizSK",
        "colab_type": "text"
      },
      "source": [
        "As visible on the magnitude and orientation plots above, only essential information regarding the edges is kept. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lSvgxKzkiiWi",
        "colab_type": "text"
      },
      "source": [
        "###HOG How-to, Step3: Compute histograms for cells\n",
        "\n",
        "The histograms are first computed for small cells. It has two major benefits\n",
        "1. the representation is more compact: Suppose we take cells of $8 \\times 8$ pixels. The gradient of each pixel is described using 2 numbers (magnitude and orientation), leading to 128 numbers. Considering an histogram applied on such a cell allows to represent those 128 numbers by a tiny array of typically 9 numbers. In total a colored image of $64 \\times 64$ pixels is represented using $9*8*8$ vector.\n",
        "2. the representation is less sensitive to noise, as applying a cell is equivalent to a low-pass filter. Higher frequency outliers are therefore of less importance.\n",
        "\n",
        "\n",
        "The choice of the cell size is a design choice that can be modified. In a later section, we will modify this parameter to see how it can affect the results. \n",
        "In the paper that first presented the technique, a cell of $8 \\times 8$ pixels was used - we will continue with this (hyper-)parameter. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OziV9UK_rMGp",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "> *You may try yourself to modify the cell size or the x_start or y_start values, to see the influence on the histogram computed for that particular cell*\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bkapi9krhA_l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "Size of a cell\n",
        "'''\n",
        "cell_h = 8\n",
        "cell_w = 8\n",
        "\n",
        "'''\n",
        "starting point of the cell on the image\n",
        "'''\n",
        "y_start = 3 * cell_h - 1\n",
        "x_start = 2 * cell_w - 1\n",
        "\n",
        "hog_val, hog_val_normalized = myhog.compute_hog_bins(y_start, x_start, cell_h, cell_w, show_results=True, figsize=(16,5))\n",
        "\n",
        "fig, ax = plt.subplots(1,1, figsize = (2*5, 5))\n",
        "ax.bar([\"]70-90]\",\"]50-70]\", \"]30-50]\", \"]10-30]\",\"]10-\\n180]\",\"]150-\\n170]\",\"]130-\\n150]\",\"]110-\\n130]\", \"]90-\\n110]\"], hog_val_normalized)\n",
        "ax.set_title(\"Histogram computed with MyHog (homemade)\")\n",
        "plt.show()\n",
        "logging.info(\"MyHog bins   normalized  : {}, {}, {}, {}, {}, {}, {}, {}, {}\".format(*np.round(hog_val_normalized,2)))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2BFakzS91ki5",
        "colab_type": "text"
      },
      "source": [
        "**Legend of the above images**\n",
        "1. source image with *cell* visible in flashy green,\n",
        "2. details of the gradients computations\n",
        "    - *leftmost*: cell enlarged with an arrow indicating the gradient: length of the arrow represent the magnitude, and orientation is the gradient orientation computed on that pixel\n",
        "    - *middle*: matrix (shape == cell) containing the orientations computed (unsigned)\n",
        "    - *rightmost*: matrix (shape == cell) containung the magnitude computed\n",
        "3. histogram computed (`keyword = \"skimage\"`)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NbKkGHW9ru-I",
        "colab_type": "text"
      },
      "source": [
        "####In details\n",
        "\n",
        "The details of building the histogram for a cell is not complicated:\n",
        "- consider N bins. N is a design parameter, and each of the bins represent a range of gradient orientations. I have chosen N = 9, following the [introducing paper](http://lear.inrialpes.fr/people/triggs/pubs/Dalal-cvpr05.pdf), as it induces a granularity fine enough to observe change in the picture.\n",
        "> The HOG is usually applied using **unsigned gradients**. The numbers on the orientation matrix are between 0 and 180 instead of 0 and 360 degrees. Concretely, an angle $\\alpha [deg] $ and $(180 + \\alpha) [deg]$ contribute to the same bin. Empirically, it has been observed that it wasn't decreasing performance in the detection. Of course, nothing forbids to use signed gradients. \n",
        "- for a particular pixel:\n",
        "    - the bin is selected according to the orientation of the gradient; \n",
        "    - the value that goes in the bin is based on the magnitude. Different methods were proposed by the [introducing paper](http://lear.inrialpes.fr/people/triggs/pubs/Dalal-cvpr05.pdf), and are also explained in details for instance in [vidha blog](https://www.analyticsvidhya.com/blog/2019/09/feature-engineering-images-introduction-hog-feature-descriptor/). The class `MyHog` above contains two implementation: either the magnitude is split proportionnaly between two bins (as described in [learnopencv](https://www.learnopencv.com/histogram-of-oriented-gradients/), or -- as done in *openCV* library --, the whole magnitude is assigned to the closest bin. \n",
        "\n",
        "While this step is not difficult, let's realize in image how it's done!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jtmUfkv3XqlQ",
        "colab_type": "text"
      },
      "source": [
        "####Creation of dedicated images\n",
        "\n",
        "The following function allows creating images \"on demand\", in order to better understand the histogram computation. The parameter \"special\" indicate what type of image is required."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4RE2qpxGt9us",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "CREATE_IMAGE returns an image created based on a keyword. \n",
        "'''\n",
        "def create_image(height, width, special=None):\n",
        "\n",
        "    mat0 = np.ones((height, width), dtype=np.uint8)*255\n",
        "    mat1 = np.ones((height, width), dtype=np.uint8)*255\n",
        "    mat2 = np.ones((height, width), dtype=np.uint8)*255\n",
        "\n",
        "    if special == \"center_black\":\n",
        "        mat0[height//2-1:height//2+1, width//2-1 : width//2+1 ] = 0\n",
        "        mat1[height//2-1:height//2+1, width//2-1 : width//2+1 ] = 0\n",
        "        mat2[height//2-1:height//2+1, width//2-1 : width//2+1 ] = 0\n",
        "    elif special == \"center_gray\":\n",
        "        mat0[height//2-3:height//2+3, width//2-3 : width//2+3 ] = 125\n",
        "        mat1[height//2-3:height//2+3, width//2-3 : width//2+3 ] = 125\n",
        "        mat2[height//2-3:height//2+3, width//2-3 : width//2+3 ] = 125\n",
        "        mat0[height//2-1:height//2+1, width//2-1 : width//2+1 ] = 0\n",
        "        mat1[height//2-1:height//2+1, width//2-1 : width//2+1 ] = 0\n",
        "        mat2[height//2-1:height//2+1, width//2-1 : width//2+1 ] = 0\n",
        "    elif special == \"90\":\n",
        "        mat0[0:height, width//2 : width ] = 0\n",
        "        mat1[0:height, width//2 : width ] = 0\n",
        "        mat2[0:height, width//2 : width ] = 0\n",
        "    elif special == \"180\":\n",
        "        mat0[height//2:height, 0 : width ] = 0\n",
        "        mat1[height//2:height, 0 : width ] = 0\n",
        "        mat2[height//2:height, 0 : width ] = 0\n",
        "    elif special == \"135\":\n",
        "        for i in range(height):\n",
        "            for j in range(i,width):\n",
        "                mat0[i,j] = mat1[i,j] = mat2[i,j] = 0\n",
        "    elif special == \"45\":\n",
        "        for i in range(height):\n",
        "            for j in range(width-i-1,width):\n",
        "                mat0[i,j] = mat1[i,j] = mat2[i,j] = 0\n",
        "    elif special == \"28_34_37\":\n",
        "        mat0[4, 6:8] = 200\n",
        "        mat0[5, 4:8] = 150\n",
        "        mat0[6, 2:8] = 100\n",
        "        mat0[7, 0:8] = 50\n",
        "        mat1 = mat0.copy()\n",
        "        mat2 = mat0.copy()\n",
        "    elif special == \"up_01\":\n",
        "        mat0[4, 4:8] = 250\n",
        "        mat0[5, 0:8] = 50\n",
        "        mat0[6, 0:8] = 50\n",
        "        mat0[7, 0:8] = 50\n",
        "        mat1 = mat0.copy()\n",
        "        mat2 = mat0.copy()\n",
        "    elif special == \"up_10\":\n",
        "        mat0[4, 4:8] = 220\n",
        "        mat0[5, 0:8] = 50\n",
        "        mat0[6, 0:8] = 50\n",
        "        mat0[7, 0:8] = 50\n",
        "        mat1 = mat0.copy()\n",
        "        mat2 = mat0.copy()\n",
        "    elif special == \"up_11\":\n",
        "        mat0[4, 4:8] = 225\n",
        "        mat0[5, 0:8] = 50\n",
        "        mat0[6, 0:8] = 50\n",
        "        mat0[7, 0:8] = 50\n",
        "        mat1 = mat0.copy()\n",
        "        mat2 = mat0.copy()\n",
        "    elif special == \"up_15\":\n",
        "        mat0[4, 4:8] = 200\n",
        "        mat0[5, 0:8] = 50\n",
        "        mat0[6, 0:8] = 50\n",
        "        mat0[7, 0:8] = 50\n",
        "        mat1 = mat0.copy()\n",
        "        mat2 = mat0.copy()\n",
        "    elif special == \"up_27\":\n",
        "        mat0[4, 4:8] = 150\n",
        "        mat0[5, 0:8] = 50\n",
        "        mat0[6, 0:8] = 50\n",
        "        mat0[7, 0:8] = 50\n",
        "        mat1 = mat0.copy()\n",
        "        mat2 = mat0.copy()\n",
        "    elif special == \"up_152\":\n",
        "        mat0 = np.ones((height, width), dtype=np.uint8)*0\n",
        "        mat1 = np.ones((height, width), dtype=np.uint8)*0\n",
        "        mat2 = np.ones((height, width), dtype=np.uint8)*0\n",
        "        mat0[0,6:7] = mat1[0,6:7] = mat2[0,6:7] = 255\n",
        "        mat0[1,7] = mat1[1,7] = mat2[1,7] = 135\n",
        "    elif special == \"down_153\":\n",
        "        mat0 = np.ones((height, width), dtype=np.uint8)*255\n",
        "        mat1 = np.ones((height, width), dtype=np.uint8)*255\n",
        "        mat2 = np.ones((height, width), dtype=np.uint8)*255\n",
        "        mat0[0,6:7] = mat1[0,6:7] = mat2[0,6:7] = 0\n",
        "        mat0[1,7] = mat1[1,7] = mat2[1,7] = 125\n",
        "    elif special == \"up_141\":\n",
        "        mat0 = np.ones((height, width), dtype=np.uint8)*0\n",
        "        mat1 = np.ones((height, width), dtype=np.uint8)*0\n",
        "        mat2 = np.ones((height, width), dtype=np.uint8)*0\n",
        "        mat0[0,6:7] = mat1[0,6:7] = mat2[0,6:7] = 255\n",
        "        mat0[1,7] = mat1[1,7] = mat2[1,7] = 210\n",
        "    elif special == \"up_111\":\n",
        "        mat0 = np.ones((height, width), dtype=np.uint8)*0\n",
        "        mat1 = np.ones((height, width), dtype=np.uint8)*0\n",
        "        mat2 = np.ones((height, width), dtype=np.uint8)*0\n",
        "        mat0[0,0:7] = mat1[0,0:7] = mat2[0,0:7] = 100\n",
        "        mat0[1,7] = mat1[1,7] = mat2[1,7] = 255\n",
        "    elif special == \"personA\":\n",
        "        mat0 = np.ones((height, width), dtype=np.uint8)*122\n",
        "        mat1 = np.ones((height, width), dtype=np.uint8)*122\n",
        "        mat2 = np.ones((height, width), dtype=np.uint8)*122\n",
        "        for i in range(24):\n",
        "            for j in range(40, width):\n",
        "                    if j >= (i+40):\n",
        "                        mat0[i,j] = mat1[i,j] = mat2[i,j] = 10\n",
        "    elif special == \"personB\":\n",
        "        mat0 = np.ones((height, width), dtype=np.uint8)*122\n",
        "        mat1 = np.ones((height, width), dtype=np.uint8)*122\n",
        "        mat2 = np.ones((height, width), dtype=np.uint8)*122\n",
        "        mat0[:,50:width] = 10\n",
        "        mat1[:,50:width] = 10\n",
        "        mat2[:,50:width] = 10\n",
        "    image = np.dstack((mat0, mat1, mat2))\n",
        "    return image"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J-N7rgcjwvB7",
        "colab_type": "text"
      },
      "source": [
        "####Examples of histogram computed on created images\n",
        "In order to understand how the bins are fulled, let's look at a few of simple images. Those images are $8 \\times 8$, meaning 1 cell == 1 image\n",
        "\n",
        "* pure 90 gradient\n",
        "* pure 180 gradient\n",
        "* diagonal: 45\n",
        "* diagonal: 135\n",
        "\n",
        "For each case, we plot:\n",
        "1. the arrows representing the gradients, \n",
        "2. the matrices of the magnitude and orientation values, \n",
        "3. the resulting histogram\n",
        "\n",
        "**and** we log:\n",
        "\n",
        "- the raw values of the histogram bins\n",
        "- the normalized values of the histogram bins, using *L2-Normalization*:\n",
        "$\\begin{align}\n",
        "bins\\_values &= [x_1, x_2, x_3, x_4, x_5, x_6, x_7, x_8, x_9] \\\\\n",
        "\\|bins\\_values\\| &= \\sqrt{x_1^2 + x_2^2 + x_3^2 + x_4^2 + x_5^2 + x_6^2 + x_7^2 + x_8^2 + x_9^2 } \\\\\n",
        "bins\\_values_{normalized} &=  \\frac{v}{\\|v\\|} \\\\ \n",
        "&= \\Bigg[ \\frac{x_1}{\\|v\\|}, \\frac{x_2}{\\|v\\|}, \\frac{x_3}{\\|v\\|},  \\frac{x_4}{\\|v\\|}, \\frac{x_5}{\\|v\\|}, \\frac{x_6}{\\|v\\|},  \\frac{x_7}{\\|v\\|}, \\frac{x_8}{\\|v\\|}, \\frac{x_9}{\\|v\\|} \\Bigg]\n",
        "\\end{align}$\n",
        "\n",
        "\n",
        "\n",
        "#### Validation of intuition\n",
        "To prove ourselves our implementation and understanding is correct, we will confront the results with the infamous library `skimage`, using `skimage.feature.hog` with the same parameters as the handmade function: 9 bins, an $8\\times 8$ cell, and 1 cell per *block* (we discuss the *blocks* in the next section). Two parameters are still unknown: transform_sqrt and multichannel \n",
        "- `transform_sqrt`: if True, then the sqrt operator is applied to the global image first. It can give better results. We can safely leave it to False for the purpose of this exercices with the HOG bins...\n",
        "- `multichannel`: simply indicates if the image is grayscale (`multichannel = False`) or in color (`multichannel = True`) \n",
        "\n",
        "<!--Note: we briefly discussed the `block_norm` parameter, but more to come in the next step.-->\n",
        "\n",
        "####Finally...\n",
        "Coming back to the very first example of the HOG, we saw the Emma Stone picture with weird white-ish pictograms describing her face... Well, thanks to the `skimage` library, it's very easy to get this image, and we show it for the toy example we are studying now. It allows grabbing the full overview of how, eventually, the complete histogram and visualization is computed.\n",
        "\n",
        "> *Of course, don't hesitate to change yourself the list of images that are analyzed, considering the list implemented. You find the keywords accepted in the special list*\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zzqs9qmavSyr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "Creation of the images\n",
        "'''\n",
        "special = [\"up_01\",\"45\",\"90\", \"135\",\"180\",\"up_10\",\"up_11\",\"up_15\", \"up_27\", \"28_34_37\", \"up_111\", \"up_141\", \"up_152\", \"down_153\",\"center_black\", \"center_gray\"]\n",
        "list_as_example = [\"90\", \"180\", \"45\", \"135\", \"28_34_37\"]\n",
        "\n",
        "# created_img = create_image(cell_h,cell_w, \"45\")\n",
        "\n",
        "'''\n",
        "Homemade implementation of the histogram\n",
        "and\n",
        "Validation with an optimized library\n",
        "'''\n",
        "for keyword in list_as_example:\n",
        "    logging.info(\"Considering image: \" + str(keyword))\n",
        "\n",
        "    # creation of the simple image\n",
        "    created_img = create_image(cell_h, cell_w, keyword)\n",
        "\n",
        "    # creation of MyHog object\n",
        "    toyhog=MyHog(created_img)\n",
        "\n",
        "    # compute bins using MyHog\n",
        "    y_start_loc = 0\n",
        "    x_start_loc = 0\n",
        "    bins, bins_normalized = toyhog.compute_hog_bins(y_start_loc, x_start_loc, cell_h, cell_w, show_src=False, show_results=True, figsize = (14,4))\n",
        "\n",
        "    # compute bins using Skimage \n",
        "    fd, hog_image = skimage_feature_hog(created_img, \n",
        "                orientations=9, \n",
        "                pixels_per_cell=(8,8), \n",
        "                cells_per_block=(1, 1), \n",
        "                block_norm = \"L2\",\n",
        "                visualize=True, \n",
        "                transform_sqrt = False,\n",
        "                multichannel=True)\n",
        "\n",
        "    # plot results from Skimage\n",
        "    plt.figure()\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 4), sharex=False, sharey=False) \n",
        "\n",
        "    # Rescale hog for better display \n",
        "    hog_image_rescaled = exposure.rescale_intensity(hog_image, in_range=(0, 10)) \n",
        "\n",
        "\n",
        "    xlabels = [\"]70-90]\",\"]50-70]\", \"]30-50]\", \"]10-30]\",\"]10-\\n180]\",\"]150-\\n170]\",\"]130-\\n150]\",\"]110-\\n130]\", \"]90-\\n110]\"]\n",
        "    x = np.arange(9)\n",
        "    w = 0.2\n",
        "    ax1.bar( x-w, bins_normalized,  width=2*w, align='center',color=\"b\")\n",
        "    ax1.bar( x+w, fd, width=2*w, align='center',color=\"r\")\n",
        "   \n",
        "    ax1.set_title(\"Histogram computed by Skimage.feature.hog\")\n",
        "    ax1.legend([\"MyHog (homemade)\", \"Skimage.feature.hog\"])\n",
        "    # start, end = ax.get_xlim()\n",
        "    # ax.xaxis.set_ticks(np.arange(start, end, 1))\n",
        "    # ax1.set_xticklabels(xlabels)\n",
        "    ax1.set_xticks(x)\n",
        "    ax1.set_xticklabels(xlabels)\n",
        "\n",
        "    ax2.imshow(hog_image_rescaled, cmap=plt.cm.gray) \n",
        "    ax2.set_title('Histogram of Oriented Gradients - visual')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "\n",
        "    logging.info(\"MyHog bins     computed  : {}, {}, {}, {}, {}, {}, {}, {}, {}\".format(*np.round(bins,2)))\n",
        "    logging.info(\"MyHog bins   normalized  : {}, {}, {}, {}, {}, {}, {}, {}, {}\".format(*np.round(bins_normalized,2)))\n",
        "    logging.info(\"Skimage bins normalized  : {}, {}, {}, {}, {}, {}, {}, {}, {}\".format(*np.round(fd,2)))\n",
        "    logging.info(\"***\"*30)\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "co65Rf5VLdZ8",
        "colab_type": "text"
      },
      "source": [
        "> Notice: as a reminder, the purpose of the `MyHog` class (or any of the other class from this tutorial) is definitely not to reproduce exactly the behavior of a well-known and optimized library, but solely to break the magic behind using a library function without understanding the algorithm behind. As a result, the HOG computed may differ in several ways."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B5u575_gKGKy",
        "colab_type": "text"
      },
      "source": [
        "####Coming back to our initial cell...\n",
        "Emma Stone cell defined above can now be shown in terms of HOG, helped by the `skimage` library.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HH2aQX3NRd4P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "Retrieving the cell defined above\n",
        "'''\n",
        "cell_img=resized_img[y_start:y_start + cell_h, x_start:x_start+cell_w]\n",
        "\n",
        "'''\n",
        "computing HOG of the cell using same parameters\n",
        "'''\n",
        "fd, hog_image = skimage_feature_hog(cell_img, \n",
        "                    orientations=9, \n",
        "                    pixels_per_cell=(8,8), \n",
        "                    cells_per_block=(1, 1), \n",
        "                    block_norm = \"L2\",\n",
        "                    visualize=True, \n",
        "                    transform_sqrt = False,\n",
        "                    multichannel=True)\n",
        "\n",
        "\n",
        "\n",
        "plt.figure()\n",
        "\n",
        "fig, (ax0, ax1, ax2) = plt.subplots(1, 3, figsize=(12, 4), sharex=False, sharey=False) \n",
        "# hog_cropped = hog_image[y_start:y_start + cell_h, x_start:x_start+cell_w]\n",
        "ax0.imshow(cv2.cvtColor(resized_img, cv2.COLOR_BGR2RGB))\n",
        "ax0.set_title('Input image') \n",
        "\n",
        "ax1.imshow(cv2.cvtColor(cell_img, cv2.COLOR_BGR2RGB))\n",
        "ax1.set_title('Extracted cell') \n",
        "\n",
        "# Rescale histogram for better display \n",
        "hog_image_rescaled = exposure.rescale_intensity(hog_image, in_range=(0, 10)) \n",
        "ax2.imshow(hog_image_rescaled, cmap=plt.cm.gray) \n",
        "ax2.set_title('Histogram of Oriented Gradients - rescaled')\n",
        "# print(hog_image_rescaled)\n",
        "plt.show()\n",
        "\n",
        "logging.info(\"Skimage bins normalized  : {}, {}, {}, {}, {}, {}, {}, {}, {}\".format(*np.round(fd,2)))\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jiAjftDx2Nxm",
        "colab_type": "text"
      },
      "source": [
        "This is the end of the Step3: the computation of the histogram for one cell! A careful eye will have seen the parameters `cells_per_block` and `block_norm` of the library method. This is linked to Step4: Block Normalization!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OXLInz6551Ak",
        "colab_type": "text"
      },
      "source": [
        "###HOG How-to, Step4: Block normalization\n",
        "\n",
        "In the Step3, we have extensively seen how to compute the histogram of gradients for a cell. We are almost at the end of the feature representation build up, but there are yet one normalization step. \n",
        "> In the previous step, we actually already normalized the histogram values using *L2-Normalization*. This is a simple case of the Block normalization where there is 1 cell per block. In general, we can define to perform Block normalization on more than 1 block. A common value is 4, as discussed in the [introducing paper](http://lear.inrialpes.fr/people/triggs/pubs/Dalal-cvpr05.pdf). \n",
        "\n",
        "####<u>Why do we need normalization ?</u>\n",
        " \n",
        "When normalized, a histogram becomes independant from the lighting variation. \n",
        "Indeed, illumination has the impact of increasing/decreasing the values of the pixels in a cell. \n",
        "\n",
        "Using normalization, a change on the pixel value has no impact if all the pixels in a cell are subject to the same change. \n",
        "> let's say a low illumination make the pixel values divided by two. Having a normalized histogram on the cell will not be affected by such a change, as in the end, the absolute value is not important: only the relative value of one pixel to others matter. This is the very essence of the normalization.\n",
        "\n",
        "As a result, normalizing the histogram makes it quite independant of the lighting condition (provided that in a cell, all the pixels have the same lighting condition, which seems a sensible assumption).\n",
        "\n",
        "####<u>Normalizing by block</u> \n",
        "A nice visualization of the normalization by block of multiple cells is given in [learnopencv](https://www.learnopencv.com/wp-content/uploads/2016/12/hog-16x16-block-normalization.gif) where we see in green the different cells, and in blue a block of 4 cells. \n",
        "Using a block normalization - so, normalizing multiple cells at ones, and slide the normalization window across the image - is introduced in the [introducing paper](http://lear.inrialpes.fr/people/triggs/pubs/Dalal-cvpr05.pdf) which shows some benefits in terms of missing rate. Typically, 4 cells per blocks is often used. In a later section (see Classification), a grid search tends to try out other block sizes.\n",
        "\n",
        "####<u>What normalization ?</u>\n",
        "Several normalization can be considered: *L1*, *L2*, *L2-Hys*, ...\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jc4AH7fJp63w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "Definition of the block size (in number of px)\n",
        "1 cell = 8 x 8\n",
        "1 block => 16 x 16 => 4 cells\n",
        "'''\n",
        "block_w = 16\n",
        "block_h = 16\n",
        "\n",
        "x_cells = np.arange(0,64,8)\n",
        "y_cells = np.arange(0,64,8)\n",
        "\n",
        "# credit: https://stackoverflow.com/questions/44816682/drawing-grid-lines-across-the-image-uisng-opencv-python\n",
        "def draw_grid(img, line_color=(0, 255, 0), thickness=1, type_=8, pxstep=8):\n",
        "    '''(ndarray, 3-tuple, int, int) -> void\n",
        "    draw gridlines on img\n",
        "    line_color:\n",
        "        BGR representation of colour\n",
        "    thickness:\n",
        "        line thickness\n",
        "    type:\n",
        "        8, 4 or cv2.LINE_AA\n",
        "    pxstep:\n",
        "        grid line frequency in pixels\n",
        "    '''\n",
        "    x = pxstep\n",
        "    y = pxstep\n",
        "    while x < img.shape[1]:\n",
        "        cv2.line(img, (x, 0), (x, img.shape[0]), color=line_color, lineType=type_, thickness=thickness)\n",
        "        x += pxstep\n",
        "\n",
        "    while y < img.shape[0]:\n",
        "        cv2.line(img, (0, y), (img.shape[1], y), color=line_color, lineType=type_, thickness=thickness)\n",
        "        y += pxstep\n",
        "\n",
        "def draw_one_block(img, origin=(0,0), line_color=(255,0, 0), block_size = 16, thickness=1, type_=8):\n",
        "    cv2.rectangle(img, origin, (origin[0]+block_size, origin[1]+block_size), line_color, thickness=thickness, lineType =type_)\n",
        "\n",
        "def draw_all_blocks(img, thickness):\n",
        "    color_list = [(255,0,0), (255,255,0), (255,0,255)]\n",
        "    x = 0\n",
        "    y = 0\n",
        "    counter = 0\n",
        "    while x < img.shape[1]-8:\n",
        "        # cv2.line(img, (x, 0), (x, img.shape[0]), color=line_color, lineType=type_, thickness=thickness)\n",
        "        # draw_one_block(img, (x,y))\n",
        "        \n",
        "        \n",
        "        while y < img.shape[0]-8:\n",
        "            # cv2.line(img, (0, y), (img.shape[1], y), color=line_color, lineType=type_, thickness=thickness)\n",
        "            lc = color_list[counter%3]\n",
        "            draw_one_block(img, (x,y),line_color=lc, thickness=thickness)\n",
        "            counter += 1\n",
        "            y += 8\n",
        "        y=0\n",
        "        x += 8\n",
        "    return counter\n",
        "\n",
        "'''\n",
        "creating a green grid covering the resized image\n",
        "'''\n",
        "\n",
        "grid_cells_img = resized_img.copy()\n",
        "draw_grid(grid_cells_img, type_=8)\n",
        "\n",
        "'''\n",
        "Creating the three first blocks\n",
        "'''\n",
        "\n",
        "first_block_img = grid_cells_img.copy()\n",
        "second_block_img = grid_cells_img.copy()\n",
        "third_block_img = grid_cells_img.copy()\n",
        "\n",
        "draw_one_block(first_block_img, origin=(0,0), line_color=(255,0,0), thickness=2)\n",
        "draw_one_block(second_block_img, origin=(8,0), line_color=(255,255,0),thickness=2)\n",
        "draw_one_block(third_block_img, origin=(16,0), line_color=(255,0,255),thickness=2)\n",
        "\n",
        "'''\n",
        "Creating all the blocks on top of the cells\n",
        "'''\n",
        "\n",
        "blocks_img = grid_cells_img.copy()\n",
        "counter = draw_all_blocks(blocks_img, thickness=1)\n",
        "\n",
        "'''\n",
        "Vizualization\n",
        "'''\n",
        "\n",
        "fig, (ax0, ax1, ax2, ax3) = plt.subplots(1, 4, figsize=(16, 4), sharex=False, sharey=False) \n",
        "ax0.imshow(cv2.cvtColor(grid_cells_img, cv2.COLOR_BGR2RGB))\n",
        "ax0.set_title('Cells') \n",
        "\n",
        "ax1.imshow(cv2.cvtColor(first_block_img, cv2.COLOR_BGR2RGB))\n",
        "ax1.set_title('first block') \n",
        "\n",
        "ax2.imshow(cv2.cvtColor(second_block_img, cv2.COLOR_BGR2RGB))\n",
        "ax2.set_title('second block') \n",
        "\n",
        "ax3.imshow(cv2.cvtColor(third_block_img, cv2.COLOR_BGR2RGB))\n",
        "ax3.set_title('third block') \n",
        "\n",
        "fig, ax = plt.subplots(1,1,figsize=(4,4))\n",
        "ax.imshow(cv2.cvtColor(blocks_img, cv2.COLOR_BGR2RGB))\n",
        "ax.set_title(\"All Blocks\")\n",
        "\n",
        "plt.show()\n",
        "\n",
        "logging.info(\"In total, there are: \" + str(counter) + \" blocks possible in the picture\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1LybYTHG0fxi",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "As shown in the previous example, on the image chosen, there are $49$ blocks possible of size $(16 \\times 16)$ pixels.\n",
        "\n",
        "###HOG How-to, Step5: concatenation\n",
        "\n",
        "After the normalization, the only step remaining is the concatenation of the computed vectors into a larger one, that represent the input image. This will be the feature representation of the image, based on the *oriented* gradients in that image.\n",
        "\n",
        "###What size is this feature representation ?\n",
        "- one cell is represented by $9$ numbers (histogram)\n",
        "- four histograms are normalized together, leading to a $(36,1)$ vector\n",
        "- there are $49$ such vectors representing the image.\n",
        "    If the image as a width of size $(w*8)$ pixels, and a height of $(h*8)$ pixels, the image dimension is $(8*h \\times  8*w)$. In such an image, they are :\n",
        "    *   h cells vertically, and w cells horizontally,\n",
        "    *  (h-1) blocks vertically and (w-1) blocks horizontally.\n",
        "\n",
        "The length of the final vector is then $36 \\cdot 49$ numbers, or a $(1764,1)$ vector.\n",
        "\n",
        "Of course, this is still large... But much more compact that our initial $(64,64,3)$ array of $12288$ numbers\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eXffH7oz8GEJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fd, hog_image = skimage_feature_hog(resized_img, \n",
        "                    orientations=9, \n",
        "                    pixels_per_cell=(8,8), \n",
        "                    cells_per_block=(2, 2), \n",
        "                    block_norm = \"L2\",\n",
        "                    visualize=True, \n",
        "                    transform_sqrt = True,\n",
        "                    multichannel=True)\n",
        "\n",
        "'''\n",
        "Plotting results\n",
        "'''\n",
        "\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 8), sharex=True, sharey=True) \n",
        "ax1.imshow(cv2.cvtColor(resized_img, cv2.COLOR_BGR2RGB))\n",
        "ax1.set_title('Input image') \n",
        "\n",
        "# Rescale histogram for better display \n",
        "hog_image_rescaled = exposure.rescale_intensity(hog_image, in_range=(0, 10)) \n",
        "ax2.imshow(hog_image_rescaled, cmap=plt.cm.gray) \n",
        "ax2.set_title('Histogram of Oriented Gradients - rescaled')\n",
        "logging.debug(\"HOG Rescaled: \" + str(hog_image_rescaled.min()) + \" -> \" + str(hog_image_rescaled.max()) )\n",
        "\n",
        "plt.show()\n",
        "\n",
        "# logging.debug(hog_image_rescaled.shape)\n",
        "logging.info(\"Shape of the HOG feature : \" + str(fd.shape))\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GuWXw_8xuTWb",
        "colab_type": "text"
      },
      "source": [
        "<!--histogram construction is based on the gradient computed - both magnitude and orientation (as defined)\n",
        "\n",
        "- the bin is selected according to the orientation (direction) of the gradient; \n",
        "- the value that goes in the bin is based on the magnitude\n",
        "\n",
        "For instance on the toy example:\n",
        "first pixel has:\n",
        "* mag = 6; orn = 45. So, the vote of this pixel goes for 75% in the bin of 40, and 25% in the bin of 60, as closer to 40. As a result, we add 4.5 to bin nb 3, and 1.5 to bin nb 4. -->"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "erQwBkq35RQ-",
        "colab_type": "text"
      },
      "source": [
        "###HOG: Exec summary\n",
        "\n",
        "* one cell (typ 8 x 8) of an image is represented by a histogram \n",
        "    * the orientation and magnitude of the gradient are computed on each pixel\n",
        "    * orientation of the gradient indicate a bin\n",
        "    * magnitude indicate the amount to place into the bin\n",
        "* the histogram is a vector of size 9 (as 9 bins)\n",
        "* one block (16 x 16) histogram is the concatenation of the 4 histograms, each representing one cell of the block, hence represented by a (36 x 1) vector.\n",
        "* the final HOG feature vector is based on the concatenation of all blocks.\n",
        "\n",
        "\n",
        "For an image of 64 x 64, it follows:\n",
        "* 8 cells along the width\n",
        "* 8 cells along the height\n",
        "* Number of cells = 64 cells\n",
        "* Number of blocks = 7 * 7 = 49 blocks\n",
        "\n",
        "Each block has a representative vector of dimension $(36 \\times 1)$, and the resulting vector has dimension $(49*36 \\times 1)$, or $(1764,)$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tfMDFGqsZvzC",
        "colab_type": "text"
      },
      "source": [
        "This ends the first part of Histogram of Oriented Gradient, where I showed in detail how to compute such a feature representation, and the meaning of the different parameters.\n",
        "\n",
        "As many other parameters, the \"hyper-parameters\" of a HOG method should always be assessed according to a specific problem. \n",
        "- Insofar, the parameters have mainly be considered equal to their suggested value by the litterature, in this [HOG paper](http://lear.inrialpes.fr/people/triggs/pubs/Dalal-cvpr05.pdf)\n",
        "- Later, in this tutorial, we will optimize a classifier using a systematic method and a cross-validation set - Stay tuned ! \n",
        "\n",
        "Recall that we shall **never** use the test set to fine tune our model parameters."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NtYBaU-bzkio",
        "colab_type": "text"
      },
      "source": [
        "<!--Still to do : \n",
        " These [hog] parameters were obtained by experimentation and examining the accuracy of the classifier  you should expect to do this as well whenever you use the HOG descriptor. Running experiments and tuning the HOG parameters based on these parameters is a critical component in obtaining an accurate classifier.-->"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "geHORWA5mMdM",
        "colab_type": "text"
      },
      "source": [
        "### Detecting an object of interest in a new image\n",
        "\n",
        "In this part, the goal is to use the HOG feature descriptor in order to detect if an object is present or not in a new image.\n",
        "\n",
        "> we are not building a classifier or identificator (yet!) The goal is *just* to detect which region of an image have structures that correspond well to the ones of the feature descriptor.\n",
        "\n",
        "\n",
        "To do so, the steps are:\n",
        "1. Select an object\n",
        "2. Compute its HOG feature description\n",
        "3. Select a new image\n",
        "4. pre-process this image in terms of dimensions\n",
        "5. compute the image HOG at every location\n",
        "6. Assess the matching between the descriptor and the image\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pDIZoHwMn_o-",
        "colab_type": "text"
      },
      "source": [
        "####Select an object and compute its hog\n",
        "\n",
        "As we have built a training set and a test set, let's just pick randomly one image of the training set. We can do even better and compute the HOG for all images in training and test sets using the parameters already seen. We store the results in the dictionary `hog_training` and `hog_test`. As *usual*, the keys are the persons names.\n",
        "\n",
        "Later, we can select any of those as the `image_of_interest`\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "guoG2jWm4Dv3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "hog_training = {}\n",
        "hog_test = {}\n",
        "\n",
        "for person in persons:\n",
        "    hog_training[person] = []\n",
        "    for src_img in training_set[person]:\n",
        "        if not color:\n",
        "            src_img = cv2.cvtColor(src_img, cv2.COLOR_BGR2GRAY)\n",
        "        resized_img = cv2.resize(src_img, (sq_size,sq_size))\n",
        "        fd, hog_image = skimage_feature_hog(resized_img, \n",
        "                                            orientations=9, \n",
        "                                            pixels_per_cell=(8,8), \n",
        "                                            cells_per_block=(2,2), \n",
        "                                            block_norm = \"L2\",\n",
        "                                            visualize=True, \n",
        "                                            transform_sqrt = True,\n",
        "                                            multichannel=color)\n",
        "        hog_training[person].append((fd, hog_image, resized_img))\n",
        "\n",
        "logging.debug(pretty_return_dict_size(hog_training))\n",
        "logging.info(\"hog_training dictionary contains the HOG descriptors (resized) for all faces of the training set\")\n",
        "\n",
        "for person in persons:\n",
        "    hog_test[person] = []\n",
        "    for src_img in test_set[person]:\n",
        "        if not color:\n",
        "            src_img = cv2.cvtColor(src_img, cv2.COLOR_BGR2GRAY)\n",
        "        resized_img = cv2.resize(src_img, (sq_size,sq_size))\n",
        "        fd, hog_image = skimage_feature_hog(resized_img, \n",
        "                                            orientations=9, \n",
        "                                            pixels_per_cell=(8,8), \n",
        "                                            cells_per_block=(2,2), \n",
        "                                            block_norm = \"L2\",\n",
        "                                            visualize=True, \n",
        "                                            transform_sqrt = True,\n",
        "                                            multichannel=color)\n",
        "        hog_test[person].append((fd, hog_image, resized_img))\n",
        "\n",
        "logging.debug(pretty_return_dict_size(hog_test))\n",
        "logging.info(\"hog_test dictionary contains the HOG descriptors (resized) for all faces of the test set\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KUtgffa1HZey",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "Let's pick an image of interest, by its index [0 -> 19]\n",
        "'''\n",
        "\n",
        "idx_of_interest = 2\n",
        "image_of_interest = training_set[personA][idx_of_interest]\n",
        "hog_of_interest = hog_training[personA][idx_of_interest]\n",
        "\n",
        "'''\n",
        "Visualization of the image and its hog selected as image_of_interest\n",
        "'''\n",
        "fig, (ax0, ax1) = plt.subplots(1,2,figsize = (8,4), sharex=False, sharey=False)\n",
        "\n",
        "ax0.imshow(cv2.cvtColor(image_of_interest, cv2.COLOR_BGR2RGB))\n",
        "ax0.set_title(\"Image of interest from training set\")\n",
        "\n",
        "ax1.imshow(hog_of_interest[1])\n",
        "ax1.set_title(\"Visualization of the HOG of interest\")\n",
        "\n",
        "logging.info(\"Shape of the descriptor       : \" + str(hog_of_interest[0].shape))\n",
        "logging.info(\"Shape of the descriptor (visu): \" + str(hog_of_interest[1].shape))\n",
        "logging.info(\"Shape of the image of interest: \" + str(image_of_interest.shape))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WnlkuONE-Qcm",
        "colab_type": "text"
      },
      "source": [
        "####Select a new image\n",
        "This is the image we want to apply the descriptor matching.\n",
        "Put in another words, we want to verify, using a distance metric such as the euclidean distance, if the HOG descriptor of my *image_of_interest* presents similarities with a region in a new image.\n",
        "\n",
        "Let's just pick a certain number of images from our **original** images downloaded, the ones that are not cropped yet, nor resized. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hBs2r9_HEWEt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "person_ = personA # can be {personA, personB, personC, personD}\n",
        "number_of_candidates = 3\n",
        "random.seed(\"7/4/2020\")\n",
        "images_candidates = random.sample(images[person_], number_of_candidates)\n",
        "\n",
        "fig = plt.figure(figsize=(12,4))\n",
        "i=0\n",
        "\n",
        "for img in images_candidates:\n",
        "    ax=fig.add_subplot(1,number_of_candidates, i+1)\n",
        "    ax.imshow( cv2.cvtColor(img, cv2.COLOR_BGR2RGB) )\n",
        "    i+=1\n",
        "plt.show()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l89dOjWmEXGU",
        "colab_type": "text"
      },
      "source": [
        "The different images selected don't have the same size."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c3PgFiwYIrQf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "logging.info(\"Shapes of image on which to look for the image_of_interest: \")\n",
        "for img in images_candidates:\n",
        "    logging.info(\"Image shape: \" + str(img.shape))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9yFQMKcxKMhf",
        "colab_type": "text"
      },
      "source": [
        "####Matching\n",
        "In this larger step, we want to see if there is a match between the descriptor of interest and the image candidate. \n",
        "\n",
        "Several problems arise:\n",
        "- The descriptor of interest has a fixed (designed) size of $(1764,1)$\n",
        "- The different image candidates have different sizes,\n",
        "- The image candidates contains more information than just a face\n",
        "\n",
        "*Why not just cropping the faces on the image and resize it?*\n",
        "\n",
        "We could of course do that - but that is not really the purpose :-) Rather, we want to assess, in **every location** of the image candidate, if there is a chance to see a pattern such as the descriptor provided of the image of interest.\n",
        "\n",
        "*Simpler case*\n",
        "Let's consider first that if the image candidate contains a face (=object), this face has approximately the same size as the original face of interest\n",
        "\n",
        "One way to proceed is to slide, across the image candidate, a window of the size of the object to detect. \n",
        "- at *each* location, crop the part of the image candidate in the sliding window\n",
        "> sliding at every each location is cumbersome and time consuming. The parameter `step`actually defines the amount of pixels that is skipped in both directions during the sliding.\n",
        "- compute the HOG of this crop\n",
        "- compare (using Euclidean distance for instance) this descriptor with the descriptor of the object to find\n",
        "- go to the next location\n",
        "\n",
        "At the end, the result is a score attributed to every location, indicating the correspondance between the object to detect, and the area in the image. \n",
        "\n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IqjJB-ImN0I7",
        "colab_type": "text"
      },
      "source": [
        "*Helper functions*\n",
        "- `get_local_crop`: realize the cropping before computing the HOG, ensuring the appropriate size to compute HOG\n",
        "- `match_hog`: homemade matching between an object of interest and an image candidate, implementing this \"sliding\" accross the image\n",
        "- `fill_matrix_min_neighbours`: compensate the effect of the `step`parameter in the plot"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "55hZpBQoGLdD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_local_crop(src_img, center_pixel, crop_shape, show = False):\n",
        "    '''\n",
        "    src_img:\n",
        "    center_pixel:\n",
        "    crop_shape:\n",
        "    '''\n",
        "    crop_height =  crop_shape[0]\n",
        "    crop_width = crop_shape[1]\n",
        "    top_= center_pixel[0] - crop_height // 2\n",
        "    bottom_ = center_pixel[0] + crop_height // 2\n",
        "    left_ = center_pixel[1] - crop_width // 2  \n",
        "    right_ = center_pixel[1] + crop_width // 2\n",
        "    if len(src_img.shape)> 2:\n",
        "        crop = src_img[ top_ : bottom_, left_:right_, :]\n",
        "    else:\n",
        "        crop = src_img[ top_ : bottom_, left_:right_]\n",
        "\n",
        "    if show:\n",
        "        cv2_imshow(crop)\n",
        "    return crop\n",
        "\n",
        "def match_hog(src_img, hog_desc, original_face_shape, win_stride = (8,8), show = False):\n",
        "    '''\n",
        "    src_img : image to analyse\n",
        "    hog_desc: (fd, hog_image) of the corresponding faces_cropped\n",
        "    original_face_shape: shape of the face used for hog_desc computation\n",
        "    '''\n",
        "    height = src_img.shape[0] # height of the image to analyze\n",
        "    width  = src_img.shape[1] # width of the image to analyze\n",
        "    \n",
        "    height_face = original_face_shape[0]\n",
        "    width_face = original_face_shape[1]\n",
        "\n",
        "    res_shape = (src_img.shape[0], src_img.shape[1])\n",
        "    res = np.ones(res_shape)*-1\n",
        "\n",
        "    if show:\n",
        "        tmp_image = src_img.copy()\n",
        "        cv2.rectangle(tmp_image, (width_face//2, height_face//2), (width - width_face//2, height - height_face//2), (0, 255, 0))\n",
        "        cv2_imshow(tmp_image)\n",
        "\n",
        "    running_h_idx = range(height_face //2, height - height_face//2+1, win_stride[0])\n",
        "    running_w_idx = range(width_face //2, width - width_face//2+1, win_stride[1])\n",
        "\n",
        "    for h_idx in running_h_idx:\n",
        "        for w_idx in running_w_idx:\n",
        "            local_crop = get_local_crop(src_img, (h_idx, w_idx), original_face_shape, False)\n",
        "            local_resized_img = cv2.resize(local_crop, (64,64))\n",
        "            local_fd = skimage_feature_hog(local_resized_img, \n",
        "                                            orientations=9, \n",
        "                                            pixels_per_cell=(8,8), \n",
        "                                            cells_per_block=(2, 2), \n",
        "                                            block_norm = \"L2\",\n",
        "                                            visualize=False, \n",
        "                                            transform_sqrt = True,\n",
        "                                            multichannel=True)\n",
        "\n",
        "            # computing euclidean distance here !!            \n",
        "            res[h_idx,w_idx]= np.linalg.norm(local_fd-hog_desc[0])\n",
        "            if show:\n",
        "                cv2_imshow(local_resized_img)\n",
        "\n",
        "    return res\n",
        "\n",
        "def fill_matrix_min_neighbours(matrx, win_stride = (16,16), margin = 0):\n",
        "    '''\n",
        "    fill the gaps in the  computation by taking the min values from closest neighbours\n",
        "    that were computed.\n",
        "    '''\n",
        "    l = np.argwhere(matrx != -1)\n",
        "    res = matrx.copy()\n",
        "\n",
        "    if len(l)<2:\n",
        "        idx_to_change = res == -1\n",
        "        logging.warning(\"len < 2 --> len(idx_to_change) = \" + str(len(idx_to_change)))\n",
        "        res[idx_to_change] = res.max() + margin\n",
        "        return res\n",
        "\n",
        "    top_left_corner = l[0]\n",
        "    bottom_right_corner = l[-1]\n",
        "\n",
        "    for i in range(top_left_corner[0], bottom_right_corner[0]+win_stride[0]//2):\n",
        "        for j in range(top_left_corner[1],bottom_right_corner[1]+win_stride[1]//2):\n",
        "            if matrx[i,j] == -1:\n",
        "                local_roi = get_local_crop(matrx, (i,j),win_stride)\n",
        "                cand = local_roi[ local_roi != -1 ]\n",
        "                res[i,j] = cand.min()\n",
        "    idx_to_change = res == -1\n",
        "    res[idx_to_change] = res.max() + margin\n",
        "    return res"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L07yM3Z15tJN",
        "colab_type": "text"
      },
      "source": [
        "*Details*\n",
        "\n",
        "For all the image in images_candidates list, \n",
        "- compute the matching on every required location (point spaced by win_stride)\n",
        "- fill the non computed point with min neighbour\n",
        "- normalized (L2) to get values between 0 -> 1\n",
        "- show the results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L06xUrOubbu_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for img in images_candidates:\n",
        "    win_stride=(16,16)\n",
        "    res = match_hog(img, hog_of_interest, image_of_interest.shape, win_stride)\n",
        "    new_res = fill_matrix_min_neighbours(res, win_stride)\n",
        "\n",
        "    logging.debug(\"worst match hog results: \" + str(new_res.max()))\n",
        "    logging.debug(\"best match hog results: \" + str(new_res.min()))\n",
        "\n",
        "    b = new_res.copy()\n",
        "    bmax, bmin = b.max(), b.min()\n",
        "    if bmax == bmin and bmax == 0:\n",
        "        logging.info(\"Perfect match!\")\n",
        "    b = (b - bmin)/(bmax - bmin)\n",
        "    # b = (b - bmin)/(bmax)\n",
        "\n",
        "    logging.debug(\"worst match hog results normalized [expexted 1]: \" + str(b.max()))\n",
        "    logging.debug(\"best match hog results [expected 0]: \" + str(b.min()))\n",
        "\n",
        "\n",
        "\n",
        "    fig, (ax1, ax2) = plt.subplots(1,2 , figsize=(16, 8), sharex=True, sharey=True) \n",
        "\n",
        "    ax1.imshow(cv2.cvtColor(img,cv2.COLOR_BGR2RGB))\n",
        "    ax1.set_title(\"Image where to find base face\")\n",
        "\n",
        "    # ax2.imshow(new_res, cmap=\"jet\")\n",
        "    # ax2.set_title(\"Results gaps filled with min neighbour\")\n",
        "\n",
        "    ax2.imshow(b, cmap=\"jet\")\n",
        "    ax2.set_title(\"Normalized results of Matching\")\n",
        "\n",
        "    plt.show()\n",
        "    logging.debug(\"=====================================================\")\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UtorNRBtUiEr",
        "colab_type": "text"
      },
      "source": [
        "####Matching - part2\n",
        "\n",
        "As showed in the results above, it works ! Several observations nonetheless:\n",
        "1. the borders are red (indicating  large distance): this is because of the sliding that considers the full object of interest is required in the image\n",
        "\n",
        "2. It works even if not the exact same shape!\n",
        "    * provided that a threshold is chosen, one could use this to recognize an object\n",
        "    * only *LOCAL* description is given: only local object shape and appearances (to some extend) can be represented. \n",
        "    * Because of locality and limit of expressiveness, the face of Bradley Cooper and Emma Stone may look alike, in terms of HOG descriptors.\n",
        "\n",
        "3. Only *ONE* size of the object is verified. If the object to find is of the same size as the object in the image, the descriptors will be very similar, and the distance small. However, if the object to find is smaller or larger in the image candidate, the HOG descriptor won't match at all. \n",
        "To solve this problem, one can perform multiscale detection, where the object is scaled several times to ensure to really detect the object, if it is present.\n",
        "\n",
        "A nice overview of this multiscaling can be found in [pyimagesearch](https://www.pyimagesearch.com/2015/11/16/hog-detectmultiscale-parameters-explained/)\n",
        "\n",
        "4. The detection of object with this sliding window takes time, and is even more time / resource consuming if used with multiscale analysis. Several techniques should be set in place (also discussed in [pyimagesearch](https://www.pyimagesearch.com/2015/11/16/hog-detectmultiscale-parameters-explained/)) such as:\n",
        "    - reducing the size of the image candidate, without losing too much information\n",
        "    - adjusting the HOG parameters so that the computation time is optimized for a **specific application**\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zwq_PBQ-v8ZN",
        "colab_type": "text"
      },
      "source": [
        "###Wrap up the feature representation\n",
        "\n",
        "We can now wrap up the results in a matrix of dimension $(n \\times p)$, where $n$ is the number of training images, and $p$ is the number of features (the length of the feature representation)\n",
        "\n",
        "We call this matrix `X_HOG_train`\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3i1UnSKbwer_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "get dimensions from previously built data structure\n",
        "'''\n",
        "local_n = sum([len(hog_training[person]) for person in persons])\n",
        "local_p = len(hog_training[personA][0][0])\n",
        "\n",
        "'''\n",
        "create data structure X_HOG_train\n",
        "'''\n",
        "\n",
        "X_HOG_train = np.empty((local_n, local_p))\n",
        "\n",
        "'''\n",
        "Fill data structure with feature representation\n",
        "'''\n",
        "i=0\n",
        "for person in hog_training.keys():\n",
        "    for item in hog_training[person]:\n",
        "        X_HOG_train[i,:] = item[0]\n",
        "        i+=1\n",
        "        \n",
        "'''\n",
        "log the shape of the data structure\n",
        "'''\n",
        "logging.info(\"X_HOG_train shape: \" + str(X_HOG_train.shape))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uPSyg9VkrxUg",
        "colab_type": "text"
      },
      "source": [
        "and let's do the same with the test set..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-KUZ_6M3rVRb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "get dimensions from previously built data structure\n",
        "'''\n",
        "local_t_n = sum([len(hog_test[person]) for person in persons])\n",
        "local_t_p = len(hog_test[personA][0][0])\n",
        "\n",
        "'''\n",
        "create data structure X_HOG_train\n",
        "'''\n",
        "\n",
        "X_HOG_test = np.empty((local_t_n, local_t_p))\n",
        "\n",
        "'''\n",
        "Fill data structure with feature representation\n",
        "'''\n",
        "i=0\n",
        "for person in hog_test.keys():\n",
        "    for item in hog_test[person]:\n",
        "        X_HOG_test[i,:] = item[0]\n",
        "        i+=1\n",
        "        \n",
        "'''\n",
        "log the shape of the data structure\n",
        "'''\n",
        "logging.info(\"X_HOG_test shape: \" + str(X_HOG_test.shape))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BbGWtNSulkEt",
        "colab_type": "text"
      },
      "source": [
        "###HOG Conclusion\n",
        "\n",
        "In this first feature representation building, we studied in details how the descriptor is built and computed, and the different parameters that comes in play, and in particular\n",
        "- the cell size\n",
        "- the block size\n",
        "- the normalization method\n",
        "\n",
        "We have then used the HOG descriptor to try and detect if an object is in an image, by computing on (almost) every position of the image the descriptor and assessing the distance (as similarity metric) with the face of interest descriptor.\n",
        "Doing so, we have then seen that the techniques works well to find region of a similar shape in the image, hence the locality of the descriptor. We also covered some of the challenges related to the images'size, locality of the descriptors, and complexity of the computations.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FMBCO6xyhskK",
        "colab_type": "text"
      },
      "source": [
        "##  Principal Component Analysis - **PCA**\n",
        "\n",
        "We have extensively covered HOG. Similarly, we will go through the PCA technique. First, we will give some intuition; then we will go through the maths, as this technique rely heavily on the computing, then we will apply PCA on the training set and try and observe some results.\n",
        "\n",
        "When discussing PCA, we will mainly focus on the technique applied on images. There are plenty of blogs out there precisely defining and tailoring the techniques for all kinds of application. This tutorial is just an example of PCA applied to face images.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "As for the previous HOG technique, homemade code is fully provided. This gives the details on the implementation and insight about how things are calculated. For this part of the tutorial, most of the examples are done with this homemade code. A section is dedicated to a demo of a library tool as well. For the classification and identification parts, however, library optimized code will be used as it's the purpose of those libraries. \n",
        "Don't hesitate to try out different parameters in the provided function, and please report any bug to geoffroy.herbin@student.kuleuven.be\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Rr9R4fwgtK_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MyPCA:\n",
        "    '''\n",
        "    homemade class to perform PCA using several methods and compare\n",
        "    Three methods are implemented\n",
        "    - method = \"svd\": singular value decomposition technique\n",
        "    - method = \"eigen\": nominal eigenvalue decomposition technique is implemented\n",
        "    - method = \"eigen_fast\": eigenvalue decomposition using dimensionality \n",
        "                            reduction is implemented\n",
        "    '''\n",
        "    def __init__(self, method = \"svd\"):\n",
        "        self.method = method\n",
        "        self.eigenvalues = None\n",
        "        self.eigenvectors = None\n",
        "        self.X_mean = None\n",
        "\n",
        "    def fit(self, data):\n",
        "        X = data.copy()\n",
        "        n, m = X.shape\n",
        "        assert n < m, \"n is not smaller than m -> you most likely need \" + \\\n",
        "                    \"to transpose your input data\"\n",
        "        self.X_mean = np.mean(X, axis = 0)\n",
        "        X -= self.X_mean\n",
        "\n",
        "        self.eigenvalues = None\n",
        "        self.eigenvectors = None\n",
        "\n",
        "        if self.method == \"svd\":\n",
        "            # singular value decomposition\n",
        "            U, S, Vt = np.linalg.svd(X)\n",
        "            self.eigenvalues = S**2 / (n - 1)\n",
        "            self.eigenvectors = Vt.T[:,0:n]\n",
        "\n",
        "        elif self.method == \"eigen_fast\":\n",
        "\n",
        "            # compute small covariance matrix\n",
        "            D = np.dot(X, X.T) / (n - 1)\n",
        "\n",
        "            # eigen decomposition\n",
        "            LD, W = np.linalg.eig(D)\n",
        "\n",
        "            order_D = np.argsort(LD)[::-1]\n",
        "            LD_sorted = LD[order_D]\n",
        "            W_sorted = W[:,order_D]\n",
        "            \n",
        "            eigenVectors_sorted_tmp = np.dot(X.T, W_sorted)\n",
        "            eigenVectors_sorted = np.empty(eigenVectors_sorted_tmp.shape)\n",
        "            for i in range(n):\n",
        "                v = eigenVectors_sorted_tmp[:,i]\n",
        "                eigenVectors_sorted[:,i] = v / np.linalg.norm(v)\n",
        "            \n",
        "            self.eigenvalues = LD_sorted\n",
        "            self.eigenvectors = eigenVectors_sorted\n",
        "                \n",
        "        elif self.method ==\"eigen\":\n",
        "            # compute covariance matrix\n",
        "\n",
        "            Cov = np.dot(X.T, X) / (n - 1)\n",
        "\n",
        "            # eigen decomposition\n",
        "            LC, V =np.linalg.eig(Cov)\n",
        "\n",
        "            # sort in appropriate order and keep only relevant component\n",
        "            order = np.argsort(LC)[::-1]\n",
        "            LC_sorted = LC[order][0:n]\n",
        "            V_sorted = V[:,order][:,0:n]\n",
        "\n",
        "            self.eigenvalues = LC_sorted\n",
        "            self.eigenvectors = V_sorted.real\n",
        "        else:\n",
        "            raise RuntimeError(\"method value unknown\")\n",
        "        \n",
        "    def projectPC(self, X, k):\n",
        "        Vk = self.eigenvectors[:, 0:k]\n",
        "        # logging.debug(\"Reduce X \" + str(X.shape) + \"using k = \" + str(k) + \" components\")\n",
        "        # logging.debug(\"Vk (4096 x k)= \" + str(Vk.shape))\n",
        "        X_reduced = np.dot(X, Vk)\n",
        "        # logging.debug(\"X_reduced (n x k) = \" + str(X_reduced.shape))\n",
        "\n",
        "        return X_reduced\n",
        "    \n",
        "    def reconstruct(self, X_reduced, k, show = False):\n",
        "        if len(X_reduced.shape) > 1:\n",
        "            X_reduced = X_reduced[:,0:k]\n",
        "        else:\n",
        "            X_reduced = X_reduced[0:k]\n",
        "        \n",
        "        Vkt = self.eigenvectors[:, 0:k].T\n",
        "        # logging.debug(\"Reduce using X_reduced = \" + str(X_reduced.shape) )\n",
        "        # logging.debug(\"Reconstruct using k = \" + str(k) + \" components\")\n",
        "        # logging.debug(\"Vkt (k x 4096)= \" + str(Vkt.shape))\n",
        "        X_hat_centered = np.dot(X_reduced, Vkt)\n",
        "        # logging.debug(\"X_hat_centered.shape (20,4096): \" + str(X_hat_centered.shape))\n",
        "        if show:\n",
        "            self.show_data(X_hat_centered, add_mean = True)\n",
        "        return X_hat_centered \n",
        "\n",
        "    def compute_error(self, X, X_hat):\n",
        "        return sqrt(mean_squared_error(X, X_hat))\n",
        "\n",
        "    def show_principal_components(self,k, figsize=(10,4)):\n",
        "        w = figsize[0]\n",
        "        h = figsize[1]\n",
        "        fig = plt.figure(figsize=(w,h)) \n",
        "        fig.subplots_adjust(left=0, right=1, bottom=0, top=1, hspace=0.05, wspace=0.05) \n",
        "        logging.debug(\"self.eigenvectors.shape = \" + str(self.eigenvectors.shape) )\n",
        "        for i in range(k):\n",
        "            pc = self.eigenvectors[:,i]\n",
        "            assert pc.shape[0]==(sq_size**2)*1 or pc.shape[0]==(sq_size**2)*3, \"Not proper shape (expected (sq_size**2) (*3)) \" + str(pc.shape)\n",
        "            \n",
        "            ax = fig.add_subplot(h, w, i+1, xticks=[], yticks=[]) \n",
        "            if color:\n",
        "                pc_img = (np.reshape(pc.real, (sq_size, sq_size, 3))*255).astype(\"uint8\") \n",
        "                ax.imshow(pc_img, interpolation='nearest') \n",
        "            else:\n",
        "                pc_img = np.reshape(pc.real, (sq_size, sq_size))\n",
        "                ax.imshow(pc_img  , cmap=plt.cm.gray, interpolation='nearest')\n",
        "\n",
        "        plt.show()\n",
        "\n",
        "    def compute_explained_variance(self, show=True):\n",
        "        sum_all_eigenValues = sum(self.eigenvalues)\n",
        "        logging.debug(\"\\nSum of all eigenValues: \" + str(sum_all_eigenValues))\n",
        "\n",
        "        explained_variance      = [(value / sum_all_eigenValues)*100 for value in self.eigenvalues]\n",
        "        cum_explained_variance  = np.cumsum(explained_variance)\n",
        "        logging.debug(\"Cum explained variance : \\n\" + str(cum_explained_variance))\n",
        "        if show:\n",
        "            fig = plt.figure(figsize=(12, 6))\n",
        "            ax1 = fig.add_subplot(121)\n",
        "            ax1.bar(range(len(self.eigenvalues)), self.eigenvalues)\n",
        "            ax1.set_xlabel('eigenvalues')\n",
        "            ax1.set_ylabel('values')\n",
        "\n",
        "            ax2 = fig.add_subplot(122)\n",
        "            ax2.bar(range(len(explained_variance)), explained_variance)\n",
        "            ax2.plot(range(len(cum_explained_variance)), cum_explained_variance, color='green', linestyle='dashed', marker='o', markersize=5)\n",
        "\n",
        "            ax2.set_xlabel('eigenvalues')\n",
        "            ax2.set_ylabel('% information ')\n",
        "            ax2.legend( labels = [\"Cumulative Expl. Var.\", \"Explained Variance\"])\n",
        "            ax2.grid()\n",
        "            plt.show()\n",
        "        return explained_variance, cum_explained_variance\n",
        "\n",
        "    def show_data(self, X, add_mean = False):\n",
        "        \n",
        "        # copy so that adding the mean does not modify the original centered\n",
        "        # data X\n",
        "        X_ = X.copy()\n",
        "\n",
        "        if len(X.shape) > 1:\n",
        "            self._show_data(X_, add_mean)\n",
        "        else:\n",
        "            fig = plt.figure(figsize=(3,3))\n",
        "            if add_mean:\n",
        "                    X_ += self.X_mean\n",
        "            # img = np.reshape(X_, (sq_size, sq_size))\n",
        "            img = my_reshape(X_, sq_size, color)\n",
        "\n",
        "            ax = fig.add_subplot(1, 1, 1, xticks=[], yticks=[]) \n",
        "            ax.imshow(img, cmap = my_color_map, interpolation='nearest') \n",
        "\n",
        "            plt.show()\n",
        "\n",
        "\n",
        "    def _show_data(self, X, add_mean = False):\n",
        "        fig = plt.figure(figsize=(8,8)) \n",
        "        fig.subplots_adjust(left=0, right=1, bottom=0, top=1, hspace=0.05, wspace=0.05) \n",
        "        i=0\n",
        "        for x in X:\n",
        "            if add_mean:\n",
        "                    x += self.X_mean\n",
        "            # assert x.shape[0]==(sq_size**2)*3, \"Not proper shape (expected (sq_size**2)*3) \" + str(x.shape)\n",
        "            ax = fig.add_subplot(8, 5, i+1, xticks=[], yticks=[]) \n",
        "            img = my_reshape(x, sq_size, color) \n",
        "            ax.imshow(img, cmap = my_color_map, interpolation='nearest') \n",
        "\n",
        "            i+=1\n",
        "        plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gQjyu0m88ZJP",
        "colab_type": "text"
      },
      "source": [
        "### Basic idea\n",
        "\n",
        "If you are completely unfamiliar with the principal components analysis, the thread in [PCA intuition](https://stats.stackexchange.com/questions/2691/making-sense-of-principal-component-analysis-eigenvectors-eigenvalues) contains a wonderful layered explanation of what the PCA is, and what can its use be.\n",
        "Another very nice introduction is given in [medium](https://medium.com/@aptrishu/understanding-principle-component-analysis-e32be0253ef0)\n",
        "\n",
        "Essentially, the PCA technique extracts the information from the data to get the main directions intrinsically present in the data. To realize that, PCA uses the covariance, which is a *measure of the extent to which corresponding elements from two sets of ordered data move in the same direction* (definition extracted from [medium website](https://medium.com/@aptrishu/understanding-principle-component-analysis-e32be0253ef0))\n",
        "\n",
        "\n",
        "Based on this covariance information, the PCA  *\\\"finds a new set of dimensions (or a set of basis of views) such that all the dimensions are orthogonal (and hence linearly independent) and ranked according to the variance of data along them. It means more important principle axis occurs first\\\"* (source: [medium website](https://medium.com/@aptrishu/understanding-principle-component-analysis-e32be0253ef0))\n",
        "If you're more of a visual person, the plot [here](https://i.stack.imgur.com/lNHqt.gif) shows the first principal component of a cloud of points, with an animation that shows how the variance is minimized.\n",
        "\n",
        "It is really an extraction of information from the data, hence an unsupervised technique, that is used to reduce the dimensionality of the problem. Applying PCA on images can therefore be used as a feature representation!\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zIFK9vVWiyuf",
        "colab_type": "text"
      },
      "source": [
        "### PCA How-to, Step1: Pre-processing\n",
        "\n",
        "The PCA is applied on *hyper-points*: points in our high-dimensional space. We reprensent each of those points by a vector. Applied on images, we then need to convert our sets of images into a useable format. \n",
        "\n",
        "The representation chosen is a $(n \\times p)$ matrix where\n",
        "- n is the number of images\n",
        "- p is the dimension of the vector\n",
        "\n",
        "The matrix containing the training data is then a $(40 \\times p)$ matrix, and -- in our specific problem insofar -- the matrix containing the test data is a $(40 \\times p)$ matrix.\n",
        "\n",
        "#### Resizing\n",
        "All the images (*hyper-points*) shall have the same dimensions to start with. There is therefore a first step of resizing all the images into a commonly appropriate size, keeping in mind that the faces are square images. The parameter indicating the lenght of this square side is defined as `sq_size` in this notebook. \n",
        "\n",
        "`sq_size = 64` indicates that the images are resized as a $(64 \\times 64)$ matrix, containing 1 or 3 channels depending on their colorscale. \n",
        "\n",
        "\n",
        "#### Color or Grayscale ?\n",
        "In the litterature, we find both implementation, and I decide not to choose at this point. A grayscale image contains only one channel, a colored image contains three. \n",
        "> to change from grayscale to color, just change the boolean parameter `color` in the beginning of this notebook.\n",
        "\n",
        "The grayscale image is then simply converted to a vector by flattening the matrix, concatenating each row one after the others. \n",
        "\n",
        "The colored image is converted applying the same technique on the three channels, then concatenating the three resulting vectors into one, longer, vector.\n",
        "\n",
        "####Resulting Dimensionality\n",
        "From a training set colored image of dimensions $( h, w )$\n",
        "- resize to $(64,64)$\n",
        "- convert to vector:\n",
        "    * if grayscale, convert to $(1, 4096)$\n",
        "    * if color, convert to ($1, 12288)$\n",
        "\n",
        "The resulting matrix representing the training set has a shape $(40,4096)$ or $(40,12288)$ depending if grayscale or color."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Erjs7Drnf9w9",
        "colab_type": "text"
      },
      "source": [
        "####Process the training set as a data matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C293QFgzgezY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "Build useable training set from (hyper-)parameters\n",
        "'''\n",
        "\n",
        "# column 0 = first image\n",
        "# column 1 = second image\n",
        "# ...\n",
        "m_src = get_matrix_from_set(training_set, color, sq_size = sq_size, flatten = True)\n",
        "\n",
        "logging.debug(\" m_src: original matrix\")\n",
        "logging.debug(\" m_src: shape = \" + str(m_src.shape))\n",
        "\n",
        "plot_matrix(m_src, color, my_color_map, h=4, w=10, transpose = False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "foS_IqQljwTu",
        "colab_type": "text"
      },
      "source": [
        "#### Process the test set as a data matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_tDYQ-ODjvAV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "Build useable test set from (hyper-)parameters\n",
        "'''\n",
        "m_test_src = get_matrix_from_set(test_set, color, sq_size = sq_size, flatten = True)\n",
        "\n",
        "\n",
        "logging.debug(\" m_test_src: original matrix\")\n",
        "logging.debug(\" m_test_src: shape = \" + str(m_test_src.shape))\n",
        "\n",
        "\n",
        "plot_matrix(m_test_src, color, my_color_map,h = 4, w=10,transpose = False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JlzOen9a4Nq_",
        "colab_type": "text"
      },
      "source": [
        "###PCA How-to, Step2: Centering the Data\n",
        "\n",
        "Centering the data is essential in order to have, eventually, the eigenvectors sorted according to the eigenvalues properly meaning what we desire, aka the directions of max variances in the data.\n",
        "\n",
        "The following code shows:\n",
        "1. (left) a plot of the data (training) matrix \n",
        "2. (middle) the mean image\n",
        "3. (right) a plot of the data where the mean image is substracted: the data are now centered\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8uVi6DotjZF9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = m_src.copy()\n",
        "X_mean = np.mean(X, axis = 0)\n",
        "Xc = X - X_mean\n",
        "\n",
        "fig = plt.figure(figsize=(18, 5))\n",
        "\n",
        "Xs = np.arange(0, X.shape[0])\n",
        "Ys = np.arange(0, X.shape[1])\n",
        "Xs, Ys = np.meshgrid(Xs, Ys)\n",
        "  \n",
        "ax1 = fig.add_subplot(131,projection='3d')\n",
        "surf1 = ax1.plot_surface(Xs, Ys, X.T, cmap=plt.cm.jet, antialiased=True)\n",
        "ax1.set_xlabel('image index')\n",
        "ax1.set_ylabel('vector index')\n",
        "ax1.set_zlabel('value')\n",
        "ax1.set_title('Original data')\n",
        "ax2 = fig.add_subplot(132)\n",
        "ax2.imshow(my_reshape(X_mean, sq_size, color), cmap = my_color_map, interpolation='nearest') \n",
        "ax2.set_title(\"Mean image\")\n",
        "\n",
        "\n",
        "ax3 = fig.add_subplot(133,projection='3d')\n",
        "surf3 = ax3.plot_surface(Xs, Ys, Xc.T, cmap=plt.cm.jet, antialiased=True)\n",
        "ax3.set_xlabel('image index')\n",
        "ax3.set_ylabel('vector index')\n",
        "ax3.set_zlabel('value')\n",
        "ax3.set_title(\"Centered data\")\n",
        "plt.show()\n",
        "\n",
        "logging.info(\"Visualization of the images - centered:\")\n",
        "\n",
        "plot_matrix(Xc, color, my_color_map, h=4, w=10,transpose = False)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oOG5NzkvKzIo",
        "colab_type": "text"
      },
      "source": [
        "###PCA How-to, Step3: Decomposition\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vc6kwotfxtH_",
        "colab_type": "text"
      },
      "source": [
        "####*Canonical - EigenDecomposition*\n",
        "####0. <u>Data </u>\n",
        "Let's take our initial data matrix $X$, a $(n \\times p)$ matrix of data where n is the number of images, and p is the number of variables. In our case, the number of variables is the number of pixels of one image (or three times this number, if color image). First, we have to center the data, hence substracting the mean image. This is done in the previous step. In the following text, $X$ is assumed centered. \n",
        "\n",
        "\n",
        "\n",
        "####1. <u>Covariance Matrix </u>\n",
        "\n",
        "We compute the covariance matrix, that indicates how a variable (= pixel intensity) varies with respect to other pixels. The Covariance matrix indicates how the variables evolve with respect to each others. \n",
        "\n",
        "$C = \\frac{X^T \\cdot X}{n-1}$ and has dimension $(p \\times p)$. This is a pretty large matrix already.\n",
        "\n",
        "####2. <u>Eigenvalues and EigenVectors </u>\n",
        "Having computed the covariance matrix C, we compute its eigenvectors and eigenvalues, indicating the main directions and their strength of how the data evolve. The eigendecomposition is expressed as: $C = V L V^T$ where\n",
        "- $L$ is a diagonal matrix of eigenvalues\n",
        "- $V$ is the $(p \\times p)$ matrix of eigenvectors\n",
        "\n",
        "\n",
        "<u>*Mathematical Trick: Exploiting the dimensionality of the matrix*</u>\n",
        "\n",
        "Computing the eigenvalues and eigenvectors of $C$ is pretty cumbersome, as $C$ is a large matrix $(p \\times p)$.\n",
        "\n",
        "Recalling our algebra skills, given the dimension of $X$, we know that only a limited amount of eigenvalues are non zero: there are only $(n-1)$ non zero eigenvalues. There is no need to compute the $p$ eigenvalues and related $(p \\times p)$ eigenvectors matrix as (all) the information is contained in only $(n-1)$ eigenvalues.\n",
        "\n",
        "As a result, to speed up the computation and take advantage of this property, instead of computing the eigenvalues and eigenvectors of $C = \\frac{X^T \\cdot X}{n-1}$ of size $(p \\times p)$, let's rather compute the $n$ eigenvalues and corresponding eigenvectors of the matrix $D = \\frac{X \\cdot X^T}{n-1}$ of size $(n \\times n)$, such that $D = W  L W^T$\n",
        "- the eigenvalues computed are the same as $C$'s\n",
        "- the corresponding eigenvectors of C, in matrix $V$, are related such that $V = X^T \\cdot W$\n",
        "\n",
        "This way, it takes advantage of the dimensions of the problem.\n",
        "\n",
        "####3. <u>Principal Components</u>\n",
        "\n",
        "The principal components are defined as the eigenvectors $V$. The eigenvectors can be sorted according to the value of their associated eigenvalue, in decreasing order.\n",
        "\n",
        "The eigenvector that has the largest eigenvalue associated is called \"first principal component\", the second largest is called \"second principal component\", and so on.\n",
        "\n",
        "In the context of faces analysis, the eigenvectors are often called **eigenfaces**, as they can be reshaped as an image, and displayed (provided the appropriate number conversion so that it's in a range visible)\n",
        "\n",
        "By projecting the original data $X$ on the new directions, the eigenfaces, we get *new coordinates* that yet *fully describe the original data*. \n",
        "\n",
        "Furthermore, the number of eigenvectors on which we project the data is reduced with respect to original problem dimensionality.   \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4HE5XZ2h1kmX",
        "colab_type": "text"
      },
      "source": [
        "####*Singular Value Decomposition*\n",
        "\n",
        "The idea behind the SVD is essentially mathematical, and help in computing the eigenvectors and eigenvalues in a different way. From a matrix X, centered, (as in previous section), one can compute its decomposition $X = U \\cdot S \\cdot V^T$ \n",
        "where $U$ is a unitary matrix, $S$ is diagonal, containing what's called the singular values $s_i$. \n",
        "One can see that $V$, right singular vectors, are related to eigenvectors of the covariance matrix from previous section.\n",
        "\n",
        "Indeed, computing this covariance matrix gives:\n",
        "\n",
        "\\begin{align}\n",
        "Cov &= \\frac{X^T \\cdot X}{n-1} \\\\\n",
        "    &= \\frac{V \\cdot S \\cdot U^T \\cdot U \\cdot S \\cdot V^T}{n-1} \\\\\n",
        "    &= V \\cdot \\frac{S^2}{n-1} V^T\n",
        "\\end{align}\n",
        "\n",
        "There is therefore a link between the singular values ($s_i$) and the eigenvalues ($\\lambda_i$):\n",
        "$$\\lambda_i = \\frac{s_i^2}{n-1}$$ and the right singular vectors are the eigenvectors $V$.\n",
        "\n",
        "Note that in practice, most of the implementation of the PCA algorithm uses singular value decomposition, and starts by centering the data - such as the library function `sklearn.decomposition.PCA` that we will extensively use later on."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T1OfmaOV4tkL",
        "colab_type": "text"
      },
      "source": [
        "####Decomposition and Eigenfaces\n",
        "\n",
        "The following lines of codes create an object of type `MyPCA`, and compute the decompositions following two methods: \"svd\" and \"eigen_fast\". \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P9c9kJzRhMVK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nb_training_faces = sum(training_sets_size.values())\n",
        "\n",
        "'''\n",
        "Ensuring reset of object if cell is rerun\n",
        "'''\n",
        "my_pca = None\n",
        "my_pca2 = None\n",
        "\n",
        "'''\n",
        "Getting the source matrix\n",
        "'''\n",
        "X = m_src.copy()\n",
        "\n",
        "'''\n",
        "Creating the MyPCA objects\n",
        "-> solving according to SVD\n",
        "-> solving according to EigenDecomposition (with Math Trick)\n",
        "'''\n",
        "my_pca = MyPCA(\"svd\")\n",
        "my_pca.fit(X)\n",
        "print(\"All principal components, using SVD\")\n",
        "my_pca.show_principal_components(k=nb_training_faces)\n",
        "\n",
        "my_pca2 = MyPCA(\"eigen_fast\")\n",
        "my_pca2.fit(X)\n",
        "print(\"All principal components, using eigendecomposition\")\n",
        "my_pca2.show_principal_components(k=nb_training_faces)\n",
        "\n",
        "# my_pca3 = MyPCA(\"eigen\")\n",
        "# my_pca3.fit(X)\n",
        "# print(\"All principal components, using nominal eigendecomposition\")\n",
        "# my_pca3.show_principal_components(k=nb_training_faces)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sWe-_cjtjRXe",
        "colab_type": "text"
      },
      "source": [
        "The two plots above show the same information: the eigenfaces, but computed in two different ways: using SVD and using eigendecomposition. Several things are important to be noticed:\n",
        "- the eigenfaces are very much alike. Actually, they are exactly the same (despite the last one, see next point) considering the well-known sign ambiguity related to decomposition, see [Standord course, sect. 5.3, Properties of eigenvectors](https://graphics.stanford.edu/courses/cs205a-13-fall/assets/notes/chapter5.pdf). Long story short, white and black may be reversed without any issue in the eigenfaces, on each image independantly. \n",
        "    * In the commonly used library implementation, there is often an `sign_flip`function implemented to ensure repeatability of the results of the decomposition. See [documentation](https://kite.com/python/docs/sklearn.utils.extmath.svd_flip)\n",
        "- the last eigenface seems to differ... Definitely, this isn't an issue. \n",
        "    * recall that there are only (n - 1) non-zero eigenvalue. The eigenface associated to this eigenvalue is therefore multiplied by 0, and doesn't play a role. \n",
        "\n",
        "If interested, you may uncomment the last lines in order to create a PCA with parameter `method = \"eigen\"`, and see the result of the true eigendecomposition, without the mathematical trick. Or you can trust me that the result is the same - with the actual noisy 40th component (time to run ~50 seconds)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CRu22Qk0KBeM",
        "colab_type": "text"
      },
      "source": [
        "####Reconstructing on k components"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ZZagj0-bU7a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "Selecting X as the input image we want to reconstruct\n",
        "'''\n",
        "X = m_src.copy()[9,:]\n",
        "\n",
        "logging.debug(\"Shape of input X = \" + str((X.shape)))\n",
        "\n",
        "'''\n",
        "Centering the data\n",
        "'''\n",
        "X_mean = np.mean(m_src.copy(), axis=0)\n",
        "Xc = X - X_mean\n",
        "\n",
        "\n",
        "'''\n",
        "Computing X_reduced, projection of Xc on the principal components space\n",
        "'''\n",
        "X_reduced = my_pca.projectPC(Xc, k=nb_training_faces)\n",
        "logging.debug(\"Shape of X_reduced = \" + str(X_reduced.shape))\n",
        "\n",
        "'''\n",
        "Reconstructing progressively\n",
        "Based on k first components only\n",
        "'''\n",
        "k=0\n",
        "fig = plt.figure(figsize=(3,3))\n",
        "img = my_reshape(X_mean, sq_size, color)\n",
        "ax = fig.add_subplot(1, 1, 1, xticks=[], yticks=[]) \n",
        "ax.imshow(img, cmap = my_color_map, interpolation='nearest') \n",
        "plt.show()\n",
        "logging.info(\"Principal components used = \" + str(k) + \";\\nReconstruction error = \" + str(np.round(my_pca.compute_error(Xc+X_mean, X_mean),2)))\n",
        "\n",
        "for k in [1,2,3,5,8,10,12,15,20,25,30,40]:\n",
        "    X_hat_centered = my_pca.reconstruct(X_reduced, k, show=True)\n",
        "    logging.info(\"\\nAbove: \\nPrincipal components used = \" + str(k) + \n",
        "                 \";\\nReconstruction error = \" + \n",
        "                 str(np.round(my_pca.compute_error(Xc, X_hat_centered),2)) + \n",
        "                 \"\\n\"+\"___\"*30)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qFsv_wOOXZIV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "expl_var, cum_expl_var = my_pca.compute_explained_variance(show = True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ddRvRypsHa81",
        "colab_type": "text"
      },
      "source": [
        "*Discussions*\n",
        "\n",
        "On the above results and images, several things can be observed:\n",
        "1. First the different reconstructions of one selected face. As expected, the reconstruction error decreases as the number of components used (k) increases. This also matches the intuition behind the explained variance.\n",
        "2. Second, the ultimate error remaining is 0, indicating no information was lost when considering the 40 components. That confirms the mathematical theory.\n",
        "3. The last two graphs show on the left, the eigenvalues, and on the right, the cumulative explained variance. \n",
        "    * the eigenvalues are sorted from the most important to the least important, confirming the right curve of the cumulative expl. var. However, the descent of the values is not fast (not exponential). Furthermore, there is no big drop off in the values.\n",
        "    * this indicates that the choice of an **optimal number p** of components such that the dimensionality of the feature space is reduced but still informative is not obvious. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wrk9j2nrK9Bq",
        "colab_type": "text"
      },
      "source": [
        "####Choice of optimal $p$\n",
        "\n",
        "\n",
        "$p$ is defined as the optimal number of components used in the reduced space so that:\n",
        "1. the dimension is reduced (lower than n)\n",
        "2. the reconstructed information is *close* to input data. It means the reconstruction is still informative.\n",
        "Selecting only the first $p$ components has the effect of removing small variances. This can be important for some application, if there are little variance between different classes.\n",
        "\n",
        "\n",
        "As said above:\n",
        "- there is no clear drop-off in the eigenvalues,\n",
        "- there is no exponential decrease if the eigenvalues.\n",
        "\n",
        "It makes the choice of an optimal $p$ complicated. \n",
        "\n",
        "To choose, we will do:\n",
        "1. compute the reconstruction loss (error):\n",
        "    * for all training examples\n",
        "    * for all possible choice of p\n",
        "2. plot this RMSE, in absolute value, \n",
        "3. plot, in percentage, the ratio between the reconstruction loss using p-components and the RMSE between mean image and input image. This gives the notion of percentage of reconstruction error -- that can actually be related to the cumulative explained variance !\n",
        "2. define a threshold of 95% of the information kept (5% of error tolerated)\n",
        "\n",
        "That will lead to a sensible choice of $p$. This is however purely arbitrary.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7jHGWbSAi5MF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train = m_src.copy()\n",
        "X_train_mean = np.mean(X_train, axis = 0)\n",
        "X_train_centered = X_train - X_train_mean\n",
        "\n",
        "my_pca = None\n",
        "my_pca = MyPCA(\"svd\")\n",
        "my_pca.fit(X_train)\n",
        "\n",
        "n = X_train.shape[0]\n",
        "'''\n",
        "np array containing all the rmse computed\n",
        "\n",
        "if n = 40, max number of components, then rmse has a size 40x40 (0 -> 39)\n",
        "> a row matches the rmse of one image wrt the dimension reconstructed. Last column should be 0 (or close, ~e-14)\n",
        "'''\n",
        "rmse = np.empty((n,n+1))\n",
        "rmse_pc = np.empty((n,n+1)) # rmse in percentage\n",
        "rmse_base = np.empty((n,))\n",
        "for i in range(n):\n",
        "    rmse_base[i]=my_pca.compute_error(X_train[i], X_train_mean)\n",
        "\n",
        "\n",
        "index_image = 0\n",
        "for img_center_vector in X_train_centered:\n",
        "    # logging.debug(\"projecting on \" + str(n) + \" principal components\")\n",
        "    X_centered_reduced = my_pca.projectPC(img_center_vector, n)\n",
        "    for k in range(n+1):\n",
        "        # from 1 to n, included\n",
        "        if k == 0:\n",
        "            rmse[index_image, k] = rmse_base[index_image]\n",
        "            rmse_pc[index_image, k] = 100 * rmse[index_image,k] / rmse_base[index_image]\n",
        "        else:\n",
        "            # logging.debug(\"reconstructing using \" + str(p) + \" principal components\")\n",
        "            X_hat_centered = my_pca.reconstruct(X_centered_reduced, k)\n",
        "            rmse[index_image,k] = my_pca.compute_error(img_center_vector, X_hat_centered)\n",
        "            rmse_pc[index_image, k] = 100 * rmse[index_image,k] / rmse_base[index_image]\n",
        "        \n",
        "    index_image += 1\n",
        "\n",
        "fig = plt.figure(figsize = (16,16))\n",
        "ax1 = fig.add_subplot(2,2,1)\n",
        "\n",
        "for idx in range(n):\n",
        "    rmse_ = rmse[idx,:]\n",
        "    ax1.plot([i for i in range(0,n+1)],rmse_, '-')\n",
        "ax1.set_title(\"reconstruction errors (RMSE) for all images\")\n",
        "ax1.set_xlabel(\"reconstruction dimension(s) \\'p\\' \")\n",
        "ax1.set_ylabel(\"RMSE\")\n",
        "\n",
        "rmse_mean = np.mean(rmse, axis = 0)\n",
        "ax2 = fig.add_subplot(2,2,2)\n",
        "ax2.plot([i for i in range(0, n+1)], rmse_mean, \"ro-\")\n",
        "ax2.set_title(\"Mean of RMSE for all images\")\n",
        "ax2.set_xlabel(\"reconstruction dimension(s) \\'p\\' \")\n",
        "ax2.set_ylabel(\"mean of RMSEs\")\n",
        "\n",
        "ax1.set_ylim((0,80))\n",
        "ax2.set_ylim((0,80))\n",
        "ax1.grid()\n",
        "ax2.grid()\n",
        "\n",
        "ax3 = fig.add_subplot(2,2,3)\n",
        "\n",
        "for idx in range(n):\n",
        "    rmse_ = rmse_pc[idx,:]\n",
        "    ax3.plot([i for i in range(0,n+1)],rmse_, '-')\n",
        "ax3.set_title(\"reconstruction errors (RMSE) for all images, in %\")\n",
        "ax3.set_xlabel(\"reconstruction dimension(s) \\'p\\' \")\n",
        "ax3.set_ylabel(\"RMSE\")\n",
        "\n",
        "rmse_pc_mean = np.mean(rmse_pc, axis = 0)\n",
        "ax4 = fig.add_subplot(2, 2,4)\n",
        "ax4.plot([i for i in range(0, n+1)], rmse_pc_mean, \"ro-\")\n",
        "ax4.set_title(\"Mean of RMSE for all images, in %\")\n",
        "ax4.set_xlabel(\"reconstruction dimension(s) \\'p\\' \")\n",
        "ax4.set_ylabel(\"mean of RMSEs\")\n",
        "\n",
        "ax3.set_ylim((0,110))\n",
        "ax4.set_ylim((0,110))\n",
        "ax3.grid()\n",
        "ax4.grid()\n",
        "\n",
        "# print(rmse_pc_mean)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X6RE5kJ1bgWw",
        "colab_type": "text"
      },
      "source": [
        "Following the explained process of defining $p$, the threshold is set at $p = 35$. This is definitely not an extraordinary result, but yet constitutes somehow a reduction, and based on a reasonable choice.\n",
        "\n",
        "We can now visualize the training image reconstructed based on the first 35 components."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6tMlcQwwcbQ0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "Define optimal p\n",
        "'''\n",
        "p = 35\n",
        "\n",
        "'''\n",
        "Selecting X as the input image we want to reconstruct\n",
        "'''\n",
        "X = m_src.copy()\n",
        "logging.debug(\"Shape of input X = \" + str((X.shape)))\n",
        "\n",
        "'''\n",
        "Centering the data\n",
        "'''\n",
        "X_mean = np.mean(m_src.copy(), axis=0)\n",
        "Xc = X - X_mean\n",
        "\n",
        "'''\n",
        "Creating data structure for outputs\n",
        "'''\n",
        "X_hat=np.empty(X.shape)\n",
        "\n",
        "\n",
        "'''\n",
        "Computing X_reduced, projection of Xc on the principal components space\n",
        "'''\n",
        "X_reduced = my_pca.projectPC(Xc, k=p)\n",
        "logging.debug(\"Shape of X_reduced = \" + str(X_reduced.shape))\n",
        "\n",
        "'''\n",
        "Reconstructing based on p first components only\n",
        "'''\n",
        "X_hat_centered = my_pca.reconstruct(X_reduced, p, show=False)\n",
        "X_hat = X_hat_centered + X_mean\n",
        "logging.info(\"\\nReconstructed images:\")\n",
        "plot_matrix(X_hat, color, my_color_map, h=4, w=10, transpose=False)\n",
        "\n",
        "'''\n",
        "Visualize original input, for comparison purpose\n",
        "'''\n",
        "logging.info(\"\\nOriginal images:\")\n",
        "plot_matrix(X, color, my_color_map,h=4, w=10, transpose=False)\n",
        "\n",
        "\n",
        "# for axis in ['top','bottom','left','right']:\n",
        "#     fig2.axes[1].spines[axis].set_linewidth(2)\n",
        "#     fig2.axes[1].spines[axis].set_color('white')\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WPCAoAaGHgLy",
        "colab_type": "text"
      },
      "source": [
        "On the above two series of images, the top one is the reconstructed using 35 components, and the bottom one plots the original data.\n",
        "- overall, the quality seems good enough to fully recognize all the faces pretty well, confirming that most of the variance is kept, and the remaining construction loss is small.\n",
        "- several images, however, clearly show this loss (analysis in grayscale):\n",
        "    * image[0,6] contains visibly some extra riddles on the bottom left of the face \n",
        "    * image[2,1] is appears difformed\n",
        "    * image[3,6] shows reminiscence of hair on Bradley Cooper's forehead.\n",
        "    * ...\n",
        "\n",
        "\n",
        "Luckily, it still show some loss in the reconstruction, confirming the previous results established. Nonetheless, the quality is considered good enough to go on with the $p=35$ selected. \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S2Kkdwd456FK",
        "colab_type": "text"
      },
      "source": [
        "####Plot on first two Principal Components\n",
        "PCA is often used as dimensionality reduction technique to represent high dimensional data. Let's show the reconstructed faces on the first two principal components base.\n",
        "\n",
        "*We do that first with the training images, reconstructed using p=35, for information. Later, we will apply the same thing on some test images*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QCv5xuzT0VTx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "my_pca = None\n",
        "\n",
        "X = m_src.copy()\n",
        "my_pca = MyPCA(\"svd\")\n",
        "my_pca.fit(X)\n",
        "\n",
        "data = m_src.copy()\n",
        "data_mean = np.mean(m_src.copy(), axis = 0)\n",
        "data_centered = data - data_mean\n",
        "\n",
        "# my_pca_svd.show_data(X)\n",
        "\n",
        "# reduce the image to the principal components (k=2)\n",
        "data_projected = my_pca.projectPC(data_centered, k=2)\n",
        "logging.debug(\"shape of data_projected = \" + str(data_projected.shape))\n",
        "\n",
        "\n",
        "plt.figure() \n",
        "fig, ax = plt.subplots(1, 1, figsize=(14, 14), sharex=True, sharey=True)\n",
        "\n",
        "eig1 = data_projected[:,0]\n",
        "eig2 = data_projected[:,1]\n",
        "ax.plot(eig1, eig2, 'bo')\n",
        "ax.set_xlabel(\"First Component\")\n",
        "ax.set_ylabel(\"Second Component\")\n",
        "\n",
        "for (x_, y_), img_vector_ in zip(data_projected, X_hat):\n",
        "    img = my_reshape(img_vector_, sq_size, color)\n",
        "    ab = AnnotationBbox(OffsetImage(img, cmap = my_color_map), (x_, y_), frameon=False)\n",
        "    ax.add_artist(ab)\n",
        "\n",
        "ax.grid()\n",
        "plt.show()\n",
        "\n",
        "logging.info(\"\\nIn order to better visualize the plot on top, here is the same view\\nwere personA is in red, and personB is in green\")\n",
        "\n",
        "plt.figure() \n",
        "fig, ax = plt.subplots(1, 1, figsize=(8, 8), sharex=True, sharey=True)\n",
        "ax.plot(eig1[0:20], eig2[0:20], 'ro')\n",
        "ax.plot(eig1[20:40], eig2[20:40], 'go')\n",
        "ax.legend([\"personA\", \"personB\"])\n",
        "labels=[i for i in range(20)]*2\n",
        "for i, txt in enumerate(labels):\n",
        "    ax.annotate(txt, (eig1[i], eig2[i]))\n",
        "plt.show()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L6P-anlyeL3Z",
        "colab_type": "text"
      },
      "source": [
        "###Demo scikit learn\n",
        "\n",
        "Of course, everything that has been done so far regarding PCA can be achieved using dedicated  - and optimized - libraries. For that purpose, we can use the `sklearn.decomposition` package that, among other things, implement the PCA using the *SVD* decomposition that we've looked at. \n",
        "\n",
        "A difference to note is the use, internally, of the `svd_flip(u, vt)`, a function that ensures the vectors to be deterministic, hence solving the sign ambiguity inherent to matrix decomposition, as already discussed above.\n",
        "\n",
        "The following part first performs the same operation as we've implemented before to get familiarized.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lhJkmJSQBtYk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "Let's start --again-- from the training set, processed as a matrix\n",
        "'''\n",
        "logging.info(\"shape of input matrix (n x p) = \" + str(m_src.shape))\n",
        "plot_matrix(m_src, color, my_color_map, h=4, w=10, transpose = False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2l2rE-lgCT29",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "Create pca sklearn object, and compute decomposition\n",
        "'''\n",
        "X = m_src.copy()\n",
        "n_components = 40\n",
        "logging.info(\"Shape input data: \"+str(X.shape))\n",
        "pca_ = sklearn_decomposition_PCA(n_components=n_components) \n",
        "pca_.fit(X)\n",
        "\n",
        "'''\n",
        "Get the eigenfaces\n",
        "'''\n",
        "\n",
        "eigenfaces = pca_.components_\n",
        "logging.debug(\"eigenfaces shape = \" + str(eigenfaces.shape))\n",
        "\n",
        "'''\n",
        "Visualize the eigenfaces, just as we did before\n",
        "'''\n",
        "if color:\n",
        "    eigenfaces_cvt = (eigenfaces*255).astype(np.uint).copy()\n",
        "else:\n",
        "    eigenfaces_cvt = eigenfaces.copy()\n",
        "plot_matrix(eigenfaces_cvt, color, my_color_map, h=4, w=10, transpose=False)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sg0VOAO_DQMD",
        "colab_type": "text"
      },
      "source": [
        "Without any surprise, we find again the same principal components as before.\n",
        "\n",
        "Continuing with sklearn library, we can reconstruct the original data based on the first $p$ components. Hopefully, the results are the same as the ones obtained with the homemade PCA. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5RLJgwgzJrO1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "p=35\n",
        "pca_ = sklearn_decomposition_PCA(n_components=p) \n",
        "pca_.fit(X)\n",
        "X_reduced_sk = pca_.transform(X)\n",
        "X_reconstructed_sk = pca_.inverse_transform(X_reduced_sk)\n",
        "\n",
        "print(X_reconstructed_sk.shape)\n",
        "plot_matrix(X_reconstructed_sk, color, my_color_map, h=4, w=10)\n",
        "logging.info(\"Difference between Homemade PCA and Scikit-learn PCA: \" + str(np.linalg.norm(X_hat - X_reconstructed_sk)))\n",
        "logging.info(\" ==> OK!\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EFJPJS_-yayg",
        "colab_type": "text"
      },
      "source": [
        "We find the exact same images as the one reconstructed using the homemade code `MyPCA`, which is the expected but nonetheless relieving and self-rewarding conclusion!\n",
        "Let's continue with the PCA then..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TqITs-xBjIIE",
        "colab_type": "text"
      },
      "source": [
        "### Projection of Test Faces reconstructed\n",
        "\n",
        "Using the same kind of plot as before, we can reconstruct, **in the same eigenfaces base** the test set images using $p$ first components. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qneLYLvEoAeD",
        "colab_type": "text"
      },
      "source": [
        "We need to apply the sames steps that were applied to the training set, to the test set. that is:\n",
        "\n",
        "\n",
        "1.   centering the data: substracting the mean **of the training set**\n",
        "2.   projecting the resulting centered data on the p first principal components computed by applying PCA on the training data. We won't fit the PCA to the test images, we *just* project the test data using the already-found principal components\n",
        "3.   reconstructing the original data based on those p first principal components, and plot the result of the 40 images on the 2-first components space.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1uNKTkyvi5Sb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "scatter_plot_3D = False\n",
        "scatter_plot_3times = False\n",
        "\n",
        "'''\n",
        "Once again, start by locally copying the data structure\n",
        "> data_test as new data\n",
        "> data_train_mean for centering\n",
        "'''\n",
        "\n",
        "data_test = m_test_src.copy()\n",
        "data_train_mean = np.mean(m_src.copy(), axis = 0)\n",
        "\n",
        "data_test_centered = data_test - data_train_mean\n",
        "\n",
        "'''\n",
        "Perojecting the data onto the p components\n",
        "'''\n",
        "\n",
        "data_test_projected = my_pca.projectPC(data_test_centered, k=p)\n",
        "logging.debug(\"shape of data_projected = \" + str(data_test_projected.shape))\n",
        "\n",
        "'''\n",
        "scatter plot in 3D - 3 first components\n",
        "'''\n",
        "if scatter_plot_3D:\n",
        "    fig = plt.figure(figsize=(8,8))\n",
        "    ax = fig.add_subplot(111, projection='3d')\n",
        "    eig1 = data_test_projected[:,0]\n",
        "    eig2 = data_test_projected[:,1]\n",
        "    eig3 = data_test_projected[:,2]\n",
        "    ax.scatter(eig1[0:10], eig2[0:10], eig3[0:10], 'b')\n",
        "    ax.scatter(eig1[10:20], eig2[10:20], eig3[10:20], 'r')\n",
        "    ax.scatter(eig1[20:30], eig2[20:30], eig3[20:30], 'g')\n",
        "    ax.scatter(eig1[30:40], eig2[30:40], eig3[30:40], 'y')\n",
        "    plt.show()\n",
        "\n",
        "if scatter_plot_3times:\n",
        "    fig = plt.figure(figsize=(24,8))\n",
        "\n",
        "    eig1 = data_test_projected[:,0]\n",
        "    eig2 = data_test_projected[:,1]\n",
        "    eig3 = data_test_projected[:,2]\n",
        "\n",
        "\n",
        "    ax1 = fig.add_subplot(1,3,1)\n",
        "    eig1 = data_test_projected[:,0]\n",
        "    eig2 = data_test_projected[:,1]\n",
        "    eig3 = data_test_projected[:,2]\n",
        "    ax1.plot(eig1[0:10], eig2[0:10], 'bo')\n",
        "    ax1.plot(eig1[10:20], eig2[10:20], 'ro')\n",
        "    ax1.plot(eig1[20:30], eig2[20:30], 'go')\n",
        "    ax1.plot(eig1[30:40], eig2[30:40], 'yo')\n",
        "    \n",
        "    \n",
        "    ax2 = fig.add_subplot(1,3,2)\n",
        "    ax2.plot(eig1[0:10], eig3[0:10], 'bo')\n",
        "    ax2.plot(eig1[10:20], eig3[10:20], 'ro')\n",
        "    ax2.plot(eig1[20:30], eig3[20:30], 'go')\n",
        "    ax2.plot(eig1[30:40], eig3[30:40], 'yo')\n",
        "\n",
        "    ax3 = fig.add_subplot(1,3,3)\n",
        "    ax3.plot(eig2[0:10], eig3[0:10], 'bo')\n",
        "    ax3.plot(eig2[10:20], eig3[10:20], 'ro')\n",
        "    ax3.plot(eig2[20:30], eig3[20:30], 'go')\n",
        "    ax3.plot(eig2[30:40], eig3[30:40], 'yo')\n",
        "\n",
        "    labels=[0,1,2,3,4,5,6,7,8,9]*4\n",
        "    for i, txt in enumerate(labels):\n",
        "        ax1.annotate(txt, (eig1[i], eig2[i]))\n",
        "        ax2.annotate(txt, (eig1[i], eig3[i]))\n",
        "        ax3.annotate(txt, (eig2[i], eig3[i]))\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YlOKOtPlRGT2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "eig1 = data_test_projected[:,0]\n",
        "eig2 = data_test_projected[:,1]\n",
        "\n",
        "'''\n",
        "Reconstructing the data, using p first principal components\n",
        "'''\n",
        "data_test_reconstructed = my_pca.reconstruct(data_test_projected, p, show=False)\n",
        "\n",
        "\n",
        "'''\n",
        "Visualization of the reconstructed data\n",
        "'''\n",
        "\n",
        "plt.figure() \n",
        "fig, ax = plt.subplots(1, 1, figsize=(16, 16), sharex=True, sharey=True)\n",
        "ax.grid()\n",
        "ax.plot(eig1, eig2, 'bo')\n",
        "ax.set_title(\"Visualization of the reconstruced data using p components onto 2 first PC\")\n",
        "\n",
        "for x_, y_, img_vector_ in zip(eig1, eig2, data_test_reconstructed):\n",
        "    img = my_reshape(img_vector_ + data_train_mean, sq_size, color)\n",
        "    ab = AnnotationBbox(OffsetImage(img, cmap = my_color_map), (x_, y_), frameon=False)\n",
        "    ax.add_artist(ab)\n",
        "\n",
        "\n",
        "\n",
        "plt.figure() \n",
        "fig, ax = plt.subplots(1, 1, figsize=(8, 8), sharex=True, sharey=True)\n",
        "eig1 = data_test_projected[:,0]\n",
        "eig2 = data_test_projected[:,1]\n",
        "ax.plot(eig1[0:10], eig2[0:10], 'bo')\n",
        "ax.plot(eig1[10:20], eig2[10:20], 'ro')\n",
        "ax.plot(eig1[20:30], eig2[20:30], 'go')\n",
        "ax.plot(eig1[30:40], eig2[30:40], 'yo')\n",
        "ax.legend([\"personA\", \"personB\", \"personC\", \"personD\"])\n",
        "ax.set_title(\"Visualization of the reconstruced data using p components onto 2 first PC\")\n",
        "\n",
        "labels=[0,1,2,3,4,5,6,7,8,9]*4\n",
        "for i, txt in enumerate(labels):\n",
        "    ax.annotate(txt, (eig1[i], eig2[i]))\n",
        "plt.show()\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jelojYy7RGMf",
        "colab_type": "text"
      },
      "source": [
        "The plots of the test reconstructions confirms the intuition behing the eigenfaces:\n",
        "- Emma Stone, in blue, is mainly on the top; while Bradley Cooper, in red, in mainly on the bottom. They are well separated according to the 2nd eigenface, which seems related to the shape of the face, with a very dark part on the bottom right.\n",
        "- Jane Levy, in green, a woman that resembles to Emma Stone for a human, is mainly on the top of the plot,\n",
        "- Marc Blucas, in yellow, is a white man similar to Bradley Cooper. While half of the points are located in the lower left half, where most of Bradley cooper images also are, the rest of Marc Blucas images is in the middle of blue and green points.\n",
        "\n",
        "Two two first eigenfaces, or principal components, already gives us some of the important information present in the data, even if their cumulative explained variance - fitted for the training set inputs - was actually not that high!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6MNLtaC9of-J",
        "colab_type": "text"
      },
      "source": [
        "Although we will certainly not modify any hyper-parameters based on the test set images, it is still interesting to reproduce the metrics that we built for the training set and the choice of an optimal $p$. It is interesting to answer such questions as:\n",
        "- what is the final relative reconstruction error ?\n",
        "- How does it evolve with p ?\n",
        "- Was $p$ a nice choice, regarding the test sets ?\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s5TqYKddlAip",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "np array containing all the rmse computed\n",
        "\n",
        "if n = 40, max number of components, then rmse has a size 40x41 (0 -> 40 included)\n",
        "> a row matches the rmse of one image wrt the dimension reconstructed. \n",
        "Last column should NOT be 0 as we compute construction of test images (= with loss)\n",
        "'''\n",
        "n = m_test_src.shape[0]\n",
        "\n",
        "rmse_test = np.empty((n,n+1))\n",
        "rmse_test_pc = np.empty((n,n+1)) # rmse in percentage\n",
        "rmse_test_base = np.empty((n,))\n",
        "for i in range(n):\n",
        "    rmse_test_base[i]=my_pca.compute_error(data_test[i], X_train_mean)\n",
        "\n",
        "\n",
        "index_image = 0\n",
        "for img_center_vector in data_test_centered:\n",
        "    X_test_centered_reduced = my_pca.projectPC(img_center_vector, n)\n",
        "    for k in range(n+1):\n",
        "        # from 1 to n, included\n",
        "        if k == 0:\n",
        "            rmse_test[index_image, k] = rmse_test_base[index_image]\n",
        "            rmse_test_pc[index_image, k] = 100 * rmse_test[index_image,k] / rmse_test_base[index_image]\n",
        "        else:\n",
        "            # logging.debug(\"reconstructing using \" + str(p) + \" principal components\")\n",
        "            X_test_hat_centered = my_pca.reconstruct(X_test_centered_reduced, k)\n",
        "            rmse_test[index_image,k] = my_pca.compute_error(img_center_vector, X_test_hat_centered)\n",
        "            rmse_test_pc[index_image, k] = 100 * rmse_test[index_image,k] / rmse_test_base[index_image]\n",
        "        \n",
        "    index_image += 1\n",
        "\n",
        "'''\n",
        "Visualization !\n",
        "'''\n",
        "\n",
        "fig = plt.figure(figsize = (16,16))\n",
        "ax1 = fig.add_subplot(2,2,1)\n",
        "\n",
        "for idx in range(n):\n",
        "    rmse_ = rmse_test[idx,:]\n",
        "    ax1.plot([i for i in range(0,n+1)],rmse_, '-')\n",
        "ax1.set_title(\"reconstruction errors (RMSE) for all test images\")\n",
        "ax1.set_xlabel(\"reconstruction dimension(s) \\'p\\' \")\n",
        "ax1.set_ylabel(\"RMSE\")\n",
        "\n",
        "rmse_test_mean = np.mean(rmse_test, axis = 0)\n",
        "ax2 = fig.add_subplot(2,2,2)\n",
        "ax2.plot([i for i in range(0, n+1)], rmse_test_mean, \"ro-\")\n",
        "ax2.set_title(\"Mean of RMSE for all test images\")\n",
        "ax2.set_xlabel(\"reconstruction dimension(s) \\'p\\' \")\n",
        "ax2.set_ylabel(\"mean of RMSEs\")\n",
        "\n",
        "ax1.set_ylim((0,80))\n",
        "ax2.set_ylim((0,80))\n",
        "ax1.grid()\n",
        "ax2.grid()\n",
        "\n",
        "ax3 = fig.add_subplot(2,2,3)\n",
        "\n",
        "for idx in range(n):\n",
        "    rmse_ = rmse_test_pc[idx,:]\n",
        "    ax3.plot([i for i in range(0,n+1)],rmse_, '-')\n",
        "ax3.set_title(\"reconstruction errors (RMSE) for all test images, in %\")\n",
        "ax3.set_xlabel(\"reconstruction dimension(s) \\'p\\' \")\n",
        "ax3.set_ylabel(\"RMSE\")\n",
        "\n",
        "rmse_test_pc_mean = np.mean(rmse_test_pc, axis = 0)\n",
        "ax4 = fig.add_subplot(2, 2,4)\n",
        "ax4.plot([i for i in range(0, n+1)], rmse_test_pc_mean, \"ro-\")\n",
        "ax4.set_title(\"Mean of RMSE for all test images, in %\")\n",
        "ax4.set_xlabel(\"reconstruction dimension(s) \\'p\\' \")\n",
        "ax4.set_ylabel(\"mean of RMSEs\")\n",
        "\n",
        "ax3.set_ylim((0,110))\n",
        "ax4.set_ylim((0,110))\n",
        "ax3.grid()\n",
        "ax4.grid()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MLl_-HFCZZ_3",
        "colab_type": "text"
      },
      "source": [
        "*Observations*\n",
        "- the reconstruction loss, computed in the same fashion as for the training set images, decreases also as $p$ increases\n",
        "- the slope seems to become near 0 as $p$ is ~35. It comforts us with the choice of $p=35$. However, assessing this parameter on a validation set, or even better performing cross-validation (or leave-one-out cross-validation) would be preferable. On the test set, one could also argue that not much info is gained for the component after the 20th.\n",
        "- using all the components, the remaining error in the reconstruction is still 60% of the error of the base error. there is \"no way\" to do better.\n",
        "> as a reminder, the \"base\" error is the RMSE between the input image, and the mean image of the training set. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DNMZAJ5Iz6DU",
        "colab_type": "text"
      },
      "source": [
        "###PCA Conclusion\n",
        "\n",
        "In this section, we performed a lot :-)! We tried to give the basic idea of the technique, and explained the pre-processing steps required in order to obtain genuine results using PCA (resizing, centering). \n",
        "\n",
        "Then, we have covered some of the math behind the technique, and discussed about the nominal *eigendecomposition*, the mathematical *trick* associated, and the *singular value decomposition*. Those three methods have been fully implemented in a class `MyPCA`and tested against each other. \n",
        "\n",
        "Using `MyPCA`, we have furthermore detailed and visualized what the eigenfaces are, and discussed about the reconstruction to find back our original data. This involves the choice of an *optimal* $p$, number of components used, which is a trade-off between information loss and dimensionality reduction. We discussed abundantly this topic and showed one way to choose $p$. \n",
        "\n",
        "Using this number, we finally plotted the train images **and** the test images onto the 2 first components space, and discussed those results.\n",
        "\n",
        "Finally, we also repeated some steps about eigenfaces generation and reconstruction using a well-known and optimized library `sklearn`, which confirmed all the results obtained using the homemade implementation.\n",
        "\n",
        "If you've reached this line: Congrat's! I know it's dense, but it's worth it!\n",
        "More to come..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SG8p6jkivNVD",
        "colab_type": "text"
      },
      "source": [
        "##Transfer Learning\n",
        "\n",
        "*Doing this project alone as a working student, this part can be skipped*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5bkR41a-n8Dj",
        "colab_type": "text"
      },
      "source": [
        "##Features 2D Visualizations\n",
        "\n",
        "t-SNE  is a quite nice technique for dimensionality reduction used in order to visualize high dimensional data into the 2D (or 3D) space. Other dimensionality reduction techniques often make use of the variance only in order to complete this dimensionality reduction, while t_SNE uses probabilities of being similar (or not). t-SNE stands for *t-Distributed Stochastic Neighbor Embedding*\n",
        "\n",
        "We won't go through the details of the technique, but you can surely find the theory in the [original paper](http://www.jmlr.org/papers/volume9/vandermaaten08a/vandermaaten08a.pdf), and some other complete tutorial in [towardsdatascience](https://towardsdatascience.com/t-sne-python-example-1ded9953f26) for instance. \n",
        "\n",
        "\n",
        "Using this dimensionality reduction technique, and its `sklearn` implementation, we can try and visualize our feature representation built.\n",
        "\n",
        "\n",
        "This technique is not trivial, and some interesting remarks and insights are explained in [How to Use t-SNE Effectively](https://distill.pub/2016/misread-tsne/). In particular, it's important to note the hyperparameters:\n",
        "- `perplexity`, which intuitively is *a guess of the number of close neighbors each point has*\n",
        "- `learning_rate`, very common in iterative methods. \n",
        "\n",
        "Those two parameters are tailored for our application.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tMwenMkxKShf",
        "colab_type": "text"
      },
      "source": [
        "Let's specify what the labels are on the training set:\n",
        "- personA, Emma Stone: 0\n",
        "- personB, Bradley Cooper: 1\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0fJKkSmSMYcU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_train = np.ones((40,))\n",
        "y_train[0:20] = 0\n",
        "\n",
        "logging.info(\"y_train shape   : \" + str(y_train))\n",
        "logging.info(\"y_train sum [20]: \" + str(sum(y_train)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "szqGBDWSMYH1",
        "colab_type": "text"
      },
      "source": [
        "Util function to generate a colored and scattered plot, inspired by [datacamp](https://www.datacamp.com/community/tutorials/introduction-t-sne)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lJmP5O5tCEWe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Utility function to visualize the outputs t-SNE\n",
        "# inspired by https://www.datacamp.com/community/tutorials/introduction-t-sne\n",
        "\n",
        "def tsne_scatter(x, colors, title):\n",
        "    # choose a color palette with seaborn.\n",
        "    num_classes = len(np.unique(colors))\n",
        "    palette = np.array(sns.color_palette(\"bright\", num_classes))\n",
        "    # palette = sns.color_palette(\"bright\", num_classes)\n",
        "    # create a scatter plot.\n",
        "    f = plt.figure(figsize=(8, 8))\n",
        "    ax = f.add_subplot(111)\n",
        "    sc = ax.scatter(x[:,0], x[:,1], lw=0, s=40, c=palette[colors.astype(np.int)])\n",
        "    ax.set_title(title)\n",
        "    ax.grid()\n",
        "\n",
        "    # # add the labels for each digit corresponding to the label\n",
        "    txts = []\n",
        "\n",
        "    for i in range(num_classes):\n",
        "        # Position of each label at median of data points.\n",
        "        xtext, ytext = np.median(x[colors == i, :], axis=0)\n",
        "        txt = ax.text(xtext, ytext, str(i), fontsize=24)\n",
        "        txts.append(txt)\n",
        "\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W8kFwbXHUFf7",
        "colab_type": "text"
      },
      "source": [
        "Let's generate a random number based on a seed, for the sake of reproducibility"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NWNo73nyULsG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "random.seed(8042020)\n",
        "rand_nb = random.randrange(0,1000)\n",
        "\n",
        "logging.info(\"Random Number generated is: \" + str(rand_nb))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uGOGcDaZzv6u",
        "colab_type": "text"
      },
      "source": [
        "Now, we can call the t-sne `sklearn` implementation, using two tailored parameters:\n",
        "- `perplexity` is set to the theoretical number of neighbours, that we now at this point, \n",
        "- `learning_rate` is set to a small value which seems a good balance. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HWCj7lf5N40N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tsne_hog = sklearn.manifold.TSNE(random_state=rand_nb, perplexity=20, learning_rate=50.0)\n",
        "X_HOG_embedded = tsne_hog.fit_transform(X_HOG_train)\n",
        "\n",
        "tsne_scatter(X_HOG_embedded, y_train, \"Projection of HOG features of the training set using t-SNE\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z0Z5vbAzz-EN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train = get_matrix_from_set(training_set, color, sq_size,flatten=True)\n",
        "p = 35\n",
        "pca_ = sklearn_decomposition_PCA(n_components=p) \n",
        "pca_.fit(X_train)\n",
        "X_PCA_train = pca_.transform(X_train)\n",
        "logging.info(\"X_PCA_train shape: \" + str(X_PCA_train.shape))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CM5JABN9tkJQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tsne_pca = sklearn.manifold.TSNE(random_state=rand_nb, perplexity=20, learning_rate=50.0)\n",
        "X_PCA_embedded = tsne_pca.fit_transform(X_PCA_train)\n",
        "\n",
        "tsne_scatter(X_PCA_embedded, y_train, \"Projection of PCA features of the training set using t-SNE\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hd4saeMjOYQg",
        "colab_type": "text"
      },
      "source": [
        "###*Comments on the t-SNE plots*\n",
        "\n",
        "Before going into the comments, let's first remind two of the six key messages taken from the deep analysis in [How to Use t-SNE Effectively](https://distill.pub/2016/misread-tsne/).\n",
        "1. Hyper-parameters really matters,\n",
        "2. Cluster sizes in a t-SNE plot mean nothing,\n",
        "3. Distances between well-separated clusters in a t-SNE plot may mean nothing\n",
        "\n",
        "\n",
        "> This analysis was performed with `color = False`, meaning the PCA computed in grayscale. The results, specifically t-SNE reproduction, are of course influenced by this change.\n",
        "\n",
        "---\n",
        "\n",
        "The two different plots, for the two feature representations, are built with the hyperparameter `perplexity` set to the theoretical number of neighbours. The hyperparameter `learning_rate` was also modified to try and cope with the problem. Changing those values change the results; as is, it seems to give pretty *interesting* results. \n",
        "\n",
        "\n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XKL-HqG2IB6H",
        "colab_type": "text"
      },
      "source": [
        "###*Interesting* result ?\n",
        "\n",
        "Those two graphs are based on the feature representations built: HOG and PCA. Those are high dimensional features, and t-SNE technique is applied to project, in a non-linear fashion, the features onto a 2D figure. \n",
        "\n",
        "The two plots shows what we *hope* to find: some notion of distances/similarities between points of the same class. \n",
        "- the plots indicate that in both cases, the features seem to be (mostly)separable between the classes, even not linearly in 2D after projection. This is a nice and promising result to build upon. \n",
        "- Considering the size and distance between the cluster, it should not really matters in the analysis as reminded above\n",
        "- As announced by the litterature, the `perplexity` hyperparameter matters a lot.\n",
        "\n",
        "\n",
        "Intuitively, one could say -- based on the plots above -- that several classifiers may work better than others for certain points. \n",
        "> for instance, 3-NN may not work well if an image has a HOG feature representation projected onto [2, 4.5] or a PCA feature projected onto [2, 4] by the t-SNE transformation. \n",
        "\n",
        "This kind of intuition may be erroneous, due to the high non-linearity inherent to this transformation. Let's be cautious then, and verify those feelings/ideas in the next steps (see Identification part).\n",
        "While t-SNE is a great technique for visualization purpose, one shall remain careful regarding the conclusion drawn based on the plots."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lgl_oMoQp4ny",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# tsne of the test set for the HOG feature\n",
        "\n",
        "# tsne_hog_test = sklearn.manifold.TSNE(random_state=rand_nb, perplexity=10, learning_rate=50.0)\n",
        "# X_HOG_embedded = tsne_hog.fit_transform(X_HOG_test)\n",
        "# y_test = np.ones((40,))\n",
        "# y_test[0:10]=0\n",
        "# y_test[20:30]=2\n",
        "# y_test[30:40]=3\n",
        "# tsne_scatter(X_HOG_embedded, y_test, \"Projection of HOG features of the training set using t-SNE\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Djz9YO4Guhr",
        "colab_type": "text"
      },
      "source": [
        "# Exploit Feature Representations\n",
        "\n",
        "In this part, we will use the representations learnt before in order to build a classifier and identification system. \n",
        "> From an academic stand point, I was exempted of performing the *Classification* topic, being alone on this project as a working student. I appreciate the flexibility. However, as part of the *Impress your TA's*, and as this ought to be a *fun* part, I have decided to cover this topic as well.\n",
        "\n",
        "In the next two parts I will construct a classification and an identification system for each of the feature representations, HOG then PCA, and I will qualitatively and quantitatively compare the results obtained. \n",
        "\n",
        "> In the analysis, unless specified otherwise, we assume `color=False` and `sq_size=64`. Results with other parameters may differ."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3_lFbvTnNGje",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def show_missed(X_test, y_test, y_predict):\n",
        "    missed = np.where( np.array(y_test != y_predict))\n",
        "    correct = np.where( np.array(y_test == y_predict))\n",
        "\n",
        "    if len(missed[0]) > 0:\n",
        "        logging.info(\"Mis-classified images: \")\n",
        "        logging.info(\"Index: \" + str(missed[0]))\n",
        "        plot_matrix(X_test[missed[0],:], color, my_color_map, h=1, w=len(missed[0]))\n",
        "    \n",
        "    logging.info(\"Properly classifier images: \")\n",
        "    logging.info(\"Index: \" + str(correct[0]))\n",
        "    plot_matrix(X_test[correct[0],:], color, my_color_map, h=2, w=1+len(correct[0])//2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GO2I8rHUHRuC",
        "colab_type": "text"
      },
      "source": [
        "## Classification - howto\n",
        "While the following sections may appear quite dense, the overall idea is the following:\n",
        "1. pre-process data\n",
        "    1. Resizing to appropriate size, as already discussed in the first parts of this tutorial. Typically, the size is smaller than all images in the trainging and test set. \n",
        "    2. color or grayscale, as this notebook is fully compatible with both.\n",
        "    3. shuffle the (ordered) training set\n",
        "    \n",
        "2. compute feature representation, as described in the previous part\n",
        "3. train the corresponding classifier using training sets\n",
        "\n",
        "4. apply the classifier on unseen images (test)\n",
        "    * apply same preprocessing as for training (except shuffling)\n",
        "    * predict\n",
        "    * compute metrics\n",
        "    * observations and discussions\n",
        "\n",
        "In case you're lost, don't hesitate to go back a few steps - keeping an eye on the table of content.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yLF5zzo4IFBZ",
        "colab_type": "text"
      },
      "source": [
        "### Preprocess the data\n",
        "\n",
        "- Let's just check the \"meta-hyper-parameters\": color and size.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nk6XzxXHH1q3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "logging.info(\"smallest px shall be less than: \" + str(min(min(get_min_size(training_set)), min(get_min_size(test_set)))))\n",
        "logging.info(\"hyper-parameter sq_size : \" + str(sq_size))\n",
        "logging.info(\"hyper-parameter color   : \" + str(color))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gqBussWgOwAd",
        "colab_type": "text"
      },
      "source": [
        "The smallest face crop that we have in training set and test set is (70,70), so we can without issue rescale all our face crops to (64,64) as part of the data pre-processing. \n",
        "\n",
        "> In this notebook, this resizing is handled with the parameter `sq_size = 64`, set at the very beginning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SI3IY9ZV98lq",
        "colab_type": "text"
      },
      "source": [
        "- Get raw training data.\n",
        "\n",
        "Those are the raw face cropped from personA and personB. \n",
        "In order to be sure not to be disturbed by previous execution or previous code snipper, let's just get those data again.\n",
        "> this is not a performance issue to repeat this step considering the relative small amount of data we actually deal with.\n",
        "\n",
        "- Resize the raw images to a common image size\n",
        "\n",
        "The image size is define by `sq_size`, and we need to resize the training raw image accordingly.\n",
        "\n",
        "Those two actions are performed in a single function. Code is of course provided. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_kb7kzbG3cTB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train = get_matrix_from_set(training_set, color, sq_size = sq_size, flatten = False)\n",
        "X_test = get_matrix_from_set(test_set, color, sq_size = sq_size, flatten = False)\n",
        "y_train = np.zeros((40,))\n",
        "y_train[20:40] = 1\n",
        " \n",
        "'''\n",
        "For now, set up \"0\" for personC; \"1\" for personD !\n",
        "see discussion in a later cell.\n",
        "'''\n",
        "y_test = np.zeros((40,))\n",
        "y_test[10:20] = 1\n",
        "y_test[30:40] = 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jx2ZQXHP0WLf",
        "colab_type": "text"
      },
      "source": [
        "#### Shuffling\n",
        "\n",
        "The training mathematical methods are of course numerical methods, for which the ordening of the training input may have its influence. In order to prevent as much as possible this bias, the classifier `fit` method automatically shuffle the training data between each epoch, unless specified otherwise. We leave the default parameter to ensure this shuffling. Nonetheless, as the training data are currently completely ordered, we introduce a pre-shuffling at this stage, before starting the very first run.\n",
        "\n",
        "For the sake of repeatability, we seed the RNG, as before in this tutorial.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mkPgqls10eKR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def shuffle_training(X_train, y_train):\n",
        "    np.random.seed(0)\n",
        "    np.random.shuffle(X_train)\n",
        "    np.random.seed(0)\n",
        "    np.random.shuffle(y_train)\n",
        "    np.random.seed()\n",
        "\n",
        "shuffle_training(X_train, y_train)  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bLxuFSFE-lW6",
        "colab_type": "text"
      },
      "source": [
        "In order to make sure to keep those data as we may need them in a future (optimization ;-) ) step, let's create some \"backup\" variables. It is ok to do that as the amount of data remain small."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h4JQBgsJdFVu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train_HOG_shuffled_back = X_train.copy()\n",
        "y_train_HOG_shuffled_back = y_train.copy()\n",
        "X_test_HOG_back = X_test.copy()\n",
        "y_test_HOG_back = y_test.copy()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "99UpN-pO_DHS",
        "colab_type": "text"
      },
      "source": [
        "###Visualization of the training images (shuffled)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hr-i3JSN7nYn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "logging.info(\"Training set :\")\n",
        "logging.info(\"Training set shuffled [0 -> 19]:\")\n",
        "plot_matrix(X_train[0:20,:], color, my_color_map, h=1, w=20, transpose = False)\n",
        "logging.info(\"Training set shuffled [20 -> 39]:\")\n",
        "plot_matrix(X_train[20:40,:], color, my_color_map, h=1, w=20, transpose = False)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iqPcy3Wb--YS",
        "colab_type": "text"
      },
      "source": [
        " Obviously, we see that Emma Stone and Bradley Cooper are now interleaved (at least, their images)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cDTMdPL23g-a",
        "colab_type": "text"
      },
      "source": [
        "###Visualization of the test images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tp2e63nG5j5E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "logging.info(\"Test set :\")\n",
        "logging.info(\"Test set PersonA [0 -> 9]:\")\n",
        "plot_matrix(X_test[0:10, :], color, my_color_map, h=1, w=10, transpose = False)\n",
        "logging.info(\"Test set PersonB [10 -> 19]:\")\n",
        "plot_matrix(X_test[10:20, :], color, my_color_map, h=1, w=10, transpose = False)\n",
        "logging.info(\"Test set PersonC [20 -> 29]:\")\n",
        "plot_matrix(X_test[20:30, :], color, my_color_map, h=1, w=10, transpose = False)\n",
        "logging.info(\"Test set PersonD [30 -> 39]:\")\n",
        "plot_matrix(X_test[30:40, :], color, my_color_map, h=1, w=10, transpose = False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c7rzLV5N2NuR",
        "colab_type": "text"
      },
      "source": [
        "###Standard scaler\n",
        "\n",
        "It is often a question of \"should we scale our features or not\" ? Scaling as in \"have a variance between 0 and 1\".\n",
        "\n",
        "While one could argue it's always better, in the context of this tutorial, we will not. \n",
        "The very essence of the inputs are pixel intensities: they already are on the same scale of data, and there isn't order of magnitude differences between them. Of course, it does not lead to having scaled feature representations...Yet, it does not seem to matter very much in our problem.\n",
        "\n",
        "As it does not strictly participate to the educative goal of this tutorial, I dediced not to include the scaler step in the different systems (or Pipeline, as we will call them). \n",
        "Nonetheless, if one wanted to try out, a common scaler is `sklearn.preprocessing.StandardScaler()`.\n",
        "\n",
        "If the data are not scaled, they however **do need** to be centered for the PCA technique, as discussed previously. This does not change. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K2w_3OUNVmOx",
        "colab_type": "text"
      },
      "source": [
        "## HOG Classification\n",
        "\n",
        "In this section, we focus on the classifier based on the HOG feature representation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bVtgNOmB6BAH",
        "colab_type": "text"
      },
      "source": [
        "Util function to plot side by side an color face, and its HOG descriptor, for educative purpose"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VjQkjaOB4aw6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def show_one_image_hog(idx_of_interest, person=personA, set=\"training\"):\n",
        "    if set == \"training\":\n",
        "        source_set = training_set\n",
        "        hog_set = hog_training\n",
        "    elif set == \"test\":\n",
        "        source_set = test_set\n",
        "        hog_set = hog_test\n",
        "\n",
        "\n",
        "    image_of_interest = source_set[person][idx_of_interest]\n",
        "    hog_of_interest = hog_set[person][idx_of_interest]\n",
        "\n",
        "    '''\n",
        "    Visualization of the image and its hog selected as image_of_interest\n",
        "    '''\n",
        "    fig, (ax0, ax1) = plt.subplots(1,2,figsize = (8,4), sharex=False, sharey=False)\n",
        "\n",
        "    ax0.imshow(cv2.cvtColor(image_of_interest, cv2.COLOR_BGR2RGB))\n",
        "    ax0.set_title(\"Face \\'barely\\' properly classified\")\n",
        "\n",
        "    ax1.imshow(hog_of_interest[1])\n",
        "    ax1.set_title(\"Visualization of the HOG of interest\")\n",
        "\n",
        "    logging.info(\"Shape of the descriptor       : \" + str(hog_of_interest[0].shape))\n",
        "    logging.info(\"Shape of the descriptor (visu): \" + str(hog_of_interest[1].shape))\n",
        "    logging.info(\"Shape of the image of interest: \" + str(image_of_interest.shape))\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ztVlTGrVE0M",
        "colab_type": "text"
      },
      "source": [
        "### Construction of HOG Transformer\n",
        "Following the method and advices of [Kapernikov](https://kapernikov.com/tutorial-image-classification-with-scikit-learn/)\n",
        "\n",
        "####[Kzako](https://forum.wordreference.com/threads/k%C3%A9zako.245210/)?\n",
        "We won't go into the details of the computer science design pattern leading to the Transformer building by `sklearn`, but in very essence, a transformer is a class that takes some input and perform some transformation on that, depending (possibly) on extra parameters. \n",
        "\n",
        "We actually already used such a transformer during the PCA demo using `sklearn`, and more specifically `sklearn.decomposition.PCA`. This `PCA` class is a `Transformer` because it inherits from `BaseEstimator` and `TransformerMixin`.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BXeow_gYCJvF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import inspect\n",
        "tuple_ancestor = inspect.getmro(sklearn_decomposition_PCA)\n",
        "for ancestor_class in tuple_ancestor:\n",
        "    print(\"child of \" + str(ancestor_class))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "khhxVIj_DOZG",
        "colab_type": "text"
      },
      "source": [
        "In particular, `sklearn.decomposition.PCA` implements -among others- the methods `fit` and `transform`, which are required to be called a `Transformer`.  A Transformer helps in defining a systematic way of performing some actions on the data. \n",
        "\n",
        "We can build our own transformer, called `HogTransformer`, make it inheriting of `BaseEstimator` and `TransformerMixin` and benefit from the same capabilities. \n",
        "\n",
        "*Don't worry if it's still fuzzy, it'll become clearer when we will use it.*\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tddO9f8APPda",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class HogTransformer(BaseEstimator, TransformerMixin):\n",
        "    '''\n",
        "    Expects an array of 2D arrays (1 channel images)\n",
        "    Calculates hog features for each image\n",
        "    '''\n",
        "    def __init__(self, y=None, \n",
        "                 orientations = 9, \n",
        "                 pixels_per_cell = (8,8), \n",
        "                 cells_per_block = (2,2),\n",
        "                 block_norm = \"L2-Hys\", \n",
        "                 transform_sqrt = False,\n",
        "                 multichannel = False):\n",
        "        self.y = y\n",
        "        self.orientations = orientations\n",
        "        self.pixels_per_cell = pixels_per_cell\n",
        "        self.cells_per_block = cells_per_block\n",
        "        self.block_norm = block_norm\n",
        "        self.transform_sqrt = transform_sqrt\n",
        "        self.multichannel = multichannel # default is grayscale\n",
        "    \n",
        "    def fit(self, X, y=None):\n",
        "        logging.debug(\"[HOGTransformer.fit] X.Shape \" + str(X.shape))\n",
        "        return self\n",
        "    \n",
        "    def transform(self, X, y=None):\n",
        "        logging.debug(\"[HOGTransformer.transform] X.Shape \" + str(X.shape))\n",
        "\n",
        "        def local_hog(X):\n",
        "            if self.multichannel:\n",
        "                X_ = X.copy() #.T\n",
        "                # logging.debug(\"TO CHECK IF TRANSPOSE STILL NEEDED ?\")\n",
        "                # logging.debug(\"[HOGTransformer.transform] (1) X.Shape \" + str(X.shape))\n",
        "                # logging.debug(\"[HOGTransformer.transform] (2) X_.Shape \" + str(X_.shape))\n",
        "            else:\n",
        "                X_ = X.copy()\n",
        "            # logging.debug(\"[HOGTransformer.transform.local_HOG]\" )\n",
        "            # cv2_imshow(X_)\n",
        "            return skimage_feature_hog(X_,\n",
        "                                       orientations = self.orientations, \n",
        "                                       pixels_per_cell = self.pixels_per_cell,\n",
        "                                       cells_per_block = self.cells_per_block,\n",
        "                                       block_norm = self.block_norm,\n",
        "                                       visualize = False, \n",
        "                                       transform_sqrt = self.transform_sqrt, \n",
        "                                       feature_vector = True, \n",
        "                                       multichannel = self.multichannel)\n",
        "\n",
        "        try: \n",
        "            # tmp = [str(image.shape) for image in X]\n",
        "            # logging.debug( str(tmp) )\n",
        "            return np.array([local_hog(image) for image in X])\n",
        "        except ValueError as ve:\n",
        "            logging.error(str(ve))\n",
        "        except NameError as ne:\n",
        "            logging.error(str(ne))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kMSvINfaF_zo",
        "colab_type": "text"
      },
      "source": [
        "/!\\ A careful reader could recommend to also build a Transformer for the color conversion according to `color` attribute, as well as the resizing according to the `sq_size` attribute. \n",
        "\n",
        "==> That is completely True !\n",
        "\n",
        "Nonetheless, this part of the code has been covered much later that the pre-processing steps, and it is not mandatory to cover the full scope of this tutorial. \n",
        "That being said, a future version could indeed replace the utility functions handling those `color` and `sq_size` parameters as Transformers.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ajQ4PN5asVjc",
        "colab_type": "text"
      },
      "source": [
        "From the previous section, we have ready the training data `X_train` and `y_train`. Let's apply (= fit, then transform) the `HogTransformer`. \n",
        "\n",
        "We log the final shape of the training data hog representations."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "mef1tejVsYN6",
        "colab": {}
      },
      "source": [
        "\n",
        "# scalify = StandardScaler()\n",
        "hogify = HogTransformer(    orientations = 9, \n",
        "                            pixels_per_cell = (8,8), \n",
        "                            cells_per_block = (2,2),\n",
        "                            block_norm = \"L2\", \n",
        "                            transform_sqrt = True,\n",
        "                            multichannel = color)\n",
        "\n",
        "X_train_hog = hogify.fit_transform(X_train)\n",
        "# X_train_prepared = scalify.fit_transform(X_train_hog)\n",
        "X_train_hog_prepared = X_train_hog\n",
        "\n",
        "logging.info(\"X_train prepared for HOG classification. Shape: \" + str(X_train_hog_prepared.shape))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DfC-WnSiH7Dp",
        "colab_type": "text"
      },
      "source": [
        "Now that we have the training data ready to train the classifier, let's go!\n",
        "\n",
        "Many possibilities exist, and let's try out a stochastic gradient descent classifier, from `sklearn`.\n",
        "In a first step, we can leave most of the parameters as is. Considering the loss function and penalty parameters, it leads to a linear SVM classifier, see [sklearn SGDClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html) for more details.\n",
        "\n",
        "When created, we can call the function `fit` to train the classifier. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iM3bM3va1Pc2",
        "colab_type": "text"
      },
      "source": [
        "####Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BSnOwwFICgPZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sgd_clf = SGDClassifier(random_state=42, max_iter=1000, tol=1e-6, verbose = 1, shuffle=True)\n",
        "sgd_clf.fit(X_train_hog_prepared, y_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "04ItKJqGRhzy",
        "colab_type": "text"
      },
      "source": [
        "The classifier is now trained.\n",
        "\n",
        "We can test the result on the test sets. \n",
        "\n",
        "> Important note: there was no validation set dedicated to parameters evaluation. While this is a good practice, it's not crucial for the moment as the goal is to demonstrate the methodology. In a few sections, while we will discuss optimization of the classifier, we will realize cross-validation using dedicated tools from `sklearn`.\n",
        "\n",
        "\n",
        "We need to apply the same pre-processing steps as we did on the training set. As we already resized and set up the color according to global hyperparameters, we just need to apply the `transform`method of the `Hog_Transformer`\n",
        "\n",
        "> Why not fit? The `fit` method will never be applied on the test data, as it implies tuning the Transfomer for the data - which we don't want with the test set. For the present HOG case, it does not change anything as there is nothing done in the method; for other cases such as PCA, this is highly important as we don't want the Principal Components to be modified by the test set.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UIJOJVF3TnOs",
        "colab_type": "text"
      },
      "source": [
        "###Test on personA and personB\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8wh9Yt4L0LOw",
        "colab_type": "text"
      },
      "source": [
        "####Gathering and processing the test data\n",
        "\n",
        "Those two persons correspond to the persons of the training set. Their indices in the test set are from [0,19] so that we can create `X_test_ab` that contains the input data for the test only of those 2 persons.\n",
        "\n",
        "Similarly, we extract the labels in `y_test_ab`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "183bDFIGELk1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_test_ab = X_test[0:20,:]\n",
        "y_test_ab = y_test[0:20]\n",
        "\n",
        "print(\"X_test    shape -> \" + str(X_test.shape))\n",
        "print(\"X_test_ab shape -> \" + str(X_test_ab.shape))\n",
        "print(\"sum y_test_ab [10]      -> \" + str(sum(y_test_ab)))\n",
        "\n",
        "'''\n",
        "Application of the hog transform\n",
        "'''\n",
        "X_test_hog_ab = hogify.transform(X_test_ab)\n",
        "X_test_hog_ab_prepared = X_test_hog_ab\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fpQMMCnfAE87",
        "colab_type": "text"
      },
      "source": [
        "As before, let's save those as backup variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9t0EHWwQvrtE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_test_HOG_ab_back = X_test_ab.copy()\n",
        "y_test_HOG_ab_back = y_test_ab.copy()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4MmJHfciUHeh",
        "colab_type": "text"
      },
      "source": [
        "####Tests and metrics\n",
        "we use our classifier to predict the labels of the test inputs. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TmLqEffkUOe_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_pred_hog_ab = sgd_clf.predict(X_test_hog_ab_prepared)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iz40gfwsUbMC",
        "colab_type": "text"
      },
      "source": [
        "Now, we can compute the accuracy for the classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dIv6c4JHUb00",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "accuracy = np.sum(y_pred_hog_ab == y_test_ab)/len(y_test_ab)\n",
        "logging.info(\"Accuracy: \" + str(accuracy) )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ua0qFCObVL8v",
        "colab_type": "text"
      },
      "source": [
        "Note that if we are really not sure about how to compute the accuracy, or if we prefer to use library functions, we can!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "46qg9h7_VLl-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "logging.info(\"Accuracy [SKlearn]: \" + str(metrics.accuracy_score(y_test_ab, y_pred_hog_ab )))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HTIRYZvGVq4I",
        "colab_type": "text"
      },
      "source": [
        "Luckily, both accuracy give the same answers ;-)!\n",
        "#####Summary\n",
        " \n",
        "- we built a Transformer that modifies the input (= raw images) into HOG features\n",
        "- we created a stochastic Gradient descent classifier\n",
        "- we trained this classifier using the HOG feature\n",
        "- we transformed the test set raw images into HOG features\n",
        "- we tested the classifier against those test HOG features, for personA (= class 0) and personB (=class 1)\n",
        "- we computed the accuracy\n",
        "\n",
        "It is not 100%, and it may be interesting to observe the failures."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ITqw0y7dWsDq",
        "colab_type": "text"
      },
      "source": [
        "####Visualization: Confusion Matrix\n",
        "\n",
        "Let's create a (very simple) confusion matrix. \n",
        "The goal is to see where are the faulty classification. In other words, in this simple classification that we run, what were the failing images ?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wb1HK9yEWHPF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import plot_confusion_matrix\n",
        "\n",
        "'''\n",
        "Confusion matrix\n",
        "'''\n",
        "def my_plot_confusion_matrix(classifier, X_test, y_test, true_labels, predicted_labels=None):\n",
        "    if predicted_labels == None:\n",
        "        predicted_labels = true_labels\n",
        "    \n",
        "    titles_options = [(\"Confusion matrix, without normalization\", None),\n",
        "                    (\"Normalized confusion matrix\", 'true')]\n",
        "    for title, normalize in titles_options:\n",
        "        disp = plot_confusion_matrix(classifier, \n",
        "                                    X_test, \n",
        "                                    y_test,\n",
        "                                    display_labels=None,\n",
        "                                    cmap=plt.cm.viridis,\n",
        "                                    normalize=normalize,\n",
        "                                    include_values=True, \n",
        "                                    xticks_rotation='horizontal', \n",
        "                                    values_format=None)\n",
        "        disp.ax_.set_title(title)\n",
        "        disp.ax_.set_xticklabels(predicted_labels)\n",
        "        disp.ax_.set_yticklabels(true_labels)\n",
        "\n",
        "        plt.show()\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mkYFiPjIKHan",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "true_labels= [\"Emma Stone\", \"Bradley Cooper\"]\n",
        "my_plot_confusion_matrix(sgd_clf,\n",
        "                         X_test_hog_ab_prepared, \n",
        "                         y_test_ab,\n",
        "                         true_labels = true_labels,\n",
        "                         predicted_labels = true_labels) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J8e51G-dZ9Dx",
        "colab_type": "text"
      },
      "source": [
        "The result is pretty clear: all the images that were misclassified (4) are Emma Stone faces that were classified as Bradley Cooper faces.\n",
        "\n",
        "Let's dig into the analysis to see what precisely are those images.\n",
        "- let's find which is index of the misclassification, \n",
        "- let's show the test images, separated from all the others."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OfzJRgnBZ7ll",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mis_idx = np.where( np.array(y_test_ab - y_pred_hog_ab) != 0)\n",
        "logging.info(\"Indices misclassified: \" + str(mis_idx[0]))\n",
        "\n",
        "images_misclassified = X_test_ab[mis_idx[0]].copy()\n",
        "logging.info(\"Images misclassified: \")\n",
        "plot_matrix(images_misclassified, color, my_color_map, h=1, w=mis_idx[0].shape[0])\n",
        "\n",
        "logging.info(\"Images correctly classified: \")\n",
        "correct_idx = np.where(np.array(y_test_ab - y_pred_hog_ab) == 0)\n",
        "remaining_images = X_test_ab[correct_idx[0]].copy() \n",
        "plot_matrix(remaining_images, color, my_color_map, h=2, w=1+correct_idx[0].shape[0]//2)\n",
        "\n",
        "\n",
        "logging.info(\"Again - Overview of the training data to ease the understanding...\")\n",
        "plot_matrix(X_train, color, my_color_map, h=4, w=10)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DDaElTHKf0xl",
        "colab_type": "text"
      },
      "source": [
        "####Observations and Discussions on test results\n",
        "\n",
        "> those observations are done with `color = False` and `sq_size = 64`\n",
        "\n",
        "The good thing with working with images is that we may get a better understanding by simply looking at the high dimensional input data: the raw images. \n",
        "Images #1, #2 and #7 and #9 are misclassified. They are all personA images, and we will focus more on those images first.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WuRG9l5z1Qi3",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "#####personA (mis)classification\n",
        "- Images #2, #7, #9 appear visually really different from all the others: hair color and haircut are dramatically different than most of Emma Stone faces. \n",
        "\n",
        "To be more convinced, let's come back to the HOG feature descriptor, for an image we've already seen.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Byz3sqH_huIf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "show_one_image_hog(2, person=personA, set=\"training\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "55zUki1niypr",
        "colab_type": "text"
      },
      "source": [
        "On this descriptor, it appears clear that the haircut plays a key role in the description of the face, as visible in the top right corner where a clear oblique line is visible. This line cannot be present in the descriptor of the test set images #2 nor #7 nor #9, leading to a harder classification.\n",
        "\n",
        "- Image #1 is also misclassified. It may be more difficult to understand why there was a mistake there. Some intuition however:\n",
        "    - it is the only image from the person A training and test sets that has this rotation\n",
        "    - the haircut is - besides rotated - not as sharp as most of the other images\n",
        "    - there is a strong dark background on the left, leading to a large gradient magnitude at this side, which is not present for other personA image.\n",
        "\n",
        "\n",
        "To confirm all those intuitions, let's look at the real descriptor used for those misclassified image. We are used to these image from the first part of this tutorial."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gvEJw8C-klw9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "computation of the HOG descriptor for the misclassified images\n",
        "'''\n",
        "hog_missed = []\n",
        "for index in mis_idx[0]:\n",
        "    hog_image = hog_test[personA][index][1]\n",
        "    hog_missed.append(hog_image)\n",
        "\n",
        "fig, ax_ = plt.subplots(1,mis_idx[0].shape[0],figsize = (16,4), sharex=True, sharey=True)\n",
        "\n",
        "for count in range(len(ax_)):\n",
        "    ax_[count].imshow(hog_missed[count])\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tb2MY0nKa97r",
        "colab_type": "text"
      },
      "source": [
        "Our assumptions seem to cope with the descriptors:\n",
        "1. Image #1 is rotated, the haircut doesn't lead to a clear difference, and the background on the left is captured by the descriptor\n",
        "2. Images #2, #7 and #9 indeed do not show the haircut.\n",
        "\n",
        "##### Decision Boundaries\n",
        "Another information given by the classifier confirms the intuition that\n",
        "- image #1 seems more alike the others, but the rotation makes it harder to be classified, \n",
        "- image #2, #7, #9 don't have a key element of the descriptor, hence are more easily classified wrongly. \n",
        "\n",
        "This is confirmed by the *decision function* from the classifier, which returns the distance with respect to the boundary line. As stated in the documentation, it predicts the confidence scores for samples which is the signed distance of that sample to the hyperplane, see [sklearn source](https://github.com/scikit-learn/scikit-learn/blob/95d4f0841/sklearn/linear_model/_base.py#L247)\n",
        "\n",
        ">  $  distance \\gt 0 \\Rightarrow class = 1$\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LoDjFy1Bss3v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "Printing classifier scores (= distance to decision plane)\n",
        "'''\n",
        "scores = sgd_clf.decision_function(X_test_hog_ab_prepared)\n",
        "logging.info(\"scores (decision functions): \" + str(scores))\n",
        "\n",
        "'''\n",
        "Visualization\n",
        "'''\n",
        "# plt.figure(figsize=(8,8))\n",
        "fig, ax = plt.subplots(1, 1, figsize=(8,8))\n",
        "scores_a = scores[0:10]\n",
        "scores_b = scores[10:20]\n",
        "ax.scatter(range(len(scores_a)), scores_a)\n",
        "ax.scatter(range(len(scores_a), len(scores_a)+len(scores_b)), scores_b)\n",
        "ax.plot(range(len(scores)), np.zeros((len(scores),)),\"r-\")\n",
        "ax.set_title(\"Visualization of distance to classification boundary\")\n",
        "ax.legend([\"Boundary\", \"personA\", \"personB\"])\n",
        "ax.set_xlabel(\"test image index\",fontsize=12)\n",
        "ax.set_ylabel(\"Distance to boundary decision\",fontsize=12)\n",
        "if not color:\n",
        "    ax.set_xlim(left=-0.5, right=20.5)\n",
        "    ax.set_ylim(bottom=-275.0, top=275.0)\n",
        "    ax.text(2,150,\"Classified as \\nPersonB\",fontsize=12)\n",
        "    ax.text(12,-150,\"Classified as \\nPersonA\",fontsize=12)\n",
        "    \n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gEe4S6uGyeGq",
        "colab_type": "text"
      },
      "source": [
        "On the above plot, personA is in blue, on the left part (indices 0 -> 9 included) and personB is on the right side, in orange, (indices 10 -> 19 included). \n",
        "\n",
        "From this graph, everything that is said is confirmed:\n",
        "* 4 images from personA are in the wrong side of the line, hence misclassified as personB instead of personA,\n",
        "* misclassified images #1 are nonetheless close to the boundary line. Image #1, in particular, is the rotated image: although visually, the image looks well personA's face, the rotation makes it harder for our classifier. Besides, it is close from the boundary, indicating the classifier *is not so confident* about its choice.\n",
        "* Images #2 and #7, two of the other three misclassified images -- that don't show the nominal personA haircut -- are further away from the boundary line. This corresponds to the visual hints that those test images actually do not look alike the training image, because of the haircut.\n",
        "* Image #9, the last test image of personA is not properly classified, but barely! It indeed shows the same characteristics as misclassified #2 and #7 (blond hair, different haircut, looks younger, ...) \"Thanks to\" the background and the viewpoint, however, the HOG descriptor differences do not lead to such a misclassification as for the two other similar images #2 and #7. The descriptor is reminded here below.\n",
        "* On the contrary, personB is classified, with a high confidence, properly for all test images. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aH_1eP9c3ZVT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "show_one_image_hog(index, person=personA, set=\"test\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JBqQBFgzox5Z",
        "colab_type": "text"
      },
      "source": [
        "#####personB classification\n",
        "\n",
        "Bradley Cooper, personB, was properly classified 100% of the time already, indicating a nice resemblance between personB test set and training set HOG descriptors.\n",
        "As visible on the distance to boundary decision plot above, the classifier is really condifent about its choice, specifically in comparison with results from personA tests images.  \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hdNvFEFybL8p",
        "colab_type": "text"
      },
      "source": [
        "###One More Thing... Pipelining!\n",
        "\n",
        "Until now, the training and test have been quite *manual*\n",
        "- we define data structure between the transformers\n",
        "- we specify the transformation manually one after the others\n",
        "\n",
        "The use of the `Transformer`'s that we have intriduced already gives the opportunity to do (much) better and take advantage of a dedicated *Architecture Style* of software programs called [Pipes and Filters](https://medium.com/@syedhasan010/pipe-and-filter-architecture-bd7babdb908-). \n",
        "\n",
        "The very good thing is that it's already provided by `sklearn`, and fully applicable to `Transformer`'s, which are in fact just `Filter`'s.\n",
        "\n",
        "This makes the classification much less of a manual process:\n",
        "- no more care about the follow-up of action for each run, \n",
        "- no more intermediate data structure creation, \n",
        "- **very** easy to modify and add/remove steps, withou messing with the data structures.\n",
        "\n",
        "Concretely, we need to define a `Pipeline` object that we will call to `fit` and `predict`. This `Pipeline`will make use of our `Transformer` objects and automatically connects the output of one to the input of the next one.\n",
        "\n",
        "\n",
        "We are now ready to reproduce the results using the `Pipeline` architecture!\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5HF3gHSlMmCb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "Definition of a pipeline\n",
        "'''\n",
        "\n",
        "HOG_pipeline = Pipeline([('hogify', HogTransformer(\n",
        "                             orientations = 9,\n",
        "                             pixels_per_cell = (8,8),\n",
        "                             cells_per_block = (2,2),\n",
        "                             block_norm='L2',\n",
        "                             transform_sqrt=True, \n",
        "                             multichannel = color)\n",
        "                         ),\n",
        "                         ('classify', SGDClassifier(\n",
        "                             random_state = 42, \n",
        "                             max_iter = 1000, \n",
        "                             tol=1e-3)\n",
        "                         )\n",
        "\n",
        "])\n",
        "\n",
        "'''\n",
        "Training\n",
        "'''\n",
        "# we set the X_train before the hogify of course...\n",
        "classifier = HOG_pipeline.fit(X_train, y_train)\n",
        "# logging.info(\"Percentage correct: \" + str( 100* np.sum(y_pred == y_test_ab)/len(y_test_ab) ) + \" %\")\n",
        "\n",
        "'''\n",
        "Predicting\n",
        "'''\n",
        "y_pred_= classifier.predict(X_test_ab) \n",
        "\n",
        "'''\n",
        "Computing and Showing accuracy\n",
        "'''\n",
        "logging.info(\"Percentage correct: \" + str( 100*np.sum(y_pred_ == y_test_ab)/len(y_test_ab)) + \" %\")\n",
        "misclassifier_index = np.where( np.array(y_test_ab != y_pred_))\n",
        "logging.info(\"Indices misclassified: \" + str(misclassifier_index[0]))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QYUcaqpEeowi",
        "colab_type": "text"
      },
      "source": [
        "At the end of the pipeline, we have reproduced exactly the same results as before.\n",
        "\n",
        "***From now on, I will use Pipelining instead of regular manual scripting***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7_y68B-ZpkwA",
        "colab_type": "text"
      },
      "source": [
        "###Tests on personC and personD\n",
        "\n",
        "We have a *not great* accuracy so far for personA and personB, and we can wonder how the current classifier, based on HOG descriptors which are local, performs when personC and personD come into play.\n",
        "\n",
        "- we create `X_test_cd` containing the test data for personC and personD\n",
        "- we create `y_test_cd` containing the 0/1 labels for personC and personD\n",
        "\n",
        "####Wait - What? Why?\n",
        "Hum... Indeed, assigning a label between 0 and 1 means we expect personC to be classified as personA, and personD as personB. This is disputable. Let's see this step as just a way to assess how the classification based on our feature works, and how the metric evolves when the inputs are \"so\" different, keeping in mind the results should not be as high as previously. \n",
        "\n",
        "Another way to see it is to understand persons A-C and B-D from the same classes, but only a biased training set is available. We want to assess how the final classifier perform on images never seen and quite different from training set, yet part of the classes (say, *white_young_female*, *white_40s_male*).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2qt-OpjGjEOM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_test_cd = X_test[20:40, :]\n",
        "y_test_cd = y_test[20:40]\n",
        "\n",
        "logging.info(\"X_test_cd shape: \" + str(X_test_cd.shape))\n",
        "logging.info(\"y_test_cd shape: \" + str(y_test_cd.shape))\n",
        "\n",
        "X_test_HOG_cd_back = X_test_cd.copy()\n",
        "y_test_HOG_cd_back = y_test_cd.copy()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QyIAXKhpj2E-",
        "colab_type": "text"
      },
      "source": [
        "Based on that, we can simply reuse the classifier already created"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eTeWtdDidv4b",
        "colab_type": "text"
      },
      "source": [
        "####Prediction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aJzFhyWXd04N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "Predicting\n",
        "'''\n",
        "# note that as we use the pipeline, we can directly set as input the data matrix. \n",
        "# the hogify step is included.\n",
        "y_pred_cd= classifier.predict(X_test_cd) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "73VsTvzCd2M6",
        "colab_type": "text"
      },
      "source": [
        "####Confusion Matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2_LyGzgnj-US",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "Computing and Showing accuracy\n",
        "'''\n",
        "logging.info(\"Percentage correct: \" + str( 100*np.sum(y_pred_cd == y_test_cd)/len(y_test_cd)) + \" %\")\n",
        "misclassifier_index = np.where( np.array(y_test_cd != y_pred_cd))\n",
        "properclassifier_index = np.where( np.array(y_test_cd == y_pred_cd))\n",
        "\n",
        "logging.info(\"Indices misclassified: \" + str(misclassifier_index[0]))\n",
        "\n",
        "true_labels=[\"Jane Levy\", \"Marc Blucas\"]\n",
        "predicted_labels = [\"Emma Stone\", \"Bradley Cooper\"]\n",
        "my_plot_confusion_matrix(classifier,\n",
        "                         X_test_cd, \n",
        "                         y_test_cd,\n",
        "                         true_labels = true_labels,\n",
        "                         predicted_labels = predicted_labels) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PC4dG4K4MiIQ",
        "colab_type": "text"
      },
      "source": [
        "Besides the *accuracy* of 55%, this gives a very interesting results:\n",
        "- All the images of personD were *correctly* classified\n",
        "- All the images of personC, except index 0, were *wrongly* classified\n",
        "\n",
        "> Again, I emphasize the fact that *correctly* and *wrongly* may not be appropriate considering previous remark.\n",
        "\n",
        "Let's have a closer look at the images."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ydYjAYsqlNHI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "logging.info(\"Mis-classified images: \")\n",
        "plot_matrix(X_test_cd[misclassifier_index[0],:], color, my_color_map, h=1, w=len(misclassifier_index[0]))\n",
        "\n",
        "logging.info(\"Properly classifier images: \")\n",
        "plot_matrix(X_test_cd[properclassifier_index[0],:], color, my_color_map, h=2, w=1+len(properclassifier_index[0])//2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W171ssXCZl9p",
        "colab_type": "text"
      },
      "source": [
        "#### Decisions boundary -- confidence score\n",
        "\n",
        "In order to visually understand current classification results, we can plot the decision boundary. \n",
        "As indicated in the documentation, \"the confidence score for a sample is the signed distance of that sample to the hyperplane\", see [sklearn](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html#sklearn.linear_model.SGDClassifier.decision_function)\n",
        "It gives an idea of how far the sample is from the hyperplane, hence an idea of the confidence the classifier has. \n",
        "This is great to observe what samples are easy / tricky to classify."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zN652CNtr04H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "Printing classifier scores (= distance to decision plane)\n",
        "'''\n",
        "scores = classifier.decision_function(X_test_cd)\n",
        "logging.info(\"scores (decision functions): \" + str(scores))\n",
        "\n",
        "'''\n",
        "Visualization\n",
        "'''\n",
        "# plt.figure(figsize=(8,8))\n",
        "fig, ax = plt.subplots(1, 1, figsize=(8,4))\n",
        "scores_a = scores[0:10]\n",
        "scores_b = scores[10:20]\n",
        "ax.scatter(range(len(scores_a)), scores_a)\n",
        "ax.scatter(range(len(scores_a), len(scores_a)+len(scores_b)), scores_b)\n",
        "ax.plot(range(len(scores)), np.zeros((len(scores),)),\"r-\")\n",
        "ax.set_title(\"Visualization of distance to classification boundary\")\n",
        "ax.legend([\"Boundary\", \"personA\", \"personB\"])\n",
        "ax.set_xlabel(\"test image index\",fontsize=12)\n",
        "ax.set_ylabel(\"Distance to boundary decision\",fontsize=12)\n",
        "if not color:\n",
        "    ax.set_xlim(left=-0.5, right=20.5)\n",
        "    ax.set_ylim(bottom=-275.0, top=275.0)\n",
        "    ax.text(2,150,\"Classified as \\nPersonB\",fontsize=12)\n",
        "    ax.text(12,-150,\"Classified as \\nPersonA\",fontsize=12)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7jxDuAOS3ORs",
        "colab_type": "text"
      },
      "source": [
        "Looking at the above plots of the distance to boundary decision line:\n",
        "- there is no clear difference in classification of personC and personD\n",
        "- personD was always properly classified as personB (class = \"1\"). We can however say that the average distance to boundary line (hence confidence of the classifier) is lower than for previous personB test images (see previous section). This is perfectly normal: the classifier is correct, but less confident, for personD (absent from training) than for personB.\n",
        "- personC is almost always misclassified, except for one image, that we plot after.\n",
        "    - personC is similar to personA **but** does not have this same haircut, characteristics to personA\n",
        "    - the rest of the descriptor cannot make it for the haircut, and it leads to a misclassification. In particular, the lateral sides are mostly vertical lines, more characteristics to personB than personA."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lJEdMsKceLDn",
        "colab_type": "text"
      },
      "source": [
        "####Better understanding\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wYSlYO8U5H2u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "show_one_image_hog(0,personC, \"test\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TQPWyVLAzJsI",
        "colab_type": "text"
      },
      "source": [
        "Above: this is the only image of personC classified as personA so far. We recognize, due to the view point and rotation of the picture, the oblique part on the upper right corner.  Other images do not show this, and are then classified as personB.\n",
        "\n",
        "Example given, Image#1 of personC, and Image#0 of personB are shown here after. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BgkUIrQ9zcjE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "show_one_image_hog(1,personC, \"test\")\n",
        "show_one_image_hog(1,personB, \"training\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o5p9VmNn2iGq",
        "colab_type": "text"
      },
      "source": [
        "### HOG Classifier conclusion\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DTdY3BsD3MLh",
        "colab_type": "text"
      },
      "source": [
        "In this section, we used our HOG feature to train a simple classifier (stochastic Gradient Descent), a linear model, that performed with an accuracy of 80% on the test set of personA and personB, and 55% on the test set of personC and personD. \n",
        "These numbers need to be taken with caution considering:\n",
        "- the small amount of images (both in training and test sets)\n",
        "- there was currently no optimization -- using a cross validation technique -- on the model hyperparameters. In particular:\n",
        "    * `orientations`\n",
        "    * `pixels_per_cell`\n",
        "    * `cells_per_block`\n",
        "    * `block_norm`\n",
        "    * `transform_sqrt`\n",
        "\n",
        "The results have been assessed, and in particular the (relative) poor performance on PersonA test images, and personC test images. This emphasize the inherent \"quality\" of the representation, its locality, and fine orientation *granularity* (changes in the orientation can be perceived quite well), spatial *granularity*,...all that depending of key parametres listed above. Surely, some optimizations are possible and will be treated in a later stage."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V37pPCfnubEQ",
        "colab_type": "text"
      },
      "source": [
        "## PCA Classification\n",
        "\n",
        "Applying the same routine as described for the HOG feature representation, we will build a classifier based on the PCA feature representation. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LUrEq8kIua7b",
        "colab_type": "text"
      },
      "source": [
        "###Gathering and processing the data\n",
        "\n",
        "As already discussed in the HOG section, we retrieve the original data, resize, convert them according to `color` attribute, and reshape them in a useable matrix. Those could also be done using a `Transformer`. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Becf33An8_iX",
        "colab_type": "text"
      },
      "source": [
        "####Training data\n",
        "\n",
        "Code Subtlety: because of the `get_matrix_from_set` function and current organization of the code, we need to reapply the shuffling as we re-use the import from the beginning according to the different parameters. \n",
        "\n",
        "In case of a production code, this would need to be improved.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zVjfgPEbFlph",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "get_matrix_from_set\n",
        "'''\n",
        "X_train_PCA = get_matrix_from_set(training_set, color, sq_size, flatten=True)\n",
        "y_train_PCA = np.zeros((40,))\n",
        "y_train_PCA[20:40] = 1\n",
        "\n",
        "\n",
        "'''\n",
        "Shuffling\n",
        "'''\n",
        "logging.info(\"Labels before shuffling:\\n\" + str(\", \".join([str(i.astype(np.uint8)) for i in y_train_PCA])))\n",
        "\n",
        "shuffle_training(X_train_PCA, y_train_PCA)\n",
        "\n",
        "logging.info(\"Labels  after shuffling:\\n\" + str(\", \".join([str(i.astype(np.uint8)) for i in y_train_PCA])))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tDAX5UCc9B3U",
        "colab_type": "text"
      },
      "source": [
        "####Test data\n",
        "\n",
        "As an anticipation, we already pre-process the data we will use for the test. we split the dataset to our convenience between:\n",
        "- test on personA and personB\n",
        "- test on personC and personD\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uNtXiyIW9Rrf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "get_matrix_from_set\n",
        "'''\n",
        "X_test_PCA= get_matrix_from_set(test_set, color, sq_size, flatten = True)\n",
        "y_test_PCA = y_test.copy()\n",
        "\n",
        "logging.info(\"X_test_PCA Shape          = \" + str(X_test_PCA.shape))\n",
        "logging.info(\"y_test_PCA Shape          = \" + str(y_test_PCA.shape))\n",
        "\n",
        "'''\n",
        "AB\n",
        "'''\n",
        "X_test_PCA_ab = X_test_PCA[0:20, :].copy()\n",
        "y_test_PCA_ab = y_test_PCA[0:20].copy()\n",
        "logging.info(\"X_test_PCA_ab Shape       = \" + str(X_test_PCA_ab.shape))\n",
        "logging.info(\"y_test_PCA_ab Shape       = \" + str(y_test_PCA_ab.shape))\n",
        "logging.info(\"Sum y_test_PCA_ab [10]    = \" + str(sum(y_test_PCA_ab)))\n",
        "\n",
        "'''\n",
        "CD\n",
        "'''\n",
        "X_test_PCA_cd = X_test_PCA[20:40, :].copy()\n",
        "y_test_PCA_cd = y_test_PCA[20:40].copy()\n",
        "logging.info(\"X_test_PCA_cd Shape       = \" + str(X_test_PCA_cd.shape))\n",
        "logging.info(\"y_test_PCA_cd Shape       = \" + str(y_test_PCA_cd.shape))\n",
        "logging.info(\"Sum y_test_PCA_cd [10]    = \" + str(sum(y_test_PCA_cd)))\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zDXH-9rgrS5S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "Define backup variables\n",
        "'''\n",
        "X_train_PCA_shuffle_back = X_train_PCA.copy()\n",
        "y_train_PCA_shuffle_back = y_train_PCA.copy()\n",
        "X_test_PCA_back = X_test_PCA.copy()\n",
        "y_test_PCA_back = y_test_PCA.copy()\n",
        "\n",
        "X_test_PCA_ab_back = X_test_PCA_ab.copy()\n",
        "y_test_PCA_ab_back = y_test_PCA_ab.copy()\n",
        "\n",
        "X_test_PCA_cd_back = X_test_PCA_cd.copy()\n",
        "y_test_PCA_cd_back = y_test_PCA_cd.copy()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6amoqn_75s1I",
        "colab_type": "text"
      },
      "source": [
        "###PCA pipeline\n",
        "\n",
        "Using Scikit-Learn library, that we already introduced in the previous part, we create the PCA Transformer.\n",
        "\n",
        "As input, we give first the `n_components` equal to the optimal $p$ that we found in previous section. \n",
        "This does not represent much of a dimensionality reduction, as already discussed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JQ1nx5PMItFo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "Below commented code create and train a classifier \\\"manually\\\"\"\n",
        "'''\n",
        "# p = 35\n",
        "# pcaify = sklearn_decomposition_PCA(n_components = p)\n",
        "# X_train_PCA = pcaify.fit_transform(X_train)\n",
        "# X_train_prepared = X_train_PCA\n",
        "\n",
        "# logging.info(\"PCA fit_transform result:\\nshape:\" + str(X_train_prepared.shape))\n",
        "\n",
        "# sgd_clf = SGDClassifier(random_state=42, max_iter = 1000, tol=1e-4, verbose = 1)\n",
        "# sgd_clf.fit(X_train_prepared, y_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EdP5c4kYApfZ",
        "colab_type": "text"
      },
      "source": [
        "The pipeline is created, and trained using input data directly."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aaBbluh6NDUA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "Definition of a PCA pipeline\n",
        "'''\n",
        "pcaify = sklearn_decomposition_PCA(n_components = 35)\n",
        "classify = SGDClassifier( random_state = 42, max_iter = 10000, tol=0.0001)\n",
        "\n",
        "PCA_pipeline = Pipeline([('pcaify', pcaify),\n",
        "                         ('classify', classify)])\n",
        "\n",
        "'''\n",
        "Training\n",
        "'''\n",
        "clf_pca = PCA_pipeline.fit(X_train_PCA, y_train_PCA)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KqzhPbyL73yc",
        "colab_type": "text"
      },
      "source": [
        "###Test on personA and personB\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jDmwrnssBtB6",
        "colab_type": "text"
      },
      "source": [
        "####Predictions\n",
        "\n",
        "Using the pipeline, we can predict the results for the tests images of personA and personB.\n",
        "\n",
        "We compute the accuracy\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cc1yJcEl8s5q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "Predicting A and B\n",
        "'''\n",
        "y_pred_PCA_ab = clf_pca.predict(X_test_PCA_ab) \n",
        "logging.info(\"Percentage correct: \" + str( 100*np.sum(y_pred_PCA_ab == y_test_PCA_ab)/len(y_test_PCA_ab)) + \" %\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IbHgyp_HwgSi",
        "colab_type": "text"
      },
      "source": [
        "For the sake of completion, we can - one extra time - get the eigenfaces used, according to the `n_components = 35` parameter."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K3eXyGTxrQ8s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "efaces = pcaify.components_\n",
        "logging.debug(\"eigenfaces shape = \" + str(efaces.shape))\n",
        "\n",
        "'''\n",
        "Visualize the eigenfaces, just as we did before\n",
        "'''\n",
        "if color:\n",
        "    efaces_cvt = (efaces*255).astype(np.uint).copy()\n",
        "else:\n",
        "    efaces_cvt = efaces.copy()\n",
        "plot_matrix(efaces_cvt, color, my_color_map, h=4, w=10, transpose=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "COeGY9DJ_IUc",
        "colab_type": "text"
      },
      "source": [
        "####Confusion Matrix\n",
        "As previously, it's interesting to get the confusion matrix view, even if it is quite simple in our problem, considering only two classes. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vMpSyiex_HdI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "true_labels = [\"Emma Stone\", \"Bradley Cooper\"]\n",
        "my_plot_confusion_matrix(clf_pca,\n",
        "                         X_test_PCA_ab, \n",
        "                         y_test_PCA_ab,\n",
        "                         true_labels = true_labels) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YpFZ9s6eCBVQ",
        "colab_type": "text"
      },
      "source": [
        "The confusion matrix directly shows that on test images for A and B, there is an accuracy of 100% and there is no misclassified images."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3l29SLNgB1w-",
        "colab_type": "text"
      },
      "source": [
        "####Observations and Discussions on test results\n",
        "\n",
        "\n",
        "The first thing to note is the absolute result 100% accuracy with the first attempt. This is pretty good. \n",
        "As a reminder, we had \"only\" 80% using HOG feature representation. However, we should be careful to draw any conclusion at this point as none of the classifier have been optimized yet in terms of hyperparameters (specifically not the HOG classifier - see later)\n",
        "\n",
        "Similarly to what we did for the HOG feature representation, let's analyze deeper the results of the classification on test images of personA and personB using the PCA feature representation, with hyperparameter $p=35$.\n",
        "This analysis is performed with `sq_scale = 64` and `color = False`.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8NYOmDizG_ZP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "Printing classifier scores (= distance to decision plane)\n",
        "'''\n",
        "scores = clf_pca.decision_function(X_test_PCA_ab)\n",
        "logging.info(\"scores (decision functions): \" + str(scores))\n",
        "\n",
        "'''\n",
        "Visualization\n",
        "'''\n",
        "# plt.figure(figsize=(8,8))\n",
        "fig, ax = plt.subplots(1, 1, figsize=(8,4))\n",
        "scores_a = scores[0:10]\n",
        "scores_b = scores[10:20]\n",
        "ax.scatter(range(len(scores_a)), scores_a)\n",
        "ax.scatter(range(len(scores_a), len(scores_a)+len(scores_b)), scores_b)\n",
        "ax.plot(range(len(scores)), np.zeros((len(scores),)),\"r-\")\n",
        "ax.set_title(\"Visualization of distance to classification boundary\")\n",
        "ax.legend([\"Boundary\", \"personA\", \"personB\"])\n",
        "ax.set_xlabel(\"test image index\",fontsize=12)\n",
        "ax.set_ylabel(\"Distance to boundary decision\",fontsize=12)\n",
        "if not color:\n",
        "    ax.text(2,0.5e8,\"Classified as \\nPersonB\",fontsize=12)\n",
        "    ax.text(12,-0.5e8,\"Classified as \\nPersonA\",fontsize=12)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GfcX6NLGA_Kg",
        "colab_type": "text"
      },
      "source": [
        "Visualizing the decision boundaries score, it seems the classifier is *pretty certain* about the personB classification, and slightly *less certain* for personA. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6gCjZDu2CMvO",
        "colab_type": "text"
      },
      "source": [
        "###Tests on personC and personD\n",
        "\n",
        "Similarly for what we did with the HOG feature representation, we can use our classifier on person C and person D test data. \n",
        "\n",
        "The remarks we made before regarding the intrinsic meaning of this test stay applicable.\n",
        "\n",
        "We use `X_test_PCA_cd` and `y_pred_PCA_cd` as data structures.\n",
        "\n",
        "We first use the classifer to predict, then print the indices and plot the misclassified images, the confusion matrix and the score, as we did before already."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K5qrHXDVFtVp",
        "colab_type": "text"
      },
      "source": [
        "####Predictions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vc46mgTuCTaD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "Predictions on C an D\n",
        "'''\n",
        "y_pred_PCA_cd = clf_pca.predict(X_test_PCA_cd) \n",
        "logging.info(\"Percentage correct: \" + str( 100*np.sum(y_pred_PCA_cd == y_test_PCA_cd)/len(y_test_PCA_cd)) + \" %\")\n",
        "\n",
        "\n",
        "'''\n",
        "get misclassified and correctly classified images index\n",
        "show related images\n",
        "'''\n",
        "show_missed(X_test_PCA_cd, y_test_PCA_cd, y_pred_PCA_cd)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NJQkhyIaFvg2",
        "colab_type": "text"
      },
      "source": [
        "####Confusion Matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LWCkzMrbCk_F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "true_labels=[\"Jane Levy\", \"Marc Blucas\"]\n",
        "predicted_labels = [\"Emma Stone\", \"Bradley Cooper\"]\n",
        "my_plot_confusion_matrix(clf_pca,\n",
        "                         X_test_PCA_cd, \n",
        "                         y_test_PCA_cd,\n",
        "                         true_labels = true_labels,\n",
        "                         predicted_labels = predicted_labels) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k6ciqWY3Gprb",
        "colab_type": "text"
      },
      "source": [
        "####Decision Boundaries\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4sUVi412G-y_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "scores = clf_pca.decision_function(X_test_PCA_cd)\n",
        "logging.info(\"scores (decision functions): \" + str(scores))\n",
        "\n",
        "'''\n",
        "Visualization\n",
        "'''\n",
        "# plt.figure(figsize=(8,8))\n",
        "fig, ax = plt.subplots(1, 1, figsize=(8,4))\n",
        "scores_a = scores[0:10]\n",
        "scores_b = scores[10:20]\n",
        "ax.scatter(range(len(scores_a)), scores_a)\n",
        "ax.scatter(range(len(scores_a), len(scores_a)+len(scores_b)), scores_b)\n",
        "ax.plot(range(len(scores)), np.zeros((len(scores),)),\"r-\")\n",
        "ax.set_title(\"Visualization of distance to classification boundary\")\n",
        "ax.legend([\"Boundary\", \"personA\", \"personB\"])\n",
        "ax.set_xlabel(\"test image index\",fontsize=12)\n",
        "ax.set_ylabel(\"Distance to boundary decision\",fontsize=12)\n",
        "# if not color:\n",
        "# ax.text(4,4e7,\"Classified as \\nPersonB\",fontsize=12)\n",
        "# ax.text(12,-2.5e7,\"Classified as \\nPersonA\",fontsize=12)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VeNZW7t8FzrG",
        "colab_type": "text"
      },
      "source": [
        "####Observations and Discussions on the test results\n",
        "\n",
        "The results of the prediction on personC and D are interesting as they differ a lot from the ones of the HOG feature. \n",
        "\n",
        "In the case of the PCA feature, both person C and person D have several correct and wrong predictions. Results are slighlty better (by one image) for person D, but considering the few images on the test sets, this is most likely not statistically representative. \n",
        "\n",
        "Some hints to better understand the results: as a reminder, PCA feature representation is a projection of the images into a vector of coefficients - weights - of the eigenfaces (Principal Components) found during the training phase. \n",
        "\n",
        "If an image is not classified properly, it means its feature representation *differs too much* from the class feature representation. Else, the classifier most likely would have found the correct class. Furthermore, the principal components are the directions of maximal variance of the training images. It follows that a wrongly classified image is not explained best by a linear combination of the $p$ principal components (direction of max variance of the training phase).\n",
        "\n",
        "Looking above at the misclassified images, one can wonder how it comes that an image, looking just like another - is misclassified. There are several possible explanations:\n",
        "- too much influence from the background\n",
        "- a too different lighting conditions\n",
        "- different scale w.r.t training images\n",
        "- different orientation (pose and view points) w.r.t. training images\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_VTxuwaKFNT1",
        "colab_type": "text"
      },
      "source": [
        "####Better understanding\n",
        "\n",
        "> Note: the results of this section run in `color=True` mode may be slightly different\n",
        "\n",
        "In order to better understand how the classifier works, let's choose a test image and try to improve the results. To do so, we will modify the image input itself, to see what can lead to a good classification with the system we have.\n",
        "\n",
        "> this may not be a good practice, as usually, one would rather work on the training and validation sets, and **not** modify the test set. However, the goal of this sub section is limited to give a bit more insight about PCA and classification based on PCA information, so that modifying a chosen image lead to a change in classification results. The goal is not to improve the classifier, but rather understand the modifications in input images that makes it delivering the results.\n",
        "\n",
        "Let's choose image of person C, index #6.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YSiUbp_lGMFP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "logging.info(\"Image we work on: image personC, index 6\")\n",
        "img = X_test_PCA_cd[6,:].copy()\n",
        "fig = plt.figure(figsize=(4,4 ))\n",
        "ax= fig.subplots(1,1)\n",
        "ax.imshow(my_reshape(img, sq_size, color), cmap = my_color_map, interpolation='nearest') \n",
        "ax.set_axis_off()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kAkEqujSMVZp",
        "colab_type": "text"
      },
      "source": [
        "This image is misclassified as class \"1\" instead of \"0\".\n",
        "\n",
        "- Mean value of the image, after *training_mean* substraction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B8UBppzkMmWT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "img_centered = img - pcaify.mean_\n",
        "\n",
        "fig = plt.figure(figsize=(4,4))\n",
        "ax= fig.subplots(1,1)\n",
        "ax.imshow(my_reshape(img_centered, sq_size, color), cmap = my_color_map, interpolation='nearest') \n",
        "ax.set_axis_off()\n",
        "plt.show()\n",
        "\n",
        "logging.info(\"Remaining mean value test ab  : \" + str(np.round(np.mean(X_test_PCA_ab-pcaify.mean_),2)))\n",
        "logging.info(\"Remaining mean value test cd  : \" + str(np.round(np.mean(X_test_PCA_cd-pcaify.mean_),2)))\n",
        "logging.info(\"Remaining mean value expexted : \" + str(np.round(np.mean(X_train_PCA - pcaify.mean_),2)))\n",
        "logging.info(\"Remaining mean value image#6  : \" + str(np.round(np.mean(img_centered),2)))\n",
        "logging.info(\"Nominal   mean value image#6  : \" + str(np.round(np.mean(img),2)))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DPTnGLAJNd0U",
        "colab_type": "text"
      },
      "source": [
        "We see that the test images of personC and personD have a resulting mean lower than the training set (=0) and also lower than personA and personB test images. \n",
        "\n",
        "> Intuitively, the average is a bit \"darker\".\n",
        "\n",
        "We also see that the image is **rotated** wrt mean image. This is visible to whitish areas around the eyes and mouth, and slightly darker around the supposed chin. An intuitive PCA-related conclusion is that the variance induced by the rotation is not well explained by personA images. \n",
        "\n",
        "What if we would modify this test image so that we try to rotate it \"back\" to a regular front face ? Doing so, we hope to decrease this unexplained variance.\n",
        "- we use the `scipy.ndimage` library\n",
        "- the angle to rotate is experimentally $-22 [deg]$\n",
        "- as rotated, some pixels are missing values to fill the shape. We set those pixels at the average pixel value of the image before modification. \n",
        "- Rotation is (experimentally) enough; no need of extra shift to fit better."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9DDDl4gUWe_q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "Rotation of the image\n",
        "'''\n",
        "img_rotated = ndimage.rotate(my_reshape(img, sq_size, color), -22, reshape=False, mode = \"constant\", cval=np.mean(img)) \n",
        "img_rotated_flatten = img_rotated.flatten()\n",
        "img_rotated_centered = img_rotated_flatten - pcaify.mean_\n",
        "\n",
        "'''\n",
        "Visualization\n",
        "'''\n",
        "fig = plt.figure(figsize=(12,4))\n",
        "ax1, ax2, ax3 = fig.subplots(1,3)\n",
        "ax1.imshow(my_reshape(img, sq_size, color), cmap = my_color_map, interpolation='nearest') \n",
        "ax2.imshow(img_rotated, cmap = my_color_map, interpolation='nearest') \n",
        "ax3.imshow(my_reshape(img_rotated_centered, sq_size, color), cmap = my_color_map, interpolation='nearest') \n",
        "\n",
        "ax1.set_title(\"Original #6\")\n",
        "ax2.set_title(\"Rotated #6\")\n",
        "ax3.set_title(\"Rotated #6 - training_mean \")\n",
        "\n",
        "ax1.set_axis_off()\n",
        "ax2.set_axis_off()\n",
        "ax3.set_axis_off()\n",
        "\n",
        "\n",
        "\n",
        "logging.info(\"New mean value image#6 modified : \" + str(np.round(np.mean(img_rotated_centered),2)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ulkvwpNLcp2K",
        "colab_type": "text"
      },
      "source": [
        "Now, we can replace the image #6 that was misclassified by the newly rotated image, for which missing values added are the mean of the original image. Successively, we show the test set for person C and D originally, and the modified one (image #6 changed by its rotated version)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2r7N2SsX4fks",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "Replacement of the original image #6 by its rotated version\n",
        "'''\n",
        "X_test_PCA_cd_new = X_test_PCA_cd.copy()\n",
        "X_test_PCA_cd_new[6,:] =  img_rotated_flatten \n",
        "\n",
        "logging.info(\"Usual and nominal test set for personC and personD\")\n",
        "plot_matrix(X_test_PCA_cd, color, my_color_map, h=2, w=10)\n",
        "\n",
        "logging.info(\"Modified test set for personC and personD, image#6 replaced\")\n",
        "plot_matrix(X_test_PCA_cd_new, color, my_color_map, h=2, w=10)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NhBWNI5ge75y",
        "colab_type": "text"
      },
      "source": [
        "Let's try to predict again the class for this test \"new\" test set, which is only different by image#6."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R6IkGnje-OaC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "Prediction on new test set\n",
        "'''\n",
        "y_pred_PCA_cd_new = clf_pca.predict(X_test_PCA_cd_new) \n",
        "logging.info(\"Percentage correct: \" + str( 100*np.sum(y_pred_PCA_cd_new == y_test_PCA_cd)/len(y_test_PCA_cd)) + \" %\")\n",
        "\n",
        "misclassifier_index = np.where( np.array(y_test_PCA_cd != y_pred_PCA_cd_new))\n",
        "properclassifier_index = np.where( np.array(y_test_PCA_cd == y_pred_PCA_cd_new))\n",
        "\n",
        "logging.info(\"Indices misclassified: \" + str(misclassifier_index[0]))\n",
        "scores = clf_pca.decision_function(X_test_PCA_cd_new)\n",
        "\n",
        "\n",
        "# Visualization\n",
        "true_labels=[\"Jane Levy\", \"Marc Blucas\"]\n",
        "predicted_labels = [\"Emma Stone\", \"Bradley Cooper\"]\n",
        "my_plot_confusion_matrix(clf_pca,\n",
        "                         X_test_PCA_cd_new, \n",
        "                         y_test_PCA_cd,\n",
        "                         true_labels = true_labels,\n",
        "                         predicted_labels = predicted_labels) \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zd7VCNKlfgky",
        "colab_type": "text"
      },
      "source": [
        "Now, the image#6_rotated is properly classified, as hoped, thanks to the rotation. Concretely, the rotation had mostly the following impacts:\n",
        "- decrease of the influence of a dark area because of the hair\n",
        "    * influence of background\n",
        "    * lighting conditions\n",
        "- better match with eigenfaces\n",
        "    * variance better explained by personA-related eigenfaces\n",
        "\n",
        "This is not a rigorous method to determine the exact behavior of the classifier. Rather, it's an intuitive reasoning showing how an image can be (mis-)classified by PCA repesentation based classifier. It also shows that rotation of an image can also matter in case of the PCA feature representation, as in the HOG representation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x-BnisUNC6Zw",
        "colab_type": "text"
      },
      "source": [
        "###HOG vs PCA classification \n",
        "\n",
        "Based on the same training samples, and the same classifier technique used, PCA does a better job at classifying the test set images. \n",
        "\n",
        "The test set contains some images that are just alike the training set, but also some with different view points, and visual differences in terms of person haircut, color, ... \n",
        "\n",
        "PCA-based classification seems less dramatically confused by those aspects, and seems to have successfully captured the representation associated to personA and personB (the two classes). Yet, we have seen and analyzed that rotation may have a large impact on classification results.\n",
        "\n",
        "HOG-based classification, on the other side, suffers more with respect to these training-test sets differences and, as we saw, is also more disturbed by the local haircut change between personA and personC. \n",
        "\n",
        "However, we should remain careful at this point:\n",
        "- the classifiers have not been optimized in any sort,\n",
        "- the training set is pretty reduced.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z69CgcHOUqcs",
        "colab_type": "text"
      },
      "source": [
        "## Identification\n",
        "In an identification setup the goal is to **compute similarity scores** between pairs of data examples and use them to identify new images. \n",
        "In this section, we will:\n",
        "1. describe the visualizations used along the section\n",
        "2. compute the feature representations HOG / PCA\n",
        "3. compute the distances pairwaise between the test set images and the training set images. \n",
        "4. discuss those distance results, macroscopically and at image level, \n",
        "5. Use k-NN to label an image based on its nearest neighbours. In particular, we will:\n",
        "    - discuss the choice of $k$ parameters, using different ways\n",
        "    - discuss the results of the labeling\n",
        "    - analyze the images having the closest and furthest nearest neighbor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E2BO3qZmI9KY",
        "colab_type": "text"
      },
      "source": [
        "First, we will need to import several pairwise metrics functions from `sklearn` library."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Gnqa7bRgqqP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.metrics.pairwise import cosine_distances\n",
        "from sklearn.metrics.pairwise import euclidean_distances\n",
        "from sklearn.metrics.pairwise import manhattan_distances"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "azrVkyB8PKr-",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "```\n",
        ">>> from numpy.linalg import norm\n",
        ">>> norm(X, axis=1, ord=1)  # L-1 norm\n",
        ">>> norm(X, axis=1, ord=2)  # L-2 norm\n",
        ">>> norm(X, axis=1, ord=np.inf)  # L- norm\n",
        "```\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9GIjDdVEJJf-",
        "colab_type": "text"
      },
      "source": [
        "We create also a util function that allows plotting the similarity matrix (or distance matrix), with several parameters. This will be extensively used, while the code itself isn't that important.\n",
        "> Note that by default, the colormap \"jet\" is used. This is a personnal preference and I'm used to work with it. Should that **not** suit you, you can of course change this colormap, either locally as the argument of the plot_similarity_matrix function, or globally as the default value of this parameter.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wMYJIBugmkO9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
        "\n",
        "def plot_similarity_matrix(similarity_matrix, show_numbers = False, vmax1 = None, vmax2 = None, norm_only=False, width=16, height = 8, fontsize = 10, return_normed = False, cmap=plt.cm.jet):\n",
        "    similarity_matrix_norm = 100*similarity_matrix / np.linalg.norm(similarity_matrix, axis = 1, ord=1, keepdims=True)\n",
        "    \n",
        "    # for i in range(similarity_matrix_norm.shape[0]):\n",
        "    #     print(sum(similarity_matrix_norm[0,:]))\n",
        "\n",
        "    if norm_only:\n",
        "        fig = plt.figure()\n",
        "        ax = fig.add_subplot(111)\n",
        "        fig.set_size_inches(width, height)\n",
        "\n",
        "        im = ax.imshow(similarity_matrix_norm, vmax=vmax2, cmap=cmap)\n",
        "        ax.set_title('%')\n",
        "        divider = make_axes_locatable(ax)\n",
        "        cax = divider.append_axes(\"right\", size=\"5%\", pad=0.1)\n",
        "        fig.colorbar(im, cax=cax)\n",
        "        if show_numbers:\n",
        "            for i in range(similarity_matrix.shape[0]):\n",
        "                for j in range(similarity_matrix.shape[1]):\n",
        "                    ax.text(j, i, str(round(similarity_matrix_norm[i,j],0)).split(\".\")[0],fontsize=fontsize,va='center', ha='center')\n",
        "        \n",
        "    else:\n",
        "        fig, ax = plt.subplots(ncols=2)\n",
        "        fig.set_size_inches(width, height)\n",
        "    \n",
        "        im1 = ax[0].imshow(similarity_matrix, vmax=vmax1, cmap=cmap)\n",
        "        ax[0].set_title('as is')\n",
        "        im2 = ax[1].imshow(similarity_matrix_norm, vmax=vmax2, cmap=cmap)\n",
        "        ax[1].set_title('%')\n",
        "        dividers = [make_axes_locatable(a) for a in ax]\n",
        "        cax1, cax2 = [divider.append_axes(\"right\", size=\"5%\", pad=0.1) for divider in dividers]\n",
        "    \n",
        "        fig.colorbar(im1, cax=cax1)\n",
        "        fig.colorbar(im2, cax=cax2)\n",
        "\n",
        "        if show_numbers:\n",
        "            for i in range(similarity_matrix.shape[0]):\n",
        "                for j in range(similarity_matrix.shape[1]):\n",
        "                    ax[0].text(j, i, str(similarity_matrix[i,j]), fontsize=fontsize,va='center', ha='center')\n",
        "                    ax[1].text(j, i, str(round(similarity_matrix_norm[i,j],0)).split(\".\")[0],fontsize=fontsize,va='center', ha='center')\n",
        "    fig.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    if return_normed:\n",
        "        return similarity_matrix_norm"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JAZ67dFriP-_",
        "colab_type": "text"
      },
      "source": [
        "#### Illustration of distance measures\n",
        "\n",
        "\n",
        "In order to better understand the Visualization used, a tool example is given hereunder. The input is a matrix handmade. Later, it will be the matrix of shape (#test_image, #train_image) containing the distances computed pairwise.\n",
        "\n",
        "Regarding the input matrix:\n",
        "- on each line, the ratio between number is kept the same\n",
        "- the five rows contain three different scale of the numbers:    \n",
        "\n",
        "```\n",
        "[[ 100,  20,  30,  40,  50], \n",
        " [   2,  10,   3,   4,   5], \n",
        " [  20,  30, 100,  40,  50],\n",
        " [   2,   3,   4,  10,   5], \n",
        " [ 200, 300, 400, 500,1000]]\n",
        "```\n",
        "\n",
        "\n",
        "The numbers written indicate the value of the cell.\n",
        "\n",
        "**Left side**: The \"As is\" matrix colors the cell as the numbers are set. The colorscale is therefore really large, and it can be useful to observe the disparity of measures across all tests. \n",
        "\n",
        "If the represented matrix is a distance matrix, for instance, one can observe that the 5th test image is *very far* from all the training images, as the 5th row as larger numbers than any other row. \n",
        "The drawback is that if some numbers are much higher than others, we lose in granularity to represent the differences between those numbers. For instance, a distance of \"2\" and a distance of \"20\" are very much alike.\n",
        "\n",
        "**Right side**: The figure shows the same input matrix but normalized by row. That means that the sum of all numbers in a row is equal to 100 %. \n",
        "It is helpful to analyze *locally* the distance/similarity values for 1 specific test image with respect to all training images. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HHDxUAmWI1_H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_dist_mtx = np.array([[100,20,30,40,50], [2,10,3,4,5], [20,30,100,40,50],[2,3,4,10,5], [200,300,400,500,1000]])\n",
        "plot_similarity_matrix(test_dist_mtx, show_numbers=True, width=8, height=4, fontsize=14)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r7YRyCUtu1fh",
        "colab_type": "text"
      },
      "source": [
        "In the next sections of this tutorial, these Visualizations will be used to detail the similarity/distance measures between our different images. \n",
        "The vertical axis correspond to test images, with the following mapping\n",
        "- 0 -> 9: images of PersonA, Emma Stone\n",
        "- 10 -> 19: images of PersonB, Bradley Cooper\n",
        "- 20 -> 29: images of PersonC, Jane Levy\n",
        "- 30 -> 39: images of PersonD, Marc Blucas\n",
        "\n",
        "The horizontal axis corresponds to the training images, with the following mapping:\n",
        "- 0 -> 19: images of PersonA (training set)\n",
        "- 20 -> 39: images of PersonB (training set)\n",
        "\n",
        "If the feature descritions are appropriate, the features distance measurements should lead to\n",
        "- very small distance (= large similarity measure) between training and test images of PersonA; and similarly for personB)\n",
        "- very high distance (= small similarity measure) between training and test images of PersonA; and similarly for personB)\n",
        "\n",
        "Put in another way, persons from the same class would share feature descriptions, and not share other class feature description. This is exaclty inline with how we define what is a *good* feature, at the beginning of this notebook.\n",
        "\n",
        "Intuitively, feature description of :\n",
        "- test personA $\\simeq$ training personA $\\&$ test personA $\\neq$ training personB; \n",
        "- test personB $\\neq$ training personA $\\&$ test personB $\\simeq$ training personB; \n",
        "- test personC $\\sim$ training personA $\\&$ test personC $\\neq$ training personB; \n",
        "- test personD $\\neq$ training personA $\\&$ test personD $\\sim$ training personB; \n",
        "\n",
        "Visually plot as a matrix, it could hence look like the following images."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YGfY-40ymTgu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_dist_mtx = np.array([[5,20], [20,5], [10,20], [20,10]])\n",
        "plot_similarity_matrix(test_dist_mtx, show_numbers=True, width=5, height=5, fontsize=14)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-J8YS1AiL7js",
        "colab_type": "text"
      },
      "source": [
        "To ease later computation, we also write a simple `get_distances` (naming is not great...) that returns the sum of the distances in the eight eights of the global pairwise distances matrix. If it is not clear, it'll become soon enough when using it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WWICLesjBYS2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_distances(matrix_distances):\n",
        "    '''\n",
        "    res:[ \n",
        "            [dist(A,A), dist(A,B)]\n",
        "            [dist(B,A), dist(B,B)]\n",
        "            [dist(C,A), dist(C,B)]\n",
        "            [dist(D,A), dist(D,B)]\n",
        "        ]\n",
        "\n",
        "    '''\n",
        "    res = np.empty((4,2))\n",
        "    res[0,0] = sum(sum(matrix_distances[0:10,0:20]))\n",
        "    res[0,1] = sum(sum(matrix_distances[0:10,20:40]))\n",
        "    res[1,0] = sum(sum(matrix_distances[10:20,0:20]))\n",
        "    res[1,1] = sum(sum(matrix_distances[10:20,20:40]))\n",
        "    res[2,0] = sum(sum(matrix_distances[20:30,0:20]))\n",
        "    res[2,1] = sum(sum(matrix_distances[20:30,20:40]))\n",
        "    res[3,0] = sum(sum(matrix_distances[30:40,0:20]))\n",
        "    res[3,1] = sum(sum(matrix_distances[30:40,20:40]))\n",
        "    return res\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CDsTsCnRWqqN",
        "colab_type": "text"
      },
      "source": [
        "### HOG feature decriptors\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u9icKavSYH8d",
        "colab_type": "text"
      },
      "source": [
        "#### Pre-process data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UuwwW-e1nowh",
        "colab_type": "text"
      },
      "source": [
        "Similarly to classification, the very first step is to pre-process the data. \n",
        "We use the `get_matrix_from_set` function that allows to work with different size and color. \n",
        "\n",
        "For the identification, it is not required to shuffle the training set, as the distance to all samples is computed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RmUs-jTEYBrt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train = get_matrix_from_set(training_set, color, sq_size = sq_size, flatten = False)\n",
        "X_test = get_matrix_from_set(test_set, color, sq_size = sq_size, flatten = False)\n",
        "\n",
        "y_train = np.zeros((40,))\n",
        "y_train[20:40] = 1\n",
        " \n",
        "'''\n",
        "For now, set up \"0\" for personC; \"1\" for personD !\n",
        "'''\n",
        "y_test = np.zeros((40,))\n",
        "y_test[10:20] = 1\n",
        "y_test[30:40] = 1\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6-83e04xoIbv",
        "colab_type": "text"
      },
      "source": [
        "#####Visualization\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RA82xnpT6Ui6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "logging.info(\"Training set (horizontal matrix axis):\")\n",
        "logging.info(\"Training set PersonA [0 -> 19]:\")\n",
        "plot_matrix(X_train[0:20,:], color, my_color_map, h=1, w=20, transpose = False)\n",
        "logging.info(\"Training set PersonB [20 -> 39]:\")\n",
        "plot_matrix(X_train[20:40,:], color, my_color_map, h=1, w=20, transpose = False)\n",
        "\n",
        "logging.info(\"Test set (vertical matrix axis):\")\n",
        "logging.info(\"Test set PersonA [0 -> 9]:\")\n",
        "plot_matrix(X_test[0:10, :], color, my_color_map, h=1, w=10, transpose = False)\n",
        "logging.info(\"Test set PersonB [10 -> 19]:\")\n",
        "plot_matrix(X_test[10:20, :], color, my_color_map, h=1, w=10, transpose = False)\n",
        "logging.info(\"Test set PersonC [20 -> 29]:\")\n",
        "plot_matrix(X_test[20:30, :], color, my_color_map, h=1, w=10, transpose = False)\n",
        "logging.info(\"Test set PersonD [30 -> 39]:\")\n",
        "plot_matrix(X_test[30:40, :], color, my_color_map, h=1, w=10, transpose = False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lwnOQwJYGKWZ",
        "colab_type": "text"
      },
      "source": [
        "#####Creation of Transformer\n",
        "\n",
        "As usual now, let's create the transformer that computes the HOG for the images.\n",
        "The parameters may sound new to you: don't panic. Those parameters are actually found to be an optimization - discussed later -, and the effect of the parameters on the identification part is also discussed a bit later. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XNh9vtMhWyiq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "hogify = None\n",
        "hogify = HogTransformer(    orientations = 9, \n",
        "                            pixels_per_cell = (16,16), \n",
        "                            cells_per_block = (2,2),\n",
        "                            block_norm = \"L2-Hys\", \n",
        "                            transform_sqrt = False,\n",
        "                            multichannel = color)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jzGz5DZeGSMY",
        "colab_type": "text"
      },
      "source": [
        "#####Transformation of Inputs\n",
        " - **X_train**: application of the fit then transform method from the transformer\n",
        " - **X_test**: application of the transform method from the transformer\n",
        "\n",
        "As discussed already, it doesn't really change a thing for the HOG computation as the fit doesn't do much (recall, it is a homemade Transformer we created in the previous task)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c-l2ipZWGRVM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train_hog = hogify.fit_transform(X_train)\n",
        "X_test_hog = hogify.transform(X_test)\n",
        "\n",
        "logging.debug(\"X_train_hog Shape: \" + str(X_train_hog.shape))\n",
        "logging.debug(\"X_test_hog Shape: \" + str(X_test_hog.shape))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QfiKUASmjrIg",
        "colab_type": "text"
      },
      "source": [
        "#### Compute pairwise distances\n",
        "\n",
        "After the computation of the feature representations for both data sets (training and test), we can compute the distances pairwise -- between each pair. \n",
        "\n",
        "Three common distance formula are used:\n",
        "1. Euclidean distance\n",
        "2. Manhattan distance\n",
        "3. Cosine distance\n",
        "\n",
        "While the Euclidean distance is intuitive up to 3 dimensions, it is known to behave not as good in high dimensions where \"everything is far away\". Comparing the results of the three methods will give a better insight if there is an issue with the distance measurements or not.\n",
        "\n",
        "\n",
        "Let's recall the formalism we use:\n",
        "- horizontal axis: training images [20 personA; 20 personB]\n",
        "- vertical axis: test images [10 personA; 10 personB, 10 personC, 10 personD]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2NdCbPXpkBIe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "hog_distances_eucl = euclidean_distances(X_test_hog, X_train_hog)\n",
        "hog_distances_man = manhattan_distances(X_test_hog, X_train_hog)\n",
        "hog_distances_cos = cosine_distances(X_test_hog, X_train_hog)\n",
        "\n",
        "# logging.debug(\"DISTANCE BASED ON COSINE - LOG10\")\n",
        "# plot_similarity_matrix(np.log10(hog_distances_cos))\n",
        "\n",
        "logging.info(\"DISTANCE BASED ON EUCL\")\n",
        "plot_similarity_matrix((hog_distances_eucl))\n",
        "\n",
        "logging.info(\"DISTANCE BASED ON MANHATTAN\")\n",
        "plot_similarity_matrix((hog_distances_man))\n",
        "\n",
        "logging.info(\"DISTANCE BASED ON COSINE\")\n",
        "plot_similarity_matrix((hog_distances_cos))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U4mfRGltviLH",
        "colab_type": "text"
      },
      "source": [
        "#### Analysis on the distance computed\n",
        "\n",
        "From a macroscopic view, the first thing to note is that all the distances give *similar* results. As macroscopic, I understand the matrix as devided in eight blocks as presented above. This is particularly true when comparing normed distances, on the right side. \n",
        "> Remember that on the right side, the sum of all values of a line is equal to 100. It helps figuring relatively what is the closest/furthest training image from a specific test image, which we actually are interested in.\n",
        "\n",
        "Of course there are differences between the colors represented but they don't change drastically, and euclidean seems to give enough granularity to continue using it.\n",
        "\n",
        "As it has been a lot of colors/matrix represented, let's just plot below only the euclidean distance, normed per test sample (so, the euclidean distance as a percentage of the sum of the distances, per test sample).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ycr78CrWx0oR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "logging.info(\"Expected Look-alike matrix coloration\")\n",
        "plot_similarity_matrix(test_dist_mtx, show_numbers=True, norm_only=True, width=3, height=3, fontsize=14)\n",
        "\n",
        "\n",
        "logging.info(\"Global results of pairwise distances\")\n",
        "hog_eucl_normed = plot_similarity_matrix(hog_distances_eucl, show_numbers = False, norm_only=True, width=10, height = 10, fontsize = 10, return_normed=True)\n",
        "\n",
        "res=get_distances(hog_distances_eucl)\n",
        "logging.info(\"Macroscopic view\")\n",
        "plot_similarity_matrix(res, show_numbers=True, norm_only=True, width=4, height=4)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jKp48E4zy1BX",
        "colab_type": "text"
      },
      "source": [
        "> The second plot is a really macro view of the large matrix. It is read as *\\\"56% of the sum of the distances of all personB test images with respect to all training images regards personA training images, while only 44% regards personB training images. It seems reasonable to assume that personB test images are closer to personB training images than personA training images\\\"*.\n",
        "\n",
        "Continuing the analysis of the pairwise distance results focusing on this normed matrix:\n",
        "- from the macroscopic view, it complies with expectation for all eights (test personX - training personY) except of Jane Levy, personC\n",
        "- Test images of personC is overall closer to personB than personA, which is not what we could expect considering personC is a young white female, with similar hair color than personA.\n",
        "- PersonB test images are clearly close to training personB images; personA test images are globally further from their corresponding training image.\n",
        "\n",
        "However, this is fully inline with the classification results we obtained in previous section for the HOG feature, and the digging we made into the HOG representation. Because of the haircut, a large part of the histogram of personC actually is much more similar to the one of Bradley Cooper (personB). \n",
        "\n",
        "- At the image level, it is quite obvious that some training images seems (very) far from all others. Visually, it is the case for training set images 0, 18 and 19. This is confirmed when looking at the sum of the (positive) distances over all test examples -- see next plot. \n",
        "It seems those images are not very useful for identification purpose, at least. A careful eye confirms it is three images belonging to personA dataset. A conclusion is then: *Based on the pairwise distances between all HOG representations, three images of the personA dataset seems to be little help in identification tasks.*  Note that the threshold on the below graph is chosen purely arbitrarily."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9kECN0eh01P0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if not color:\n",
        "    threshold = 65\n",
        "else:\n",
        "    threshold = 65\n",
        "eucl_dist_vert_sum = np.sum(np.abs(hog_distances_eucl), axis = 0)\n",
        "\n",
        "# Visualization\n",
        "fig=plt.figure(figsize=(8,4))\n",
        "ax = fig.add_subplot(111)\n",
        "ax.scatter([i for i in range(len(eucl_dist_vert_sum))],eucl_dist_vert_sum)\n",
        "\n",
        "ax.plot([-1,41],[threshold,threshold], '-r')\n",
        "ax.set_title(\"Sum of the distances to test images, per training images\")\n",
        "plt.show()\n",
        "\n",
        "indices = np.where(eucl_dist_vert_sum > threshold)[0]\n",
        "logging.info(\"Indices of training images > Threshold: \" + str(indices))\n",
        "\n",
        "\n",
        "plot_matrix(X_train[indices,:], color, my_color_map, h=1, w=len(indices))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N2OpWunT57g3",
        "colab_type": "text"
      },
      "source": [
        "- The results obtained for Jane Levy are *completely* inline with the results obtained in previous classification task! The reasons behind this *bad* results, as discussed already - and in details in the previous section, mainly rely on the haircut which makes personC's HOG similar to personB's. \n",
        "\n",
        "- Nothing much special about Marc Blucas, personD, despite that the results are as expected: less similar than mean to personA, more similar to personB."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c6pJ2iWjCIJW",
        "colab_type": "text"
      },
      "source": [
        "####Hog Transformer parameters\n",
        "\n",
        "As for the classification, the results on the distance are of course dependant on the HOG transformer parameters, and specifically the `pixels_per_cell` or the `cells_per_block` which somehow define the spatial granularity of the representation. \n",
        "We expect that, with a smaller number of pixels per cells as defined, while the real value will change, the overall behavior should remain (macroscopic view). \n",
        "\n",
        "We can try that out, with `pixels_per_cell = (4,4)` for instance, hence *16 times finer*\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x8rAbIfsC8JT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "Hog Transformer\n",
        "'''\n",
        "hogify = HogTransformer(    orientations = 9, \n",
        "                            pixels_per_cell = (4,4), \n",
        "                            cells_per_block = (2,2),\n",
        "                            block_norm = \"L2-Hys\", \n",
        "                            transform_sqrt = False,\n",
        "                            multichannel = color)\n",
        "\n",
        "'''\n",
        "Preparation of the data\n",
        "'''\n",
        "X_train_hog = hogify.fit_transform(X_train)\n",
        "X_test_hog = hogify.transform(X_test)\n",
        "\n",
        "'''\n",
        "Pairwise distance computing\n",
        "'''\n",
        "hog_distances_eucl = euclidean_distances(X_test_hog, X_train_hog)\n",
        "\n",
        "'''\n",
        "Show the Matrix\n",
        "'''\n",
        "logging.info(\"DISTANCE BASED ON EUCL\")\n",
        "plot_similarity_matrix((hog_distances_eucl))\n",
        "\n",
        "'''\n",
        "Setup the new threshold value (trial-error)\n",
        "'''\n",
        "if not color:\n",
        "    threshold = 570\n",
        "else:\n",
        "    threshold = 570\n",
        "'''\n",
        "Compute sum in vertical axis\n",
        "'''\n",
        "eucl_dist_vert_sum = np.sum(np.abs(hog_distances_eucl), axis = 0)\n",
        "\n",
        "'''\n",
        "extract indices\n",
        "'''\n",
        "indices = np.where(eucl_dist_vert_sum > threshold)[0]\n",
        "\n",
        "'''\n",
        "Plot results\n",
        "'''\n",
        "# print(eucl_dist_vert_sum)\n",
        "fig=plt.figure(figsize=(8,4))\n",
        "ax = fig.add_subplot(111)\n",
        "ax.scatter([i for i in range(len(eucl_dist_vert_sum))],eucl_dist_vert_sum)\n",
        "ax.plot([-1,41],[threshold,threshold], '-r')\n",
        "ax.set_title(\"Sum of the distances to test images, per training images\")\n",
        "plt.show()\n",
        "logging.info(\"Indices of training images > Threshold: \" + str(indices))\n",
        "\n",
        "'''\n",
        "Show images\n",
        "'''\n",
        "plot_matrix(X_train[indices,:], color, my_color_map, h=1, w=len(indices))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n4n1SfjtDw5O",
        "colab_type": "text"
      },
      "source": [
        "With such a small number of pixels per cells (details of what happens in the HOG can be found in part 1 of this tutorial), the overal results remain: as expected for all eights but for PersonC's!\n",
        "\n",
        "* The most distant training images are not all the same as before:\n",
        "    - 2 Emma Stone images remain the furthest from all test images, \n",
        "    - 1 Bradly Cooper image becomes the third furthest. \n",
        "\n",
        "This parameter doesn't actually change a lot: overall, the distribution of the distances doesn't change much. However, it, once more, indicates the locality of the HOG representation, and the impact of the feature representation parameters on the subsequent tasks. \n",
        "\n",
        "* Another interesting consequence of this finer HOG is that personC and personD seems \"more distant\", globally, to training samples. This is indicated by the reddish lower part of the left-side matrix, above. The results complies with our intuition that personA and personB test images should be \"closer\" to training set, as they represent visually the same person.\n",
        "\n",
        "* Finally, it becomes much clearer, with this finer HOG, that the some test images of personA appears further away from the training examples. For instance, images #1 and #9, Emma Stone (vertical axis) don't seem to have any blue parts. This is interesting to put in perspective with classification results obtained with the HOG representation before, where images 1 and 9 were among the misclassified ones, at least at the first attempt (prior to any optimization).\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MV_iOmO5re1s",
        "colab_type": "text"
      },
      "source": [
        "####Identification of test images using k-NN\n",
        "\n",
        "In this step, using k-nearest neighbor, the goal is to label the test images according to their nearest neighbours. \n",
        "In a very intuitive way, we could say that the images on the vertical axis of the matrix above should get as label the ones from the bluest set of neighbors (either personA (=0) or personB (=1))\n",
        "\n",
        "As from previous section, we are familiar with the pipeline, we will implement a pipeline using the knn classifier in the HOG dataspace. \n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yLV-PjtDYWnI",
        "colab_type": "text"
      },
      "source": [
        "#####Choosing k\n",
        "**What k-value to take** ?\n",
        "This is often a critical question.\n",
        "We could start with k=1, assuming that given the small number of samples in the training set, the closest should be the most appropriate label. \n",
        "\n",
        "**Can we do better ?**\n",
        "Definitely, one of the best way to go is to assess different values for k using a validation set, performing cross-validation. \n",
        "Although it is a spoiler to the \"Impress you TA's \" section, next, we will use a [GridSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html#sklearn.model_selection.GridSearchCV) technique, limited to the k parameter, trying out different values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rtp95kE0reS2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "k=1\n",
        "\n",
        "\n",
        "'''\n",
        "Definition of a pipeline\n",
        "'''\n",
        "HOG_knn_pipeline = Pipeline([\n",
        "                         ('hogify', HogTransformer(\n",
        "                             orientations = 9,\n",
        "                             pixels_per_cell = (16,16),\n",
        "                             cells_per_block = (2,2),\n",
        "                             block_norm='L2-Hys',\n",
        "                             transform_sqrt=True, \n",
        "                             multichannel = color)\n",
        "                         ),\n",
        "                         ('classify', \n",
        "                          KNeighborsClassifier( n_neighbors = k, metric='euclidean')\n",
        "                         )])\n",
        "\n",
        "\n",
        "param_grid_knn = [\n",
        "    {\n",
        "        'hogify__pixels_per_cell': [(4,4),(16,16)],\n",
        "        'classify__n_neighbors':[1,2,3]}\n",
        "]\n",
        "\n",
        "grid_search = GridSearchCV(HOG_knn_pipeline,\n",
        "                           param_grid_knn, \n",
        "                           cv = 3, \n",
        "                           n_jobs = -1,\n",
        "                           scoring = \"accuracy\",\n",
        "                           verbose = 1, \n",
        "                           return_train_score = True)\n",
        "\n",
        "grid_res = grid_search.fit(X_train, y_train)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eCFWdMr3ykUD",
        "colab_type": "text"
      },
      "source": [
        "`GridSearchCV` helps in performing a systematic choice within different (hyper-)parameter values in automatically running the pipeline with those different values, and output the best result according to a (specified) metric. Here, I choose the accuracy to indicate the best system. \n",
        "\n",
        "The accuracy is computed using cross validation, and not! using the test set.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lmOmF_aFYgXG",
        "colab_type": "text"
      },
      "source": [
        "#####Prediction and metrics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mmKGSvs5I9UC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "logging.info(\"Best parameters: \" + str(grid_res.best_params_))\n",
        "logging.info(\"Best scores: \" + str(100* grid_res.best_score_ ) + \"% \")\n",
        "\n",
        "best_prediction_ab = grid_res.predict(X_test_ab)\n",
        "logging.info('Percentage correct persons A, B: ' + str(100*np.sum(best_prediction_ab == y_test_ab)/len(y_test_ab)))\n",
        "best_prediction_cd = grid_res.predict(X_test_cd)\n",
        "logging.info('Percentage correct persons C, D: ' + str( 100*np.sum(best_prediction_cd == y_test_cd)/len(y_test_cd)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8L2DwPCIKo43",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "show_missed(X_test_ab, y_test_ab, best_prediction_ab)\n",
        "show_missed(X_test_cd, y_test_cd, best_prediction_cd)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EGMc99jdOSTx",
        "colab_type": "text"
      },
      "source": [
        "As foreseen by the distances matrix shown, the accuracy is very high for A and B, and personC is the most difficult to identify.\n",
        "\n",
        "A key message from here is that the results also vary with hyperparameters. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-59xSqg2PA7V",
        "colab_type": "text"
      },
      "source": [
        "#####Closest and Furthest nearest neighbors\n",
        "\n",
        "After this optimization step on the hog transformer and k-NN hyperparameter selection, we can:\n",
        "- recompute the pairwise distance and plot the resulting matrix, similarly as before\n",
        "- show the two closest images\n",
        "- show the neighbors with the largest distance in-between"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Gq3nETkQUhs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "get hog transformed for X_test and X_train\n",
        "'''\n",
        "X_test_hog = grid_res.best_estimator_['hogify'].transform(X_test)\n",
        "X_train_hog = grid_res.best_estimator_['hogify'].transform(X_train)\n",
        "\n",
        "'''\n",
        "Compute pairwise distances\n",
        "'''\n",
        "hog_distances_eucl = euclidean_distances(X_test_hog, X_train_hog)\n",
        "\n",
        "'''\n",
        "plot matrix\n",
        "'''\n",
        "plot_similarity_matrix(hog_distances_eucl)\n",
        "\n",
        "'''\n",
        "Using classifier kNN, get closest neighbours list\n",
        "'''\n",
        "closest_neighbors = grid_res.best_estimator_['classify'].kneighbors(X_test_hog, 10)\n",
        "logging.info(\"\\nImage # => Image Training idx @ distance \\n\")\n",
        "for i in range(closest_neighbors[1].shape[0]):\n",
        "    print(\"Image Test \" + str(i) + \" => closest neighbor: Image Training  \" + str(closest_neighbors[1][i,0]) + \"  @ \" + str(np.round(closest_neighbors[0][i,0],3)))\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ue-nLlZjm5Zj",
        "colab_type": "text"
      },
      "source": [
        "From the results printed above, where the closest image is shown for all test images, an interesting point to note is the min and max values of the closest neighbor distances:\n",
        "\n",
        "- Image Test #17 => very close to #38, 0.776\n",
        "- Image Test #35 => (not so) close to #19, 1.591\n",
        "\n",
        "We clearly see that the distance information gives an hint on the certainty of the identification: distance is very low for #17, and high for #19. This indicate that personD test image is actually pretty far from everything (considering the euclidean distance). \n",
        "\n",
        "Looking at the images, it actually appears clear for a human why those are considered the closest, and why the images appear much close for personB (Bradley Cooper, on top) than for Marc Blucas, personD (bottom line)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TVx4GwtQny4Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "Closest nearest neighbors of all\n",
        "'''\n",
        "plot_matrix([X_test[17,:],X_train[38,:]], color, my_color_map, h=1, w=2)\n",
        "show_one_image_hog(17-10, personB, 'test')\n",
        "show_one_image_hog(38-20, personB, 'training')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wMFbDh45pHgD",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "- personB: same pose, same scale, same view point, same face expression... The HOG will be similar!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1esG7zSkqgX4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "Furthest nearest neighbors of all\n",
        "'''\n",
        "plot_matrix([X_test[35,:],X_train[19,:]], color, my_color_map, h=1, w=2)\n",
        "show_one_image_hog(35-30, personD, 'test')\n",
        "show_one_image_hog(19, personA, 'training')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l066Jmt3qi91",
        "colab_type": "text"
      },
      "source": [
        "- personD image has a non-nominal face pose, and almost a quarter of the image on the right side is the background, leading to a vertical edge \"in the middle\". \n",
        "Without surprise, this makes personD's image far from everything else in terms of HOG representation, and the closest image is based on this face pose: the right eye and the vertical line are the most important elements of the representation. As a result, while the distance is larger than in previous case (analysis of personB), Emma Stone image becomes the closest to Marc Blucas' image. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lebK99WQWy9H",
        "colab_type": "text"
      },
      "source": [
        "###PCA feature descriptors\n",
        "\n",
        "We can repeat the steps performed for HOG feature representation, for the PCA feature representation. The same formalism is used."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TPRr-dStzqdb",
        "colab_type": "text"
      },
      "source": [
        "####Pre-process data\n",
        "\n",
        "As usual now, and because of our current architecture, it's necessary to re-import the data using `flatten = True` parameter. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AwsBqLUqHXLA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train = get_matrix_from_set(training_set, color, sq_size = sq_size, flatten = True)\n",
        "X_test = get_matrix_from_set(test_set, color, sq_size = sq_size, flatten = True)\n",
        "y_train = np.zeros((40,))\n",
        "y_train[20:40] = 1\n",
        " \n",
        "'''\n",
        "For now, set up \"0\" for personC; \"1\" for personD !\n",
        "'''\n",
        "y_test = np.zeros((40,))\n",
        "y_test[10:20] = 1\n",
        "y_test[30:40] = 1\n",
        "\n",
        "\n",
        "'''\n",
        "AB >< CD\n",
        "'''\n",
        "X_test_ab = X_test[0:20,:]\n",
        "y_test_ab = y_test[0:20]\n",
        "\n",
        "X_test_cd = X_test[20:40,:]\n",
        "y_test_cd = y_test[20:40]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gbp_uppTz3Pq",
        "colab_type": "text"
      },
      "source": [
        "We can vizualize - once more - the different sets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zonhIEds0J70",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "logging.info(\"Training set (horizontal axis):\")\n",
        "logging.info(\"Test set PersonA [0 -> 19]:\")\n",
        "plot_matrix(X_train[0:20,:], color, my_color_map, h=1, w=20, transpose = False)\n",
        "logging.info(\"Test set PersonA [20 -> 39]:\")\n",
        "plot_matrix(X_train[20:40,:], color, my_color_map, h=1, w=20, transpose = False)\n",
        "\n",
        "logging.info(\"Test set (vertical axis):\")\n",
        "logging.info(\"Test set PersonA [0 -> 9]:\")\n",
        "plot_matrix(X_test[0:10, :], color, my_color_map, h=1, w=10, transpose = False)\n",
        "logging.info(\"Test set PersonB [10 -> 19]:\")\n",
        "plot_matrix(X_test[10:20, :], color, my_color_map, h=1, w=10, transpose = False)\n",
        "logging.info(\"Test set PersonC [20 -> 29]:\")\n",
        "plot_matrix(X_test[20:30, :], color, my_color_map, h=1, w=10, transpose = False)\n",
        "logging.info(\"Test set PersonD [30 -> 39]:\")\n",
        "plot_matrix(X_test[30:40, :], color, my_color_map, h=1, w=10, transpose = False)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yo-fqug-z8tO",
        "colab_type": "text"
      },
      "source": [
        "####Compute pairwaise distances"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HXqKpzqeFauR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pcaify = None\n",
        "pcaify = sklearn_decomposition_PCA(n_components = 35)\n",
        "\n",
        "X_train_PCA = pcaify.fit_transform(X_train)\n",
        "X_test_PCA = pcaify.transform(X_test)\n",
        "\n",
        "\n",
        "# pca_distances_cos = cosine_distances(X_test_PCA, X_train_PCA)\n",
        "pca_distances_eucl = euclidean_distances(X_test_PCA, X_train_PCA)\n",
        "\n",
        "# plot_similarity_matrix((pca_distances_cos))\n",
        "plot_similarity_matrix((pca_distances_eucl))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NesODob42gg-",
        "colab_type": "text"
      },
      "source": [
        "####Analysis on the distances computed\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N-7zP7qe8LzP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "logging.info(\"Expected look-alike macroscopic coloration\")\n",
        "plot_similarity_matrix(test_dist_mtx, show_numbers=True, norm_only=True, width=3, height=3, fontsize=14)\n",
        "res=get_distances(pca_distances_eucl)\n",
        "\n",
        "logging.info(\"Macroscopic view of the pairwise distances coloration\")\n",
        "plot_similarity_matrix(res, show_numbers=True, norm_only=True, width=4, height=4)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GmcXREBf2sv1",
        "colab_type": "text"
      },
      "source": [
        "From a macroscopic view, personA and personB seems to confirm the expected behavior, specifically using the normed plot (on the right), or even better, the tiny macro representaiton just above.\n",
        "In this case of PCA feature representation, it indicates that the variance in the personA and personB test images is explained in a similar fashion as the variance of (one or more) training images.\n",
        "\n",
        "> The formalism behind the different plot was already explained above and is not repeated here. Please read again previous sections if needed.\n",
        "\n",
        "Similarly to the analysis performed on the classification task, the results differ notably for personC and personD with respect to HOG feature representation: \n",
        "- here, some personC seems properly similar to personA, and some personD seems properly similar to personB, without a clear indication that personC is globally much further from personA than personD is from personB.\n",
        "Said differently, there may be some personC close to personA. This wasn't really the case in the previous feature representation using HOG.\n",
        "- personD test images don't look that close to personB training image \"anymore\", as it was the case for the HOG representation. \n",
        "\n",
        "\n",
        "Considering the remarks done already, and the care needed to interpret this \"average distance\", it does not directly follows that the results of the identification using k-NN will be worse for personD than in the HOG feature. \n",
        "\n",
        "---\n",
        "\n",
        "Another point, more surprising maybe -- but enlighting the differences between the two feature representations -- is the training images that are the most dissimilar to test images.\n",
        "\n",
        "As we did before, let's sum the distances vertically, and show what image is \"globally\" the furthest. \n",
        "> Of course, this metric has its limitation: the sum of the distances may suffer from one very very high distance (or reversely, benefit from one very very close...) But this is quite precisely what we wish to show, and even if there are some pitfalls, it's a convenient way to illustrate our saying."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SzzQE5Ni2sCg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if color:\n",
        "    threshold = 355000\n",
        "else:\n",
        "    threshold = 200000\n",
        "eucl_dist_vert_sum = np.sum(np.abs(pca_distances_eucl), axis = 0)\n",
        "# print(eucl_dist_vert_sum)\n",
        "fig=plt.figure(figsize=(8,4))\n",
        "ax = fig.add_subplot(111)\n",
        "ax.scatter([i for i in range(len(eucl_dist_vert_sum))],eucl_dist_vert_sum)\n",
        "ax.plot([-1,41],[threshold,threshold], '-r')\n",
        "ax.set_title(\"Sum of the distances to test images, per training images\")\n",
        "plt.show()\n",
        "\n",
        "indices = np.where(eucl_dist_vert_sum > threshold)[0]\n",
        "logging.info(\"Indices of training images > Threshold: \" + str(indices))\n",
        "\n",
        "\n",
        "plot_matrix(X_train[indices,:], color, my_color_map, h=1, w=len(indices))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2UFuJvEF5uU5",
        "colab_type": "text"
      },
      "source": [
        "- The image that is the furthest from all test images is different from the previous feature representation. Note that the threshold is arbitrarily chosen.\n",
        "\n",
        "It means that the combination of weights of the eigenfaces is the most distant of all the combinations of the test images. The way the variance is explained in this #20 training image is different from the way it is explained in any test image."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LJtT8N7w19o9",
        "colab_type": "text"
      },
      "source": [
        "####Identification of test images using k-NN\n",
        "\n",
        "In this step, using k-nearest neighbor, the goal is to label the test images according to their nearest neighbours. In a very intuitive way, we could say that the images on the vertical axis of the matrix above should get as label the ones from the bluest set of neighbors (either personA (=0) or personB (=1))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ELv6zbO_vQz6",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "#####Choosing k\n",
        "As discussed in HOG-based identification, choosing a right number for k usually implies different test on validation set. That's what we did previously. \n",
        "\n",
        "In this PCA-based identification task, we will see another way - more intuitive but yet sensible.\n",
        "\n",
        "When plotting the matrix of distances, it appears with a lot of colors and it's not easy to determine really what k would be most appropriate. The question is \"what number k is a good trade-off so that most of the images seem to be identified appropriately\". \n",
        "Some thoughts: \n",
        "- k should not lead to too noisy identification, \n",
        "- k should be kept small,\n",
        "- k should be such that only distances that **matters** are taken into account.\n",
        "\n",
        "######What does it mean ? \n",
        "Described differently, we should select k such that only what it seems to be really relevant indicate the choice. \n",
        "\n",
        "Let's say we consider a sample and its neighbours. The 2 first neighbours are really close and indicate \"class 0\", and the three next neighbours are actually all much further - yet closest then remaining samples - and indicate \"class 1\". k should not take into account (too much at least) the neighbours 3,4 and 5, or the sample could be identified as a \"class 1\" while *obviously*, it should have been \"class 0\" thanks to the two closest. \n",
        "\n",
        "######How to do that ?\n",
        "Intuitively, we could look at the matrix above and evaluate \"by the eye\" the number of \"most blues\", on average. A little subtlety, however: what we are really interested in is the order of magnitude of the distance, hence we take the logarithms of the distances to visually represent better this order of magnitude. \n",
        "\n",
        "######On what data set should we do that ?\n",
        "Well, definitely not on the test set. As we don't have much input samples, and as this mostly constitue an intuition and not a rigorous approach, we decide to follow this approach on the full training set, instead of a validation set. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iANtJaXFybcc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dist_training_training = euclidean_distances(X_train_PCA, X_train_PCA)\n",
        "for i in range(dist_training_training.shape[0]):\n",
        "    dist_training_training[i,i]= max(dist_training_training[i,:]) \n",
        "\n",
        "logging.info(\"Pairwise distance between training images\\n(diagonal = max distances, for visualization)\")\n",
        "plot_similarity_matrix((dist_training_training), False, norm_only=True,cmap=plt.cm.jet)\n",
        "\n",
        "logging.info(\"Pairwise distance in log scale between training images\\n(diagonal = max distances, for visualization)\")\n",
        "plot_similarity_matrix(np.log(dist_training_training), False, norm_only=True, cmap=plt.cm.jet)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gn0PQnFm0NxV",
        "colab_type": "text"
      },
      "source": [
        "First matrix are the distances in regular scale, and the second are the distances in log scale. It gives a better intuition about the scale of the different distances.\n",
        "\n",
        "Following this last plot, a parameter $k=1$ or $k=2$ seems an acceptable choice: most of the images seems to be closest (darkest blue) to at least  one other corresponding images.\n",
        "Of course, it could be argued it doesn't seem mathematically rigorous and reliable. I agree and this is why the cross validation was used for the previous section. Nonetheless, I have presented here another way of selecting this parameter, efficiently."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qr_VZE91K6UT",
        "colab_type": "text"
      },
      "source": [
        "#####Prediction\n",
        "\n",
        "First, we play - again using the gridsearch method - to try out some combinations, and we predict the label of the test images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p_JWPGPAHt8j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "k=2\n",
        "\n",
        "'''\n",
        "Definition of a pipeline\n",
        "'''\n",
        "PCA_knn_pipeline = Pipeline([\n",
        "                         ('pcaify', sklearn_decomposition_PCA(n_components=35)\n",
        "                         ),\n",
        "                         ('classify', \n",
        "                          KNeighborsClassifier( n_neighbors = k, metric='euclidean')\n",
        "                         )])\n",
        "\n",
        "\n",
        "param_grid_knn = [\n",
        "    {\n",
        "        'pcaify__n_components': range(15,40,1),\n",
        "        'classify__n_neighbors':[1,2,3]}\n",
        "]\n",
        "\n",
        "grid_search = GridSearchCV(PCA_knn_pipeline,\n",
        "                           param_grid_knn, \n",
        "                           cv = 2, \n",
        "                           n_jobs = -1,\n",
        "                           scoring = \"accuracy\",\n",
        "                           verbose = 1, \n",
        "                           return_train_score = True)\n",
        "\n",
        "'''\n",
        "\"Training\"\n",
        "'''\n",
        "grid_res = grid_search.fit(X_train, y_train)\n",
        "logging.info(\"Best parameters : \" + str(grid_res.best_params_))\n",
        "logging.info(\"Best scores (CV): \" + str(100* grid_res.best_score_ ) + \"% \")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kR7wpW1KVxFl",
        "colab_type": "text"
      },
      "source": [
        "Now, we are ready to use the considered \"best\" pipeline in order to predict the labels:\n",
        " - $p=18$, number of components for the PCA transformer, \n",
        " - $k=1$, number of neighbors to label the test sample\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V9pxr0WJHeLk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "prediction\n",
        "'''\n",
        "best_prediction_ab = grid_res.predict(X_test_ab)\n",
        "logging.info('Percentage correct persons A, B: ' + str(100*np.sum(best_prediction_ab == y_test_ab)/len(y_test_ab)))\n",
        "best_prediction_cd = grid_res.predict(X_test_cd)\n",
        "logging.info('Percentage correct persons C, D: ' + str( 100*np.sum(best_prediction_cd == y_test_cd)/len(y_test_cd)))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "etctd2uzIe86",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "Visualization of the images\n",
        "'''\n",
        "logging.info(\"Tests - Identification of personA and personB\")\n",
        "show_missed(X_test_ab, y_test_ab, best_prediction_ab)\n",
        "\n",
        "logging.info(\"\\n\"*2)\n",
        "logging.info(\"Tests - Identification of personC and personD\")\n",
        "show_missed(X_test_cd, y_test_cd, best_prediction_cd)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UQEKYBxxMQ09",
        "colab_type": "text"
      },
      "source": [
        "We have reused the `GridSearchCV` technique, for the number of neighbours $k$, but also for the number of components $p$ used in the PCA representation.\n",
        "* For instance, good accuracy results can be achieved already with other parameters: \n",
        "    * $p=5$, $k=1$: test AB: 90%, test CD: 60% (cv = 2)\n",
        "    * $p=20$, $k=1$: test AB: 100%, test CD: 65% (cv = 2)\n",
        "\n",
        "Those parameters are found with GridSearch on larger batches. The considered best is the couple $(p,k) = (18,1)$, leading to test AB: 100% and test CD: 65%.\n",
        "\n",
        "As a reminder, \"considered best\" is **not** regarding the accuracy on the test results, but during the cross validation step.\n",
        "\n",
        "It is important to note (again:)) than the number of training samples being limited, cross validation may not deliver the full potential.\n",
        "\n",
        "With parameters $k=1$ and $p=18$, corresponding to the distances showed before, the identification results corresponds to the expectation:\n",
        "- high accuracy on personA and personB, \n",
        "- much lower accuracy on personC and personD which are further from all, in our case. Nonetheless, the results is better than guess - luckily!\n",
        "\n",
        "Comparing the results to classification ones, on personA and B, there is no difference. For personC and D, the misclassified images were indices [2, 3, 6, 7, 15, 16, 19]. In this identification step, the errors in labeling (based on 1! neighbours and 18 components), are [0, 3, 5, 6, 7, 12, 15]. They don't fully match, but we could yet say that if it's hard to identify, it's hard to classify as well."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rz6xrSjBPI5o",
        "colab_type": "text"
      },
      "source": [
        "#####Closest and Furthest nearest neighbors\n",
        "\n",
        "As for previous section, we can show with this feature representation the closest and furthest nearest neighbors.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2l5naAV-PN8q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "get PCA transformed for X_test and X_train\n",
        "'''\n",
        "X_test_PCA = grid_res.best_estimator_['pcaify'].transform(X_test)\n",
        "X_train_PCA = grid_res.best_estimator_['pcaify'].transform(X_train)\n",
        "\n",
        "'''\n",
        "Compute pairwise distances\n",
        "'''\n",
        "pca_distances_eucl = euclidean_distances(X_test_PCA, X_train_PCA)\n",
        "\n",
        "'''\n",
        "plot matrix\n",
        "'''\n",
        "plot_similarity_matrix(pca_distances_eucl)\n",
        "\n",
        "'''\n",
        "Using classifier kNN, get n closest neighbours list\n",
        "'''\n",
        "n=10\n",
        "closest_neighbors = grid_res.best_estimator_['classify'].kneighbors(X_test_PCA, n)\n",
        "logging.info(\"\\nImage # => Image Training idx @ distance \\n\")\n",
        "for i in range(closest_neighbors[1].shape[0]):\n",
        "    print(\"Image Test \" + str(i) + \" => closest neighbor: Image Training  \" + str(closest_neighbors[1][i,0]) + \"  @ \" + str(np.round(closest_neighbors[0][i,0],3)))\n",
        "\n",
        "\n",
        "distance_max = np.max(closest_neighbors[0][:,0])\n",
        "distance_max_index_test = np.where(closest_neighbors[0][:,0] == distance_max)[0][0]\n",
        "distance_max_index_train = closest_neighbors[1][distance_max_index_test,0]\n",
        "\n",
        "distance_min = np.min(closest_neighbors[0][:,0])\n",
        "distance_min_index_test = np.where(closest_neighbors[0][:,0] == distance_min)[0][0]\n",
        "distance_min_index_train = closest_neighbors[1][distance_min_index_test,0]\n",
        "print(\"Largest  \\\"closest distance\\\" is: \" + str(np.round(distance_max,0)) + \" between (test image,training image) = (\" + str(distance_max_index_test) + \", \"+str(distance_max_index_train) + \")\" )\n",
        "print(\"Smallest \\\"closest distance\\\" is: \" + str(np.round(distance_min,0)) + \" between (test image,training image) = (\" + str(distance_min_index_test) + \", \"+str(distance_min_index_train) + \")\" )\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zMfCvQRhRx6g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "Closest nearest neighbors of all\n",
        "'''\n",
        "plot_matrix([X_test[distance_min_index_test,:],X_train[distance_min_index_train,:]], color, my_color_map, h=1, w=2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QaNPcMkTSjLZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "Furthest nearest neighbors of all\n",
        "'''\n",
        "plot_matrix([X_test[distance_max_index_test,:],X_train[distance_max_index_train,:]], color, my_color_map, h=1, w=2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zibaCdmoVlJ2",
        "colab_type": "text"
      },
      "source": [
        "This constitutes a surprising result:\n",
        "- the nearest neighbours with the shortest distance, using the PCA input, is definitely not what a human eye would have guessed: two different persons. \n",
        "It means that the variance from the mean image is explained in both images in a similar fashion. This result is confirmed by the darkest blue point of the normed distance matrix. \n",
        "\n",
        "- the nearest neighbors having the largest distance is -- without surprise now -- between Jane Levy and Emma Stone. \"Luckily\", it yet gives the correct labeling result, but it also tends to indicate that the test image is quite different, PCA-feature wise, from the others. \n",
        "\n",
        "\n",
        "\n",
        "We could go on and look for other funny things out from those numbers, such as the test image which is the furthest from any training image."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eX98Yy-RYMv_",
        "colab_type": "text"
      },
      "source": [
        "###Identification - Conclusion\n",
        "\n",
        "In this identification task, we computed a similarity score -- the euclidean distance -- pairwise. Using the distance from a test image to a training image, we labeled the test images according to $k$ neighbors.\n",
        "We confirmed the role of different parameters for both methods, and the results and trends on those results that we could already observed in the previous tasks. \n",
        "We also saw that the labeling using k-NN was at least \"as easy\" as the classification task (in terms of performance reached), specifically for personC and personD.\n",
        "\n",
        "Of course, this is not the end of the story, and many more things could be achieved:\n",
        "- analyzing in more depth the influence of the parameters of the feature representations, \n",
        "- for each test images, assessing the distribution of the distance to all other images, \n",
        "- within a class, assessing the distance between each pair of images\n",
        "    * this would be linked to the notion of cluster \n",
        "- assess if there is a \"predominant\" image, an image that is close to many others\n",
        "    * this actually would be the reverse of what we did when we observed which images were the furthest from all\n",
        "- ...\n",
        "\n",
        "There is no exhaustive list on what can be done. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yuHswiWNLqzm",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "## Impress your TA's\n",
        "\n",
        "> As said at the beginning of the classification section, I was normally exempted to perform the classification part but yet decided to do it, per personnal interest and eagerness to learn. I hope it will also help in the sense of impressing my TA's.\n",
        "\n",
        "During the sections on classification and identification, a lot of work has been done to define a clean way to perform the tasks, using the `Transformer` and `Pipeline`. Also, the notion of `GridSearchCV` was introduced and used in the identification task to help choosing the $k$ parameter based on a reliable method.\n",
        "\n",
        "\n",
        "However, until now, performance of the classification are \"behind\" the ones of the identification, specifically regarding the HOG representation. In this section, we will try to improve the performances of the HOG-based classification, and the PCA-based classification, using the *gridsearch*. \n",
        "We will also try and understand better the classification results, specifically the HOG classifier, using two template images that we create on purpose.\n",
        "\n",
        "> This part has been mostly done using `color = False` and `sq_size = 64`. There can be differences in the results if those parameters are changed."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ylHJVo3Nckds",
        "colab_type": "text"
      },
      "source": [
        "####HOG Classification - Optimization\n",
        "The results obtained above in [this cell](https://colab.research.google.com/drive/1OYq1-SZZURJ5uujmf3PTqdEx3SDGAqZQ#scrollTo=mkYFiPjIKHan&line=2&uniqifier=1) are not so bad for a first attempt, but it's worth it to assess if we can do better!\n",
        "\n",
        "> better: obtain better **accuracy** results on the test set. Of course, we don't want to tailor the parameters **for** the test set images. \n",
        "\n",
        "In order to optimize the results of the classification, we will first implement a **systematic** way of searching for optimal parameters on the training set, using [`GridSearchCV`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html):\n",
        "- intrinsic cross-validation (the training set is successively split into different subset to allow cross validation)\n",
        "- automatic parameter testing, according to the given sequence of possibilities to test. \n",
        "\n",
        "This is therefore much less of a \"manual\" process: the system automatically goes through all possible parameter combinations, and establishes the metrics of the tested models using cross validation set.\n",
        "\n",
        "\n",
        "To perform this *gridsearch*, we reimplement the same steps as before. We speedup a little the process by retrieving the backup variables we set in the previous steps"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a_VVU0ircabA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "Make sure of the inputs\n",
        "'''\n",
        "X_train = X_train_HOG_shuffled_back.copy()\n",
        "y_train = y_train_HOG_shuffled_back.copy()\n",
        "\n",
        "X_test_ab = X_test_HOG_ab_back.copy()\n",
        "y_test_ab = y_test_HOG_ab_back.copy()\n",
        "\n",
        "X_test_cd = X_test_HOG_cd_back.copy()\n",
        "y_test_cd = y_test_HOG_cd_back.copy()\n",
        "\n",
        "\n",
        "\n",
        "'''\n",
        "Definition of a pipeline\n",
        "'''\n",
        "HOG_pipeline = Pipeline([\n",
        "                         ('hogify', HogTransformer(\n",
        "                             orientations = 9,\n",
        "                             pixels_per_cell = (8,8),\n",
        "                             cells_per_block = (2,2),\n",
        "                             block_norm='L2',\n",
        "                             transform_sqrt=True, \n",
        "                             multichannel = color)\n",
        "                         ),\n",
        "                         ('classify', SGDClassifier(\n",
        "                             random_state = 42, \n",
        "                             max_iter = 1000, \n",
        "                             tol=1e-3)\n",
        "                         )])\n",
        "\n",
        "\n",
        "'''\n",
        "Definition of the parameters grid\n",
        "'''\n",
        "\n",
        "param_grid_HOG = [\n",
        "    {\n",
        "        'hogify__orientations': [9],\n",
        "        'hogify__cells_per_block': [(2, 2),(3, 3)],\n",
        "        'hogify__pixels_per_cell': [(4,4),(8, 8),(16, 16)],\n",
        "        'hogify__block_norm': ['L2-Hys','L1', 'L2'],\n",
        "        'hogify__transform_sqrt': [False, True],\n",
        "        'hogify__multichannel':[color],\n",
        "        'classify': [\n",
        "            SGDClassifier(random_state=42, max_iter=1000, tol=1e-5),\n",
        "            svm.SVC(kernel='linear', C=0.1),\n",
        "            svm.SVC(kernel='linear', C=1)]}\n",
        "]\n",
        "\n",
        "'''\n",
        "Creation of the object GridSearch\n",
        "'''\n",
        "grid_search_HOG = GridSearchCV( HOG_pipeline,\n",
        "                                param_grid_HOG, \n",
        "                                cv = 4, \n",
        "                                n_jobs = -1,\n",
        "                                scoring = \"accuracy\",\n",
        "                                verbose = 1, \n",
        "                                return_train_score = True)\n",
        "\n",
        "'''\n",
        "Train the model, trying out the combination\n",
        "'''\n",
        "grid_res_HOG = grid_search_HOG.fit(X_train, y_train)\n",
        "\n",
        "# print(grid_res_HOG.best_estimator_)\n",
        "print(\"\\n\"*3)\n",
        "print(\"==\"*40)\n",
        "print(\"Best accuracy score (cross validation): \" + str(100*grid_res_HOG.best_score_) + \" %\")\n",
        "print(\"Summary of the search best parameters:\")\n",
        "print(\"orientations = \", grid_res_HOG.best_params_['hogify__orientations'])\n",
        "print(\"cells_per_block = \", grid_res_HOG.best_params_['hogify__cells_per_block'])\n",
        "print(\"pixels_per_cell = \", grid_res_HOG.best_params_['hogify__pixels_per_cell'])\n",
        "print(\"block_norm = \", grid_res_HOG.best_params_['hogify__block_norm'])\n",
        "print(\"transform_sqrt = \", grid_res_HOG.best_params_['hogify__transform_sqrt'])\n",
        "print(\"classifier = \", grid_res_HOG.best_params_['classify'])\n",
        "print(\"==\"*40)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CuCBMbM4vsA-",
        "colab_type": "text"
      },
      "source": [
        "The best parameters found according to our *gridsearch* lead to a major change in:\n",
        "- `pixels_per_cell`: leading to a less finer cell definition. Spacially, it leads to a wider low-pass filtering. This was described in the first part of this notebook,\n",
        "- `transform_sqrt`: which, according to the authors of the base paper, needs to be tried out experimentally to \"decide\", \n",
        "- `block_norm`: which, similarly, needs to be tested on cross validation set before being adopted.\n",
        "\n",
        "The best classifier, among the ones tested, remain the SGD used already in previous sections.\n",
        "\n",
        "We can now simply used the *best* estimator found by the *gridsearch* to perform prediction, and visualize the results (both accuracy and images themselves)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y1vyGciovrDu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "predict AB\n",
        "'''\n",
        "best_prediction_ab = grid_res_HOG.predict(X_test_ab)\n",
        "logging.info(\"Percentage correct    : \" + str(100*np.sum(best_prediction_ab == y_test_ab)/len(y_test_ab)))\n",
        "show_missed(X_test_ab, y_test_ab, best_prediction_ab)\n",
        "\n",
        "my_plot_confusion_matrix(grid_res_HOG, X_test_ab, y_test_ab, [\"Emma Stone\", \"Bradley Cooper\"])\n",
        "\n",
        "\n",
        "'''\n",
        "predict CD\n",
        "'''\n",
        "best_prediction_cd = grid_res_HOG.predict(X_test_cd)\n",
        "logging.info(\"Percentage correct    : \" + str(100*np.sum(best_prediction_cd == y_test_cd)/len(y_test_cd)))\n",
        "show_missed(X_test_cd, y_test_cd, best_prediction_cd)\n",
        "my_plot_confusion_matrix(grid_res_HOG, X_test_cd, y_test_cd,[\"Jane Levy\", \"Marc Blucas\"] ,[\"Emma Stone\", \"Bradley Cooper\"])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PsF7IkPq2BAv",
        "colab_type": "text"
      },
      "source": [
        "After this optimization pass, we clearly see that the best classification result is 100% for personA and personB. This is much better than what we had originally (80%).\n",
        "This confirms that, after optimization, we cannot (at least without deeper analysis) state that one feature representation or the other is intrinsically better than the others. Most likely, it depends on other metrics as well.\n",
        "\n",
        "Because of the new hyper-parameters defined, the decision boundary changes such that more personC and personD images are misclassified.\n",
        "\n",
        "This is actually not surprising, as the hyperparameters are tailored against cross-validation sets, hence containing only personA and personB images. There is no cross-validation using personC or personD images. Their classification results are therefore not expected to improve. \n",
        "In our specific case, we even see that the accuracy in classifying C and D is just as a random classifier, basically. Looking at the confusion matrix, it clearly appears that personC is the one leading (mainly) to such a bad score. \n",
        "This behavior is exactly the one already discussed in previous section, related to HOG locality and the *particular* descriptor of personA, with the oblique hair.\n",
        "\n",
        "These optimization results also open up the possible ways of parameters tuning in regards of the ultimate goal of the system being designed: how do we want the system to perform on a test set containing personC and personD images ?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xAhQHgnmtavr",
        "colab_type": "text"
      },
      "source": [
        "####PCA Classification - Optimization\n",
        "\n",
        "While the results obtained earlier using the PCA feature representation are already very good, with 100% accuracy on the test sets of personA and personB, we can use the *gridsearch* technique to test different number of principal components taken into account, or even to try out different classifiers.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0fx_fzPdtegL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "Make sure of the inputs\n",
        "'''\n",
        "X_train_PCA = X_train_PCA_shuffle_back.copy()\n",
        "y_train_PCA = y_train_PCA_shuffle_back.copy()\n",
        "X_test_PCA_ab = X_test_PCA_ab_back.copy()\n",
        "X_test_PCA_cd = X_test_PCA_cd_back.copy()\n",
        "y_test_PCA_ab = y_test_PCA_ab_back.copy()\n",
        "y_test_PCA_cd = y_test_PCA_cd_back.copy()\n",
        "\n",
        "'''\n",
        "Set up a grid of parameters to test\n",
        "'''\n",
        "param_grid_PCA = [\n",
        "    {\n",
        "        'pcaify__n_components': range(10,41,1),\n",
        "        'classify': [\n",
        "            SGDClassifier(random_state=42, max_iter=10000, tol=1e-5),\n",
        "            svm.SVC(kernel='linear', C=1e-5, tol=1e-5 ),\n",
        "            svm.SVC(kernel='linear', C=0.1, tol=1e-5 )\n",
        "            ]}\n",
        "]\n",
        "\n",
        "'''\n",
        "create the GridSearchCV object\n",
        "'''\n",
        "grid_search_PCA = GridSearchCV(PCA_pipeline,\n",
        "                           param_grid_PCA, \n",
        "                           cv = 5, \n",
        "                           n_jobs = -1,\n",
        "                           scoring = \"accuracy\",\n",
        "                           verbose = 1, \n",
        "                           return_train_score = True)\n",
        "\n",
        "grid_res_PCA = grid_search_PCA.fit(X_train_PCA, y_train_PCA)\n",
        "\n",
        "# logging.info(\"Best Score            :\" + str(grid_res_PCA.best_score_))\n",
        "# logging.info(\"Best Parameters found :\")\n",
        "# logging.info(grid_res_PCA.best_params_)\n",
        "print(\"\\n\"*3)\n",
        "print(\"==\"*40)\n",
        "print(\"Best accuracy score (cross validation): \" + str(100*grid_res_PCA.best_score_) + \" %\")\n",
        "print(\"Summary of the search best parameters:\")\n",
        "print(\"n_components = \", grid_res_PCA.best_params_['pcaify__n_components'])\n",
        "print(\"classifier = \", grid_res_PCA.best_params_['classify'])\n",
        "print(\"==\"*40)\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jvC-6e651Ifa",
        "colab_type": "text"
      },
      "source": [
        "Based on the 5-fold-cross-validation, the best accuracy score is 100%, and more interestingly, the number of components is now only 15, when using another classifier, a linear support vector machine with a very small regularization parameter (leading actually to a really strong regularization, see [doc](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html).\n",
        "\n",
        "We can now use the \"best found\" pipeline to predict the class of our persons."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oPM1HmXi1E8P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "predict AB\n",
        "'''\n",
        "best_prediction_ab = grid_res_PCA.predict(X_test_PCA_ab)\n",
        "logging.info(\"Percentage correct    : \" + str(100*np.sum(best_prediction_ab == y_test_PCA_ab)/len(y_test_PCA_ab)))\n",
        "show_missed(X_test_PCA_ab, y_test_PCA_ab, best_prediction_ab)\n",
        "my_plot_confusion_matrix(grid_res_PCA, X_test_PCA_ab, y_test_PCA_ab,[\"Emma Stone\", \"Bradley Cooper\"])\n",
        "\n",
        "'''\n",
        "predict CD\n",
        "'''\n",
        "\n",
        "best_prediction_cd = grid_res_PCA.predict(X_test_PCA_cd)\n",
        "logging.info(\"Percentage correct    : \" + str(100*np.sum(best_prediction_cd == y_test_PCA_cd)/len(y_test_PCA_cd)))\n",
        "show_missed(X_test_PCA_cd, y_test_PCA_cd, best_prediction_cd)\n",
        "my_plot_confusion_matrix(grid_res_PCA, X_test_PCA_cd, y_test_PCA_cd,[\"Jane Levy\", \"Marc Blucas\"] ,[\"Emma Stone\", \"Bradley Cooper\"])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6lbWtU-rtfg6",
        "colab_type": "text"
      },
      "source": [
        "After this optimization pass, we clearly see that the best classification result remain 100%, without surprise, for personA and personB. \n",
        "Unlike the HOG-based classifier, the accuracy result on personC and D hasn't changed, and what's more, the samples misclassified have not changed. It tends to indicate that the eigenfaces after 15 (excluded) are not that useable for the classifier to differentiate the classes.\n",
        "\n",
        "Similarly to what was said for the HOG classifier optimization, the fact the accuracy does not improve for personC and personD is expected: the hyper-parameters are selected against crossvalidation, hence containing only personA and personB images. When looking closer, it appears that - once again - it's the performance on personC that penalized overall accuracy on C and D: the classifier acts as a random classifier for personC. Also there, we discussed already largely this behavior in the previous section.\n",
        "\n",
        "\n",
        "With this optimized pipeline, we reach the same accuracy on personA and personB test set with only 15 components, instead of 35 before. This is a nice result as it means less computing in the end."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_5419Y_tpL9l",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "####Understanding our classifiers\n",
        "\n",
        "Without going much deeper in the analysis, it may be useful to illustrate once more the differences between the classifiers (HOG-based vs PCA-based).\n",
        "- HOG: For an image to be classified as personA, it is much easier if it has, in HOG representation, the oblique line, characteristic of personA haircut. This indicates at least two things:\n",
        "    * the sensitivity of the classifier to \"edge details\" of the image. In this case, it is the haircut. It could be also, for instance a skirt collar for a man, a beard, ... Those information are visual characteristic and can be found deeply in the HOG. It is useful sometimes... but also may not be highly reliable, as we will see in the example below: a simple image with an oblique line.\n",
        "    * the lack of variability of the training set images. That is, recognizing Emma Stone is almost reduced to recognizing this oblique line. Similarly, classifying as Bradley cooper is reduced to recognizing a vertical line on the right side, thanks to its hair shape and forehead, and face's shape. \n",
        "\n",
        "To illustrate this, let's just create two images, using the function created in the first part of this notebook. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "llYpMENeiL9B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "Construction of a new set, with two template images\n",
        "'''\n",
        "\n",
        "personA_template = create_image(64, 64, special=\"personA\")\n",
        "personB_template = create_image(64, 64, special=\"personB\")\n",
        "\n",
        "new_set={}\n",
        "new_set[personA]=[personA_template]\n",
        "new_set[personB]=[personB_template]\n",
        "new_set[personC]=[]\n",
        "new_set[personD]=[]\n",
        "\n",
        "\n",
        "''' \n",
        "pre-processing\n",
        "'''\n",
        "new_X_HOG = get_matrix_from_set(new_set, color, sq_size, flatten=False)\n",
        "\n",
        "'''\n",
        "visualization\n",
        "'''\n",
        "plot_matrix(new_X_HOG, color, my_color_map, h=2, w=2)\n",
        "\n",
        "'''\n",
        "Prediction using best gridsearch output\n",
        "'''\n",
        "res_HOG = grid_res_HOG.predict(new_X_HOG)\n",
        "\n",
        "logging.info(\"First  image labeled as class: \" + str(res_HOG[0]).split(\".\")[0])\n",
        "logging.info(\"Second image labeled as class: \" + str(res_HOG[1]).split(\".\")[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yzjvCWR5pkgr",
        "colab_type": "text"
      },
      "source": [
        "We see that -- as expected -- the first image with the oblique line is classified as \"0\", Emma Stone, and the second is classified as \"1\", Bradley Cooper.\n",
        "\n",
        "\n",
        "\n",
        "- PCA: To understand better the PCA, we need to look at the eigenfaces, and the variance that each of the eigenfaces can explain. In the classification task, we deeply cover the rotation of an image of personC. Its variance introduced by the rotation could not be well explained, leading to a classifier actually assigning the wrong class. We saw that manually reducing this variance (= rotating the image) lead to a correct (= expected) result from the classifier. As already discussed, rotation isn't the only thing affecting the PCA-based classifier results (background, lighting, ...)\n",
        "\n",
        "For the sake of completeness, it is interesting to show the results on PCA-based classification of the two *template* images created above, for the HOG feature. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dlpBD1rjqFkp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "pre-processing\n",
        "'''\n",
        "new_X_PCA = get_matrix_from_set(new_set, color, sq_size, flatten=True)\n",
        "\n",
        "'''\n",
        "Visualization\n",
        "'''\n",
        "plot_matrix(new_X_PCA, color, my_color_map, h=2, w=2)\n",
        "\n",
        "'''\n",
        "Prediction\n",
        "'''\n",
        "res_PCA = grid_res_PCA.predict(new_X_PCA)\n",
        "\n",
        "logging.info(\"First  image labeled as class: \" + str(res_PCA[0]).split(\".\")[0])\n",
        "logging.info(\"Second image labeled as class: \" + str(res_PCA[1]).split(\".\")[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ieV9zFiwr5HK",
        "colab_type": "text"
      },
      "source": [
        "Clearly, this is a proof both classifiers don't work the same way. Classifying from a linear combination of the eigenfaces, the **same** class is given to both \"template\" images, while they are visually really different.\n",
        "\n",
        "While PCA-based and HOG-based classifiers don't work the same way, the interpretation of misclassification may be easier (at least, in my opinion based mostly on this project), on the HOG feature representation, which is essentially based on the edges on an image.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k-fU8twAA2aM",
        "colab_type": "text"
      },
      "source": [
        "####Other optimizations\n",
        "\n",
        "The results in classification and identification are not that bad, with 100% on the test sets for all personA and personB run once optimized. However, we may want to robustify our predictions, and include much more data. \n",
        "\n",
        "It is possible to \"artificially\" generate new data without too much effort. Indeed, we saw an example of a rotation (when understanding better Classification based on PCA). What we may be interested in is populating our current data set simply by generating new images:\n",
        "- rotation of every images several times for different angular values.\n",
        "    * using different rotation center\n",
        "    * completing the rotated images with pixels (background) having different colors (color level)\n",
        "- translation of every images several amount of pixels, in different directions\n",
        "- add some energy in the image, particularly for PCA which is not robust to lighting\n",
        "- hide some parts of the images\n",
        "\n",
        "It will quickly increase the amount of data there is, and robustify the classification results (even if the result is already very good). One shall however make sure to not fall into overfitting when training the classifier.\n",
        "\n",
        "Last point, we explained previously why we would not scale the data - that is, we would not make the variance between 0 and 1. In the context of getting the very best out of our data, we should repeat the experience with the scaling implemented, and confirm the resulting behavior."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jwWorQY8u-04",
        "colab_type": "text"
      },
      "source": [
        "##Discussion\n",
        "\n",
        "CONGRAT's !! You are at the end of this tutorial and if you've reached this step, you most likely understand everything related to HOG, PCA, Classification and Identification.\n",
        "\n",
        "A lot has already been said overall in this tutorial - we'll try to wrap this up."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1n2dypTADvvu",
        "colab_type": "text"
      },
      "source": [
        "###Summary of the activities\n",
        "This tutorial was quite long, and it's even possible we forgot what we did... So let's refresh our memory!\n",
        "\n",
        "1. we retrieved the data, carefully and really randomly. This leads to a training set of faces from personA (20) and personB (20), and a test set from personA (10) and personB(10), but also personC (10) and personD (10).\n",
        "\n",
        "2. we spent quite some time on feature representation constructions:\n",
        "    * HOG, where we detailed precisely how to compute the gradients, the histogram (we even built several toy images to understand the very essence of the Histogram). We learnt what a cell is, what a block is, how to compute normalization, ... and the influence of all these (hyper-)parameters. A homemade class was coded to reproduce library results\n",
        "    * PCA, where we also went through the maths and saw three different ways to compute the principal components. We also compared results on examples with library results, and we detailed the effect of the number of components chosen in terms of error. In particular, we spent some time on analyzing the **explained variance**, **cumulative energy**, and **optimal number of principal components**. \n",
        "\n",
        "To give a bit of insight, we represented those quite high-dimensional features in the 2D space using t-SNE tool. \n",
        "\n",
        "All of that was done first, with the ultimate goal of building systems for classification, and for identification.\n",
        "\n",
        "3. we built a classification system, first based on the HOG feature representation, then on the PCA feature representation. \n",
        "    - we learnt progressively what is:\n",
        "        * the required pre-processing steps, including the building of the features,\n",
        "        * the pipeline architecture, using Transformers\n",
        "        * how to deal with python libraries, and specifically `sklearn` functions\n",
        "    - the results were deeply analyzed:\n",
        "        * HOG: we digged into the HOG representation of misclassified images\n",
        "        * PCA: we succeeded in modifying (according to a plan well established :-) ) an input image  such that it could pass the classification tests.\n",
        "    \n",
        "In particular, we saw that it appears easier to interpret the results from HOG classification than PCA classification. Also, without any optimization per se, the PCA-based classifier eached higher accuracy scores than HOG-based classifier on the test set.\n",
        "\n",
        "4. we built an identification system, first based on the HOG feature representation, then on the PCA feature representation. \n",
        "    - before this, we detailed the formalism to study the metrics, the euclidean distance between the representations\n",
        "        - we saw that comparing different distances (manhattan, cosine, ...) euclidean distance was giving appropriate results\n",
        "    - we showed, using a colorful matrix representation, that indeed, from a macroscopic perspective, personA test images were closer to personA training images, and resp. for personB. However, we saw funny things for personC and personD. \n",
        "        - in regards of the classification results obtained before, the distances computed confirmed what we had: personC is very far from personA in terms of HOG feature, while it's more \"fuzzy\" for the PCA feature. This is a very nice take-home message: all the representation don't lead to the same results, so that there really is \"engineering\" behind such systems.\n",
        "    - using k-NN classifer, we assigned a label to personA, personB, personC and personD test images, and discussed the results. \n",
        "        * To assign this label, we needed to define the k parameter: the number of neighbors that would participate to the labeling decision. We decided not to go for a fancy weighted model, but rather to use \n",
        "            * a gridsearch technique, that is already an optimization of the kind using cross validation\n",
        "            * an intuitive technique, where the distances are shown in colors, and - using a logarithmic scale -, only a few neighbors seem of interest to label the image\n",
        "        * We confirmed the classification results in the sense that it's often \"easier\" for personA and personB than personC and personD.\n",
        "    - for both representation, we showed the closest and furthest nearest neighbor of our dataset (between test image and training image)\n",
        "        * This system/model can of course show the three closest neighbors, three furthest, ... many improvements and extra work are suggested.\n",
        "    \n",
        "5. Beyond the optimization and improvement techniques used along the way, we continued in presenting a little deeper the Grid Search, and improved the original results obtained, in particular:\n",
        "    * in terms of accuracy for the classification using HOG feature, \n",
        "    * in terms of number of useful components using PCA feature\n",
        "\n",
        "We continued showing the difference in essence between the two classifiers showing the results on two template images created specifically. Finally, we suggested different ways of populating our training set in order to robustify the classification and identification results, without requiring other faces downloads.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xskvKbE6EC0h",
        "colab_type": "text"
      },
      "source": [
        "###Main differences between PCA and HOG\n",
        "\n",
        "In this section, we come back to a few key differences between both features we used in our tutorial\n",
        "\n",
        "- HOG is based on gradients, their magnitude and orientation. Intuitively, it corresponds to the edges (cfr Obama pictures). It's therefore heavily impacted by rotation, translation, ... which makes the interpretation easy when looking at the visual representation of the HOG (Without this image of \"bars\", it's not always easy: see for instance personC classification results). Many parameters allow to tune the robustness to noise, to lighting condition, or the \"granularity\" of the representation. \n",
        "\n",
        "- PCA is based on finding the intrinsic directions of maximal variance in the images. It is a very famous and useful technique for dimensionality reduction by selecting only the $p$ most important component. This can also be used for denoising. It is however an undesirable property when little variance is needed to differentiate between classes: reducing the dimensionality may decrease the performance. \n",
        "PCA technique is sensitive to lighting conditions, rotation, scale, background, ...\n",
        "While the eigenfaces are interpretable in qualitative terms 'this eigenface tends to emphasize the contrast between this and this', it's (as far as I'm concerned) less intuitive to fully interpret the results without analyzing deeper the numbers.\n",
        "\n",
        "In terms of accuracy, (all results given after optimization), with `sq_size = 64` and `color = False`\n",
        "- A,B:     \n",
        "    * HOG: \n",
        "        - classification: 100%\n",
        "        - identification: 100%\n",
        "    * PCA:\n",
        "        - classification: 100%\n",
        "        - identification: 100% \n",
        "- C,D: \n",
        "    * HOG: \n",
        "        - classification: 50%\n",
        "        - identification: 70%\n",
        "    * PCA:\n",
        "        - classification: 65%\n",
        "        - identification: 65% \n",
        "\n",
        "\n",
        "During all our classification/identification tasks, we have observed and explained why the results in terms of accuracy were worse for personC and D than A and B. This of course matches the intuition as, even if the faces are visually similar for a human, with respect to some physical aspects, those aspects may not be the ones captured by the features and classifiers/identification systems. In particular, we saw that the HOG transform would consider Marc Blucas really similar to Bradley Cooper...as well as Jane Levy! At least, in comparison with Emma Stone, which has a remarkable oblique haircut.\n",
        "\n",
        "To go on even further, we could definitely go along the road of the \"other optimizations\" suggested (see [other optimizations](https://colab.research.google.com/drive/1OYq1-SZZURJ5uujmf3PTqdEx3SDGAqZQ#scrollTo=k-fU8twAA2aM&line=11&uniqifier=1) ). \n",
        "\n",
        "A message learnt seems to be that HOG is particularly well suited to recognized specific pattern in an image -- just like an obvious oblique haircut. Parameters of course allow to deviate from that pattern, but in essence, that's what it is. PCA, on the other hands, may grasp better overall information from the data themselves. Depending on the goal of the application, both features could lead to different results - or different ease to reach the desired performances.\n",
        "\n",
        "This leads to the question: on a real system, what is the message learnt of such results?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lclMiqFhLZrj",
        "colab_type": "text"
      },
      "source": [
        "###Classification or Identification\n",
        "\n",
        "As is, there is no clear answer :-)\n",
        "In the context of this tutorial, we reached pretty good results with both systems. \n",
        "\n",
        "Conceptually, an identification does not really learn anything - it \"simply\" computes a **relevant** metric between the feature, and gives the most appropriate label based on that. On the contrary, the classifier try to find a boundary, with a \"clear\" separation between the classes. Both systems can perform many things, but the challenges are different:\n",
        "- Identification is hard if the number of pairwaise computation is immense, \n",
        "- the metric to use as similarity measure may not be easy to find, specifically in high dimension (see the curse of dimensionality)\n",
        "- For the classification, there is moreover the challenge of finding the appropriate classifier, and its adapted hyper-parameters.\n",
        "\n",
        "What we saw in particular along this tutorial is that results on all systems differ because of intrinsic characteristic (bias) of the methods. As a perfect example, the identification with $k=1$: there needs to be only one very close training image to be properly labeled. However, this may be subject to \"noise\", as the background/viewpoint (see [HOG](https://colab.research.google.com/drive/1OYq1-SZZURJ5uujmf3PTqdEx3SDGAqZQ#scrollTo=1esG7zSkqgX4&line=1&uniqifier=1) or [PCA](https://colab.research.google.com/drive/1OYq1-SZZURJ5uujmf3PTqdEx3SDGAqZQ#scrollTo=zMfCvQRhRx6g&line=3&uniqifier=1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oDAlsnV1o82D",
        "colab_type": "text"
      },
      "source": [
        "### Authentication system\n",
        "\n",
        "Let's imagine a company wants to purchase our face detection system to perform authentication on its employees... What would be our advices ?\n",
        "\n",
        "The questions raised by an authentication system is much more complicated that it could seem. This answer will be centered on computer vision topic.\n",
        "\n",
        "First, obviously, it is currently very easy to fool the different systems, see examples of the template images. Linked to the same idea, we shall avoid that someone just print out a 1:1 scale picture of one of the employees in order to get access somewhere. A solution for that is to have an extra system verifying that there is movement, or some distance measurement so that it's not a flat picture,... This is outside of the scope of current discussion. \n",
        "\n",
        "Regarding the authentification system, the first thing to understand is the need, in particular in terms of penalty and scores.\n",
        "> what is worse: false positives (authenticating X as Y, giving to X Y's access rights) , or false negatives (not authenticating X as X) ?\n",
        "\n",
        "We saw both systems (classification and identification) can reach an accuracy of 100% quite \"easily\", while the results on \"unknown person\" (personC and D)differ. My succinct advices would then be:\n",
        "- make sure to have a training set large enough for each of the employees\n",
        "- use a threshold quite high in terms of system confidence on the output\n",
        "- use a combinations of both feature representations\n",
        "    * the combination was not used in this notebook\n",
        "\n",
        "\n",
        "#####Classification or Identification ?\n",
        "We can imagine a k-NN identification, as implemented currently, with a (low) thresholding on the distance computed, so that only a face that is really close to another gets labelled. The \"drawback\" of such system is that the training set needs to be large enough to decrease the amount of false positives: there needs to be an image \"really close\" to the test image.\n",
        "\n",
        "A priori, good results are also obtained in this prototype with the classifiers. This is most likely what we wish for:\n",
        "- high accuracy on personA and personB, \n",
        "- \"low accuracy\" (= close to random) on personC and personD.\n",
        "    * this is actually not entirely true if are looking at the results on C and D separately. \n",
        "\n",
        "Definitely, for both classifiers and identification systems, those cases of personC and personD need to be well-thought, and it seems a threshold on the system confidence score needs to be established. Again, this is to put in regards of the False Positive / False Negative penalty scores. \n",
        "\n",
        "Other possibilities to try out would be to organize a vote between several classifiers/identification systems. According to the metrics to reach, this could lead to a very **robust** and reliable system!\n",
        "\n",
        "#####Dataset sizing\n",
        "Currently, the size of the different sets is of course too low to guarantee any production-grade performances: globally, it tends to lower down the confidence results. In all cases, increasing the training sets (and test sets) would allow:\n",
        "- performing extensive and reliable cross-validation of the model parameters, \n",
        "- cover more poses, viewpoints, scale, light conditions, ... in order to avoid misclassification/misidentification, \n",
        "- potentially make the threshold even higher in terms of confidence level\n",
        "\n",
        "Provided that some regularization mechanism are implemented as well (or accuracy may drop in real life)\n",
        "\n",
        "\n",
        "#####What feature to be based on ?\n",
        "*Simpler* feature representations as HOG give already very good results provided fine tuning. Besides, taking into account the environment (light conditions, pose, ...), some features may be more robust.\n",
        "- Can there be a led indicating where the employee gaze should point at ?\n",
        "- Can they keep their glasses or not ? \n",
        "    * is it supposed to work with / without glasses ? \n",
        "- Is the lighting purely artificial (and controled), or is it to be implemented in a hall where natural light is abundant, leading to changing light conditions ?\n",
        "\n",
        "All those questions may lead to the use of different features: HOG is more robust to lighting if proper normalization is done, as already discussed.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ACBc-OIoScJQ",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "\n",
        "Thanks!\n",
        "\n",
        "Geoffroy Herbin, R0426473\n"
      ]
    }
  ]
}