{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CV_Group9_assignment.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gherbin/ComputerVisionKUL/blob/master/CV_Group9_assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SnkjeOT3GuBy",
        "colab_type": "text"
      },
      "source": [
        "Import all packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KH_DPxxjTeoI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import cv2\n",
        "import tarfile\n",
        "import zipfile\n",
        "import shutil\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import random\n",
        "import logging\n",
        "\n",
        "from urllib import request\n",
        "from socket import timeout\n",
        "from urllib.error import HTTPError, URLError\n",
        "\n",
        "from google.colab import drive\n",
        "from google.colab.patches import cv2_imshow\n",
        "from distutils.dir_util import copy_tree\n",
        "\n",
        "# added for HOG\n",
        "from skimage.feature import hog as skimage_feature_hog\n",
        "from skimage import exposure\n",
        "from matplotlib import pyplot as plt\n",
        "from scipy.interpolate import RectBivariateSpline\n",
        "\n",
        "logging.basicConfig(level=logging.DEBUG)\n",
        "\n",
        "mpl_logger = logging.getLogger(\"matplotlib\")\n",
        "mpl_logger.setLevel(logging.WARNING)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "83XvyfDBSLQX",
        "colab_type": "text"
      },
      "source": [
        "Several utils functions\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "05JhwSLCSPyI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def pretty_return_dict_size(my_dict):\n",
        "    output_list = [\"\\n\"]\n",
        "    for k in my_dict.keys():\n",
        "        output_list.append(str(k))\n",
        "        output_list.append(\":\")\n",
        "        output_list.append(str(len(my_dict[k])))\n",
        "        output_list.append(\"\\n\")\n",
        "    return ''.join(output_list)\n",
        "\n",
        "def show_images_from_dict(my_dict, show_index = False):\n",
        "    for k in my_dict.keys():\n",
        "        logging.debug(\"@------------------- Images of \" + str(k) + \" -------------------@\")\n",
        "        index = 0\n",
        "        for img in my_dict[k]:\n",
        "            if show_index:\n",
        "                logging.debug(\"Image index: \" + index)\n",
        "                index+=1\n",
        "            cv2_imshow(img)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BGBkRUir9f7g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "base_path = \"/content/sample_data/CV__Group_assignment\"\n",
        "\n",
        "if not os.path.isdir(base_path):\n",
        "  os.makedirs(base_path)\n",
        "\n",
        "vgg_face_dataset_url = \"http://www.robots.ox.ac.uk/~vgg/data/vgg_face/vgg_face_dataset.tar.gz\"\n",
        "\n",
        "with request.urlopen(vgg_face_dataset_url) as r, open(os.path.join(base_path, \"vgg_face_dataset.tar.gz\"), 'wb') as f:\n",
        "  f.write(r.read())\n",
        "\n",
        "with tarfile.open(os.path.join(base_path, \"vgg_face_dataset.tar.gz\")) as f:\n",
        "  f.extractall(os.path.join(base_path))\n",
        "\n",
        "trained_haarcascade_url = \"https://raw.githubusercontent.com/opencv/opencv/master/data/haarcascades/haarcascade_frontalface_default.xml\"\n",
        "\n",
        "with request.urlopen(trained_haarcascade_url) as r, open(os.path.join(base_path, \"haarcascade_frontalface_default.xml\"), 'wb') as f:\n",
        "    f.write(r.read())\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MOH3DL2jKk3U",
        "colab_type": "text"
      },
      "source": [
        "# New Section"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lQPukQmEpoRC",
        "colab_type": "text"
      },
      "source": [
        "How to generate datasets\n",
        "A = Emma Stone\n",
        "\n",
        "1.   A --> Emma Stone\n",
        "2.   B --> Bradley Cooper\n",
        "3.   C --> Jane Levy\n",
        "4.   D --> Marc Blucas\n",
        "\n",
        "\n",
        "#Idea to get the images dataset\n",
        "## For A and B\n",
        "1. define a seed for A, and a seed for B\n",
        "2. generate a number based on this seed\n",
        "3. using this number, select 50 images from the list of 1000 images provided in the database\n",
        "4. Select 30 out of the 50 images obtained. This constitutes the original dataset (Training and Test) for A and B.\n",
        "\n",
        "## For C and D\n",
        "1. define a seed for C, and a seed for D\n",
        "2. generator a number based on this seed\n",
        "3. using this number, select 20 images from the lsit of 1000 images provided in the database\n",
        "4. Select 10 out of the 20 images obtained. This constutes the original dataset for C and D.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YsyV36nO-SxH",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cFh6oMU6phLs",
        "colab_type": "text"
      },
      "source": [
        "Start from clean sheet\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NAMpaW-Ym0oG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "path_datasets = r\"/content/datasets/\"\n",
        "path_discard = r\"/content/discard/\"\n",
        "path_database = r\"/content/DATABASE/\"\n",
        "\n",
        "try:\n",
        "    shutil.rmtree(path_database)\n",
        "    shutil.rmtree(path_datasets)\n",
        "    shutil.rmtree(path_discard)\n",
        "except:\n",
        "    pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I4Rj5OSS3fUx",
        "colab_type": "text"
      },
      "source": [
        "Create required folders"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hQw39iLK36m1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "file_info = path_database+ r\"info_retrieved.txt\"\n",
        "\n",
        "try: \n",
        "    os.mkdir(path_database)\n",
        "    os.mkdir(path_datasets) \n",
        "    os.mkdir(path_discard)\n",
        "except OSError as error: \n",
        "    logging.error(error) \n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JuEfFZE42YeO",
        "colab_type": "text"
      },
      "source": [
        "Instead of randomly download from web, take images from \"clean\" and controled repository containing the 180 images downloaded once and for all. \n",
        "\n",
        "For that purpose, we use a github public repository created specifically for that matter. The address of this public repository is hardcoded."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TgaFPa0C2YJH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# drive.mount('/content/drive')\n",
        "load_from_local_drive = True\n",
        "if load_from_local_drive:\n",
        "    \n",
        "    !wget https://raw.githubusercontent.com/gherbin/cv_group9_database_replica/master/DATABASE-20200318T142918Z-001.zip\n",
        "\n",
        "    with zipfile.ZipFile(\"DATABASE-20200318T142918Z-001.zip\", 'r') as zip_ref:\n",
        "        zip_ref.extractall()\n",
        "    !rm -r \"DATABASE-20200318T142918Z-001.zip\"\n",
        "\n",
        "path_, dirs_, files = next(os.walk(path_database))\n",
        "if len(files) == 180+1:\n",
        "    logging.info(\"Successful database retrieval\")\n",
        "elif load_from_local_drive:\n",
        "    logging.error(\"Most Likely problem with database retrieval, number of files = \" + str(len(files)))\n",
        "else:\n",
        "    logging.info(\"No database images retrieved yet\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M09QQcnf38xN",
        "colab_type": "text"
      },
      "source": [
        "XXXXXXXXXXXXXXXXXXXXX\n",
        "XXXXXXXXXXXXXXXXXXXXX\n",
        "XXXXXXXXXXXXXXXXXXXXX"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UEGMZ43b4iaa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "personA = \"Emma_Stone.txt\"\n",
        "personC = \"Jane_Levy.txt\"\n",
        "personB = \"Bradley_Cooper.txt\"\n",
        "personD = \"Marc_Blucas.txt\"\n",
        "persons = [personA, personB, personC, personD]\n",
        "datasets_dict = {}\n",
        "images_size = {}\n",
        "images_size[personA] = 60\n",
        "images_size[personB] = 60\n",
        "images_size[personC] = 30\n",
        "images_size[personD] = 30\n",
        "\n",
        "total_images_size = sum(images_size.values())\n",
        "\n",
        "# Dictionary containing the ids of the pictures downloaded from internet\n",
        "vgg_ids = {}\n",
        "for p in persons:\n",
        "    vgg_ids[p] = []\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8qdfeFWLAL6C",
        "colab_type": "text"
      },
      "source": [
        "Populating DB - Run only if database folder is empty"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I5VsGBAJ3inK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "confirmation = False\n",
        "if confirmation:\n",
        "    try:\n",
        "        shutil.rmtree(path_database)\n",
        "    except:\n",
        "        pass \n",
        "    try:\n",
        "        os.mkdir(path_database)\n",
        "    except:\n",
        "        pass \n",
        "\n",
        "    fo = open(file_info, \"w+\")\n",
        "\n",
        "    # images = {}\n",
        "    # images_nominal_indices = {}\n",
        "    for person in persons:\n",
        "        logging.debug(\"Taking care of: \" + str(person))\n",
        "        random.seed(person)\n",
        "        # print(hash(person))\n",
        "        images_ = []\n",
        "        # images_nominal_indices_ = []\n",
        "        prev_index = []\n",
        "\n",
        "\n",
        "        with open(os.path.join(base_path, \"vgg_face_dataset\", \"files\", person), 'r') as f:\n",
        "            lines = f.readlines()       \n",
        "        \n",
        "\n",
        "        while len(images_) < images_size[person]:\n",
        "            index = random.randrange(0, 1000)\n",
        "            logging.debug(\"Index = \" + str(index))\n",
        "            if index in prev_index:\n",
        "                logging.debug(\"Index = \" + str(index) + \" => already there\")\n",
        "                continue\n",
        "            else:\n",
        "                prev_index.append(index)\n",
        "                line = lines[index]\n",
        "                # only curated data\n",
        "                if int(line.split(\" \")[8]) == 1:\n",
        "                    url = line[line.find(\"http://\"): line.find(\".jpg\") + 4]\n",
        "                    logging.debug(\"URL > \\\"\" + str(url))\n",
        "                    try:\n",
        "                        res = request.urlopen(url, timeout = 1)\n",
        "                        img = np.asarray(bytearray(res.read()), dtype=\"uint8\")\n",
        "                        img = cv2.imdecode(img, cv2.IMREAD_COLOR)\n",
        "\n",
        "                        h, w = img.shape[:2]\n",
        "                        cv2_imshow(cv2.resize(img, (w//4, h//4)))\n",
        "                        # images_nominal_indices_.append(index)\n",
        "\n",
        "                        filename = path_database +  str(index) + \"_\" + str(person.split(\".\")[0]) + \".jpg\"\n",
        "\n",
        "                        value = cv2.imwrite(filename, img) \n",
        "                        # logging.debug(\"saved in DB: \" + str(filename))\n",
        "                        images_.append(img)\n",
        "                        fo.write(line)\n",
        "                    except ValueError as e:\n",
        "                            logging.error(\"Value Error >\" + str(e))\n",
        "                    except (HTTPError, URLError) as e:\n",
        "                            logging.error('ERROR RETRIEVING URL >' + str(e))\n",
        "                    except timeout:\n",
        "                            logging.error('socket timed out - URL %s', str(url))\n",
        "                    except cv2.error as e: \n",
        "                            logging.error(\"ERROR WRITING FILE IN DB  >\" + str(e))\n",
        "                    except:\n",
        "                        logging.error(\"Weird exception : \" + str(line))\n",
        "                else:\n",
        "                    logging.debug(\"File not curated => rejected (id = \" + str(index) + \" )\")    \n",
        "                \n",
        "                # images[person] = images_\n",
        "                # images_nominal_indices[person] = images_nominal_indices_\n",
        "\n",
        "    fo.close()\n",
        "else:\n",
        "    logging.warning(\"If you really want to erase and renew the database, please change first the \\\"confirmation\\\" boolean variable, at the beginning of this cell\")\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P3eNIMAdCLVu",
        "colab_type": "text"
      },
      "source": [
        "From local database (not the web, not the drive), building lists of images for all persons"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AdFihucjCO6e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open(file_info, 'r') as f: \n",
        "    lines = f.readlines()\n",
        "\n",
        "assert len(lines)==total_images_size, \"amount of lines in file incompatible\" \n",
        "\n",
        "images = {}\n",
        "\n",
        "for p in persons:\n",
        "    images[p] = []\n",
        "\n",
        "\n",
        "images_index = {}\n",
        "for running_index in range(len(lines)):\n",
        "    if running_index in range(0,images_size[personA]):\n",
        "        p = personA\n",
        "    elif running_index in range(images_size[personA],images_size[personA]+images_size[personB]):\n",
        "        p = personB\n",
        "    elif running_index in range(images_size[personA]+images_size[personB],images_size[personA]+images_size[personB]+images_size[personC]):\n",
        "        p = personC\n",
        "    elif running_index in range(images_size[personA]+images_size[personB]+images_size[personC],total_images_size):\n",
        "        p = personD\n",
        "    ind = str(int(lines[running_index].split(\" \")[0])-1)\n",
        "    vgg_ids[p].append(ind)\n",
        "    filename = ind + \"_\" + str(p.split(\".\")[0]) + \".jpg\"\n",
        "    images[p].append(cv2.imread(path_database+filename, cv2.IMREAD_COLOR))\n",
        "    \n",
        "# print(len(images.keys()))\n",
        "# print(len(images[personA]))\n",
        "# print(len(images[personB]))\n",
        "# print(len(images[personC]))\n",
        "# print(len(images[personD]))\n",
        "\n",
        "# for person in persons:\n",
        "#     counter = 0\n",
        "#     for img in images[person]:\n",
        "#         logging.debug(\"------------------------------------------------------\")\n",
        "#         logging.debug(\"Photo ID = \" + str(counter))\n",
        "#         cv2_imshow(img)\n",
        "#         counter += 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4d-u5s3sHWp-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9v4tuRVO67u1",
        "colab_type": "text"
      },
      "source": [
        "To remove:\n",
        "From DB loaded from the internet, several images are to be removed. The main reasons are:\n",
        "\n",
        "\n",
        "*   Too much make up\n",
        "*   too different hair with usual representation\n",
        "*   really poor image quality\n",
        "*   relevance and error in dataset\n",
        "*   same image as already in dataset\n",
        "*   cropped image\n",
        "\n",
        "considering the tight selection of images to train our model, and the relative global amount of image candidates, it is acceptable to first sort the images according to visual insights. \n",
        "\n",
        "From the initial retrieved images, we then remove the undesired images, that we copy in discard images folder, for tracking purposes. We may want to use them later on to assess the training, for an academical purpose.\n",
        "\n",
        "From the remaing imaging, we can apply the same strategy of selecting the required datasets, that we finally load in separate folders, and save in drive.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZPKRTSB5r8pg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Dictionary of the size required (see section 3)\n",
        "datasets_size = {}\n",
        "datasets_size[personA] = 30\n",
        "datasets_size[personB] = 30\n",
        "datasets_size[personC] = 10\n",
        "datasets_size[personD] = 10\n",
        "\n",
        "\n",
        "# manually remove images that are not relevant or considered not good enough to be part of the dataset\n",
        "to_remove = {}\n",
        "to_remove[personA] = [0,1,4,8,12,13,16,23,28,34,36,42,44,47,48,49,54]\n",
        "to_remove[personB] = [4,7,11,12,13,16,21,22,23,25,26,27,32,36,39,41,46,49,53,55,58]\n",
        "to_remove[personC] = [0,1,4,6,7,11,14,16,17,19,20]\n",
        "to_remove[personD] = [0,3,5,6,8,10,15,16,17,24]\n",
        "\n",
        "# goal is to sort in descending to remove elements from lists without modifying the indexes\n",
        "for p in persons:\n",
        "    to_remove[p].sort(reverse = True)\n",
        "\n",
        "\n",
        "# retrieve images candidates\n",
        "# --------------------------\n",
        "if len(os.listdir(path_datasets) ) == 0 or True:\n",
        "    logging.debug(\"datasets empty - need to retrieve all !\")\n",
        "    # removing images to discard\n",
        "    for person in persons:\n",
        "        for index in to_remove[person]:\n",
        "            img = images[person].pop(index)\n",
        "            logging.debug(\"Removing item \" + str(index) + \" from list \" + str(person))\n",
        "            try:\n",
        "                filename = path_discard +  str(index) +\"_discarded_\" + str(person.split(\".\")[0]) + \".jpg\"\n",
        "                cv2.imwrite(filename, img) \n",
        "            except:\n",
        "                logging.error(\"Error while writing discarded image \" + str(filename))\n",
        "\n",
        "    # randomly select among remaining images\n",
        "    for person in persons:\n",
        "        # build list of indices from remaining images\n",
        "        logging.debug(\"Phase 2 -> random selection for \" + str(person))\n",
        "\n",
        "        images_ = []\n",
        "        indices = []\n",
        "        new_ids = []\n",
        "        # prev_index = []\n",
        "        random.seed(person)\n",
        "\n",
        "        while len(indices) < datasets_size[person]:       \n",
        "            index = random.randrange(0, len(images[person]))\n",
        "            if index in indices:\n",
        "                logging.debug(\"Index among remaining = \" + str(index) + \" => already there\")\n",
        "                continue\n",
        "            else:\n",
        "                # prev_index.append(index)\n",
        "                indices.append(index)\n",
        "\n",
        "        logging.debug(\"Phase 2 -> random selection idx:  \" + str(indices))\n",
        "\n",
        "        for index in indices:\n",
        "            img = images[person][index]\n",
        "            images_.append(img)\n",
        "            filename = path_datasets +  str(vgg_ids[person][index]) + \"_\" + str(person.split(\".\")[0]) + \".jpg\"\n",
        "            logging.debug(\"saved: \" + str(filename))\n",
        "            cv2.imwrite(filename, img) \n",
        "            new_ids.append(vgg_ids[person][index])\n",
        "        images[person] = images_\n",
        "        vgg_ids[person] = new_ids\n",
        "else:\n",
        "    logging.debug(\"folders not empty => can build directly images dictionnary\")\n",
        "\n",
        "# logging.debug(\"Number of images keys=\" + len(images.keys))\n",
        "# logging.debug(\"Number of images values=\" + len(images.values))\n",
        "\n",
        "logging.info(pretty_return_dict_size(images))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XwJjZ1oaHEV9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Save to drive folders\n",
        "to_db_confirmation = False\n",
        "\n",
        "path_drive_DB = r\"/content/drive/My Drive/ComputerVision/DATABASE\"\n",
        "path_drive_Datasets = r\"/content/drive/My Drive/ComputerVision/DATASETS\"\n",
        "\n",
        "\n",
        "\n",
        "# drive folders should be properly set up\n",
        "\n",
        "if to_db_confirmation:\n",
        "    try:\n",
        "        drive.mount('/content/drive')\n",
        "    except:\n",
        "        pass\n",
        "    try:\n",
        "        shutil.rmtree(path_drive_DB)\n",
        "        shutil.rmtree(path_drive_Datasets)\n",
        "    except:\n",
        "        logging.error(\"Error in rmtree\") \n",
        "\n",
        "\n",
        "    try: \n",
        "        os.mkdir(path_drive_DB) \n",
        "        os.mkdir(path_drive_Datasets)\n",
        "    except OSError as error: \n",
        "        logging.error(error) \n",
        "    \n",
        "    logging.debug(\"Saving database in drive : start\")\n",
        "\n",
        "    fromDirectory = path_database\n",
        "    toDirectory = path_drive_DB\n",
        "    copy_tree(fromDirectory, toDirectory)\n",
        "\n",
        "    logging.debug(\"Saving datasets in drive : start\")\n",
        "\n",
        "    fromDirectory = path_datasets\n",
        "    toDirectory = path_drive_Datasets\n",
        "    copy_tree(fromDirectory, toDirectory)\n",
        "\n",
        "    logging.debug(\"Saving: done !\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GAdqr8emBM0J",
        "colab_type": "text"
      },
      "source": [
        "<!-- ## New Section title\n",
        "New section texte.\n",
        "\n",
        "[square]\n",
        "- List item1\n",
        "** * * *   List item2\n",
        "*   List item3\n",
        "    * list item 31\n",
        "    ** list item 32 -->\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5AF-XMBgbGRg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def draw_box(information_file_lines, index, image, person):\n",
        "    \"\"\"\n",
        "    information_file_lines is a list of lines of all images in the database\n",
        "    index is the vgg_idx\n",
        "    image is the image of interest, as retrieved from website\n",
        "    \"\"\"\n",
        "    raise RuntimeError(\"Probably not the smartest way...\")\n",
        "    logging.debug(\"VGG INDEX = \" + str(index))\n",
        "\n",
        "    if person == personA:\n",
        "        lines = information_file_lines[0:59]\n",
        "    elif person == personB:\n",
        "        lines = information_file_lines[59:119]\n",
        "    elif person == personC:\n",
        "        lines = information_file_lines[119:149]\n",
        "    elif person == personD:\n",
        "        lines = information_file_lines[150:179]\n",
        "    \n",
        "    for line in lines:\n",
        "        if int(line.split(\" \")[0]) == index:\n",
        "            h, w = img_.shape[:2]\n",
        "            \n",
        "            left = int(round(float(line.split(\" \")[2])))\n",
        "            # left = w - left\n",
        "            top = int(round(float(line.split(\" \")[3])))\n",
        "            # top = h - top\n",
        "            right = int(round(float(line.split(\" \")[4])))\n",
        "            # right = w - right\n",
        "            bottom = int(round(float(line.split(\" \")[5])))\n",
        "            # bottom = h - bottom\n",
        "            image = cv2.rectangle(image, (left,top), (right,bottom), [0,0,255], 10)\n",
        "\n",
        "        else:\n",
        "            continue\n",
        "\n",
        "    return image"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9HilePrqbDWC",
        "colab_type": "text"
      },
      "source": [
        "recognize faces based on haarCascade\n",
        "\n",
        "tutorial in: [How to detect faces using Haar Cascade](https://www.digitalocean.com/community/tutorials/how-to-detect-and-extract-faces-from-an-image-with-opencv-and-python)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G582IgL-fvi1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "faceCascade = cv2.CascadeClassifier(os.path.join(base_path, \"haarcascade_frontalface_default.xml\"))\n",
        "faces_cropped = {}\n",
        "\n",
        "with open(file_info, 'r') as f: \n",
        "    lines = f.readlines()\n",
        "\n",
        "for person in persons:\n",
        "\n",
        "    faces_cropped[person] = []\n",
        "\n",
        "    for img in images[person]:\n",
        "        img_ = img.copy()\n",
        "        img_gray = cv2.cvtColor(img_, cv2.COLOR_BGR2GRAY)\n",
        "        faces = faceCascade.detectMultiScale(\n",
        "            img_gray,\n",
        "            scaleFactor=1.13,\n",
        "            minNeighbors=10,\n",
        "            minSize=(30, 30),\n",
        "            flags=cv2.CASCADE_SCALE_IMAGE\n",
        "        )\n",
        "        for (x,y,w,h) in faces:\n",
        "            faces_cropped[person].append(img[y:y+h, x:x+w])\n",
        "            cv2.rectangle(img_, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
        "\n",
        "        # h, w = img_.shape[:2]\n",
        "        # draw_box(lines, int(vgg_ids[person][running_index])+1, img_, person)\n",
        "        # cv2_imshow(cv2.resize(img_, (w // 5, h // 5)))\n",
        "logging.info(\"Faces extracted and saved in dictionnary\")\n",
        "logging.info(pretty_return_dict_size(faces_cropped))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zVrtCWGgRFKX",
        "colab_type": "text"
      },
      "source": [
        "From Datasets of images to Trainng set and validation sets;\n",
        "\n",
        "```\n",
        "images[person]\n",
        "```\n",
        "is a dictionary of all the images in the database that are dedicated to a specific person. In order to obtain a :\n",
        "* training set\n",
        "* test set\n",
        "\n",
        "for the person A and B, one can randomly select 20 images for validation sets, and 10 images for test sets. \n",
        "\n",
        "For the sake of reproducibility, the details are logged"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ylnkm0G5SCcR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "training_sets_size = {}\n",
        "training_sets_size[personA] = 20\n",
        "training_sets_size[personB] = 20\n",
        "training_sets_size[personC] = 0\n",
        "training_sets_size[personD] = 0\n",
        "\n",
        "test_sets_size = {}\n",
        "test_sets_size[personA] = 10\n",
        "test_sets_size[personB] = 10\n",
        "test_sets_size[personC] = 10\n",
        "test_sets_size[personD] = 10\n",
        "\n",
        "training_set = {}\n",
        "test_set = {}\n",
        "for person in persons:\n",
        "    image_ = faces_cropped[person]\n",
        "    training_set_ = []\n",
        "    random.seed(person)\n",
        "    init_set = set(range(0, len(image_)))\n",
        "\n",
        "    indices_training = random.sample(init_set, training_sets_size[person])\n",
        "    indices_test = list(init_set - set(indices_training))\n",
        "\n",
        "    training_set[person] = [faces_cropped[person][i] for i in indices_training] \n",
        "    test_set[person] = [faces_cropped[person][i] for i in indices_test]\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZEo9lXETYXWR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# show_images_from_dict(training_set)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-pOSPIokbVSV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# show_images_from_dict(test_set)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j73x0LDhZX9k",
        "colab_type": "text"
      },
      "source": [
        "***AJOUTER UNE VISUALISATION*** "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cPlaQzrHZncq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#  Visualisation des images prises\n",
        "\n",
        "logging.error(\"NO visualisation found !\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fq0JMIBgOhih",
        "colab_type": "text"
      },
      "source": [
        "Feature Descriptor - HOG"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PrC1L60wOadz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "src = faces_cropped[personA][0]\n",
        "cv2_imshow(src)\n",
        "\n",
        "#1 resizing\n",
        "resized_img = cv2.resize(src, (128,128))\n",
        "\n",
        "fd, hog_image = skimage_feature_hog(resized_img, \n",
        "                    orientations=9, \n",
        "                    pixels_per_cell=(8,8), \n",
        "                    cells_per_block=(2, 2), \n",
        "                    block_norm = \"L2\",\n",
        "                    visualize=True, \n",
        "                    transform_sqrt = True,\n",
        "                    multichannel=True)\n",
        "\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 8), sharex=True, sharey=True) \n",
        "\n",
        "ax1.imshow(cv2.cvtColor(resized_img, cv2.COLOR_BGR2RGB))\n",
        "ax1.set_title('Input image') \n",
        "\n",
        "# Rescale histogram for better display \n",
        "hog_image_rescaled = exposure.rescale_intensity(hog_image, in_range=(0, 10)) \n",
        "\n",
        "ax2.imshow(hog_image_rescaled, cmap=plt.cm.gray) \n",
        "ax2.set_title('Histogram of Oriented Gradients - rescaled')\n",
        "print(\"HOG Rescaled: \" + str(hog_image_rescaled.min()) + \" -> \" + str(hog_image_rescaled.max()) )\n",
        "\n",
        "# ax3.imshow(hog_image, cmap=plt.cm.gray) \n",
        "# ax3.set_title('Histogram of Oriented Gradients')\n",
        "# print(\"HOG: \" + str(hog_image.min()) + \" -> \" + str(hog_image.max()) )\n",
        "plt.show()\n",
        "\n",
        "print(hog_image_rescaled.shape)\n",
        "print(fd.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b_b7lnLnmuDk",
        "colab_type": "text"
      },
      "source": [
        "## Detail procedure for HOG\n",
        "# computation of the gradients\n",
        "* each pixel\n",
        "* magnitude and orientation\n",
        "    * unsigned gradient : On the right, we see the raw numbers representing the gradients in the 8×8 cells with one minor difference — the angles are between 0 and 180 degrees instead of 0 to 360 degrees. These are called “unsigned” gradients because a gradient and it’s negative are represented by the same numbers. In other words, a gradient arrow and the one 180 degrees opposite to it are considered the same. But, why not use the 0 – 360 degrees ? Empirically it has been shown that unsigned gradients work better than signed gradients for pedestrian detection. Some implementations of HOG will allow you to specify if you want to use signed gradients\n",
        "    \n",
        "* plot on a cell\n",
        "* size of the cell\n",
        "    * design choice\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "stIpfAn9FIIz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MyHog():\n",
        "    \n",
        "    def __init__(self, img):\n",
        "        self.img = img # image of the size 64x128; 128x128; ... => resized image of the original\n",
        "        self.mag_max, self.orn_max = self.compute_gradients()\n",
        "\n",
        "    def compute_gradients(self):\n",
        "        gx = cv2.Sobel(self.img, cv2.CV_32F, 1, 0, ksize = 1)\n",
        "        gy = cv2.Sobel(self.img, cv2.CV_32F, 0, 1, ksize = 1)\n",
        "\n",
        "        mag, angle = cv2.cartToPolar(gx, gy, angleInDegrees=True)\n",
        "        orn = angle.copy()\n",
        "\n",
        "        logging.debug(\"mag shape :\" + str(mag.shape))\n",
        "        logging.debug(\"orn shape :\" + str(orn.shape))\n",
        "\n",
        "        # constructing matrices of max dimension\n",
        "        mag_max = np.zeros((mag.shape[0], mag.shape[1]))\n",
        "        orn_max = np.zeros((orn.shape[0], orn.shape[1]))\n",
        "        for i in range(mag.shape[0]):\n",
        "            for j in range(mag.shape[1]):\n",
        "                mag_max[i,j] = mag[i,j].max()\n",
        "                idx = np.argmax(mag[i,j])\n",
        "                orn_max[i,j] = orn[i,j,idx] \n",
        "\n",
        "        mag_max = mag_max.T    \n",
        "        orn_max = orn_max.T \n",
        "\n",
        "        return mag_max, orn_max\n",
        "\n",
        "    def get_cells_mag_orn(self, y_start, x_start, cell_h, cell_w):\n",
        "        \"\"\"\n",
        "        returns the cell magnitude, orientation and \"clipped\" orientation \n",
        "        ( where 0 -> 360 is mapped into 0 -> 180)\n",
        "        \"\"\"\n",
        "        cell_mag = np.zeros((cell_h,cell_w))\n",
        "        cell_orn = np.zeros((cell_h,cell_w))\n",
        "        for i in range(cell_h):\n",
        "            for j in range(cell_w):\n",
        "                cell_mag[i,j] = self.mag_max[y_start+i, x_start+j]\n",
        "                cell_orn[i,j] = round(self.orn_max[y_start+i,x_start+j])\n",
        "        \n",
        "        cell_orn_clipped = cell_orn.copy() \n",
        "        cell_orn_clipped = ((cell_orn_clipped) + 90 )% 360\n",
        "        for i in range(cell_h):\n",
        "            for j in range(cell_w):\n",
        "                if 0 <= cell_orn_clipped[i,j] < 180:\n",
        "                    cell_orn_clipped[i,j] = 180 - cell_orn_clipped[i,j]\n",
        "                elif 180 <= cell_orn_clipped[i,j] <=360:\n",
        "                    cell_orn_clipped[i,j] = 180 - cell_orn_clipped[i,j] % 180\n",
        "\n",
        "        return cell_mag, cell_orn, cell_orn_clipped\n",
        "    \n",
        "    def fill_bins_one_pixel(self, mag, orn, bin_list, implementation_type = \"opencv\"):\n",
        "        \"\"\"\n",
        "        # mag: magnitude of the gradient of 1 px\n",
        "        # orn: orientation of the gradient of 1 px\n",
        "        bin_list: reference, list of bins that is incremented\n",
        "        \"\"\"\n",
        "        N_BUCKETS = len(bin_list)\n",
        "        assert N_BUCKETS == 9, \"N_BUCKETS is not 9!!!\"\n",
        "        size_bin = 20.\n",
        "        if orn >= 160:\n",
        "            left_bin = 8\n",
        "            right_bin = 9\n",
        "            left_val= mag * (right_bin * 20 - orn) / 20\n",
        "            right_val = mag * (orn - left_bin * 20) / 20\n",
        "            left_bin = 8\n",
        "            right_bin = 0\n",
        "        else:\n",
        "            left_bin = int(orn / size_bin)\n",
        "            right_bin = (int(orn / size_bin) + 1) % N_BUCKETS\n",
        "            left_val= mag * (right_bin * 20 - orn) / 20\n",
        "            right_val = mag * (orn - left_bin * 20) / 20\n",
        "\n",
        "        assert left_val >= 0, \"leftval = \" + str(left_val) + \", \" + str(\"mag = \") + str(mag) + \" & orn = \" + str(orn)\n",
        "        assert right_val >= 0, \"rightval = \" + str(right_val) + \", \" + str(\"mag = \") + str(mag) + \" & orn = \" + str(orn)\n",
        "\n",
        "        # print(left_val)\n",
        "        # print(right_val)\n",
        "\n",
        "        bin_list[left_bin] += left_val\n",
        "        bin_list[right_bin] += right_val\n",
        "\n",
        "    def compute_hog_bins(self, y_start, x_start, cell_h, cell_w, show=True):\n",
        "        \"\"\"\n",
        "        y_start: y value of the top left pixel\n",
        "        x_start: x value of the top left pixel\n",
        "        cell_h : height of the cell in which HOG is computed\n",
        "        cell_w : width of the cell in which HOG is computed\n",
        "        \"\"\"\n",
        "        cell_img = self.img[y_start:y_start + cell_h, x_start:x_start+cell_w]\n",
        "\n",
        "        if show:\n",
        "            tmp = self.img.copy()\n",
        "            cv2.rectangle(tmp, (x_start, y_start), (x_start+cell_w, y_start+cell_h), (0,255,0))\n",
        "            cv2_imshow(tmp)\n",
        "\n",
        "        # if created_img is None:\n",
        "        #     mat = np.ones((cell_h, cell_w))*125\n",
        "        #     create_img = np.dstack((mat, mat, mat))\n",
        "\n",
        "        # construction of the magnitude and orn matrices\n",
        "        cell_mag, cell_orn, cell_orn_clipped = self.get_cells_mag_orn(y_start, x_start, cell_h, cell_w)\n",
        "\n",
        "        number_of_bins = 9\n",
        "        bin_list = np.zeros(number_of_bins)\n",
        "        for i in range(cell_h):\n",
        "            for j in range(cell_w):\n",
        "                # m = round(mag_normalized[y_start+j,x_start+i].max())\n",
        "                m = cell_mag[i,j]\n",
        "                d = cell_orn_clipped[i,j]\n",
        "                # print(\"m,d =\" + str((m,d)))\n",
        "                self.fill_bins_one_pixel(m,d,bin_list)\n",
        "        \n",
        "        logging.debug(\"Bins computed:\" + str(bin_list))\n",
        "        n = np.linalg.norm(bin_list)\n",
        "        bin_norms = bin_list/n\n",
        "        logging.debug(\"Bins normalized:\" + str(bin_norms))\n",
        "             \n",
        "        if show:\n",
        "            fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(16, 8), sharex=True, sharey=True) \n",
        "\n",
        "        # cell_orn_matrix_clipped.astype(np.uint8)\n",
        "        # ax1 => just to visually represent the arrows\n",
        "            for i in range(cell_h):\n",
        "                for j in range(cell_w):\n",
        "                    # r = round(mag_normalized[y_start+j,x_start+i].max())/(mag_normalized.max() - mag_normalized.min())\n",
        "                    r = cell_mag[i,j] / (cell_mag.max() - cell_mag.min())\n",
        "                    angle_ = cell_orn[i,j]\n",
        "                    \n",
        "                    c = cell_orn_clipped[i,j]\n",
        "\n",
        "                    mag_value = round(cell_mag[i,j])\n",
        "                    \n",
        "                    ax1.arrow(i, j, r*np.cos(np.deg2rad(angle_)), r*np.sin(np.deg2rad(angle_)), head_width=0.15, head_length=0.15, fc='b', ec='b')\n",
        "                    ax2.text(i, j, str(c), fontsize=10,va='center', ha='center')\n",
        "                    ax3.text(i, j, str(mag_value), fontsize=10,va='center', ha='center')\n",
        "\n",
        "            ax1.imshow(cv2.cvtColor(cell_img, cv2.COLOR_BGR2RGB))\n",
        "            ax1.set_title('Input image') \n",
        "\n",
        "            ax2.matshow(cell_orn_clipped, alpha=0)\n",
        "            ax2.set_title('Orientation values')\n",
        "\n",
        "            ax3.set_title('Magnitude values')\n",
        "            intersection_matrix = np.ones(cell_mag.shape)\n",
        "            # for i in range(cell_h):\n",
        "            #     for j in range(cell_w):\n",
        "            #         mag_value = round(cell_mag_matrix[i,j])\n",
        "            #         intersection_matrix[i,j] = mag_value+1\n",
        "            ax3.matshow(cell_mag, alpha = 0)\n",
        "\n",
        "\n",
        "            fig, ax = plt.subplots(1,1)\n",
        "            ax.bar(range(9), bin_norms)\n",
        "            ax.set_title(\"Histogram computer - implemeted method\")\n",
        "\n",
        "            \n",
        "            plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hQiWXisWJItc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "logging.debug(\"Toy Example\")\n",
        "\n",
        "img = faces_cropped[personA][0].copy()\n",
        "resized_img = cv2.resize(img, (128,128))\n",
        "\n",
        "# resized_img = resized_img[0:64, 0:128]\n",
        "# resized_img = cv2.imread(\"cropped.png\")\n",
        "\n",
        "cv2_imshow(img)\n",
        "cv2_imshow(resized_img)\n",
        "\n",
        "cell_w = 8\n",
        "cell_h = 8\n",
        "\n",
        "block_w = 16\n",
        "block_h = 16\n",
        "\n",
        "y_start = 4 * cell_h - 1\n",
        "x_start = 6 * cell_w - 1\n",
        "\n",
        "# y_start = 0\n",
        "# x_start = 0\n",
        "\n",
        "# cell_img = resized_img[y_start:y_start + cell_h, x_start:x_start+cell_w]\n",
        "\n",
        "# tmp = resized_img.copy()\n",
        "# cv2.rectangle(tmp, (x_start, y_start), (x_start+cell_w, y_start+cell_h), (0,255,0))\n",
        "# cv2_imshow(tmp)\n",
        "\n",
        "myhog = MyHog(resized_img)\n",
        "myhog.compute_hog_bins(y_start, x_start, cell_h, cell_w, show=True)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xza7xfE4rHBY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HH2aQX3NRd4P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "cell_img=resized_img[y_start:y_start + cell_h, x_start:x_start+cell_w]\n",
        "fd, hog_image = skimage_feature_hog(cell_img, \n",
        "                    orientations=9, \n",
        "                    pixels_per_cell=(8,8), \n",
        "                    cells_per_block=(1, 1), \n",
        "                    block_norm = \"L2\",\n",
        "                    visualize=True, \n",
        "                    transform_sqrt = True,\n",
        "                    multichannel=True)\n",
        "print(fd)\n",
        "plt.figure()\n",
        "plt.bar( range(9), fd)\n",
        "\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(8, 8), sharex=True, sharey=True) \n",
        "# hog_cropped = hog_image[y_start:y_start + cell_h, x_start:x_start+cell_w]\n",
        "\n",
        "ax1.imshow(cv2.cvtColor(cell_img, cv2.COLOR_BGR2RGB))\n",
        "ax1.set_title('Input image') \n",
        "\n",
        "# Rescale histogram for better display \n",
        "hog_image_rescaled = exposure.rescale_intensity(hog_image, in_range=(0, 10)) \n",
        "ax2.imshow(hog_image_rescaled, cmap=plt.cm.gray) \n",
        "ax2.set_title('Histogram of Oriented Gradients - rescaled')\n",
        "# print(hog_image_rescaled)\n",
        "plt.show()\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a_muPjY3ScMI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test = False\n",
        "if test:\n",
        "    img = cv2.imread(\"hog-preprocessing.jpg\")\n",
        "    cv2_imshow(img)\n",
        "    y=95\n",
        "    x=200\n",
        "    h = 172\n",
        "    w = 84\n",
        "\n",
        "    crop_img = img[y:y+h, x:x+w]\n",
        "    crop_img = cv2.resize(crop_img, (64,128))\n",
        "    cv2_imshow(crop_img)\n",
        "    cell_h = 8\n",
        "    cell_w = 8\n",
        "    y_ = 10\n",
        "    x_ = 24\n",
        "    crop_img_cell = crop_img[y_:y_+k, x_:x_+k]\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GuWXw_8xuTWb",
        "colab_type": "text"
      },
      "source": [
        "histogram construction is based on the gradient computed - both magnitude and orientation (as defined)\n",
        "\n",
        "- the bin is selected according to the orientation (direction) of the gradient; \n",
        "- the value that goes in the bin is based on the magnitude\n",
        "\n",
        "For instance on the toy example:\n",
        "first pixel has:\n",
        "* mag = 6; orn = 45°. So, the vote of this pixel goes for 75% in the bin of 40°, and 25% in the bin of 60°, as closer to 40°. As a result, we add 4.5 to bin nb 3, and 1.5 to bin nb 4. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RFSq-4SXoDZi",
        "colab_type": "text"
      },
      "source": [
        "Computations of the bins - pile up on our cell"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "smMfwDmsZAcL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if test:\n",
        "    print(bin_list)\n",
        "    toy_dir = np.array([[80,36,5,10,0,64,90,73],\n",
        "                        [37,9,9,179,78,27,169,166],\n",
        "                        [87,136,173,39,102,163,152,176],\n",
        "                        [76,13,1,168,159,22,125,143],\n",
        "                        [120,70,14,150,145,144,145,143],\n",
        "                        [58,86,119,98,100,101,133,113],\n",
        "                        [30,65,157,75,78,165,145,124],\n",
        "                        [11,170,91,4,110,17,133,110]])\n",
        "    toy_mag = np.array([[2,3,4,4,3,4,2,2],\n",
        "                        [5,11,17,13,7,9,3,4],\n",
        "                        [11,21,23,27,22,17,4,6],\n",
        "                        [23,99,165,135,85,32,26,2],\n",
        "                        [91,155,133,136,144,152,57,28],\n",
        "                        [98,196,76,38,26,60,170,51],\n",
        "                        [165,60,60,27,77,85,43,136],\n",
        "                        [71,13,34,23,108,27,48,110]])\n",
        "    # print(mag_normalized.shape)\n",
        "    n_bins = 9\n",
        "    bin_list = np.zeros(n_bins)\n",
        "    for i in range(8):\n",
        "        for j in range(8):\n",
        "            m = round(mag_normalized[10+j,24+i].max())\n",
        "            d = orn_matrix_clipped[i,j]\n",
        "            # print(\"m,d =\" + str((m,d)))\n",
        "            fill_bins_one_pixel(m,d,bin_list)\n",
        "\n",
        "    print(bin_list)\n",
        "\n",
        "    n = np.linalg.norm(bin_list)\n",
        "    bin_norms = bin_list/n\n",
        "    plt.bar( range(9), bin_norms)\n",
        "    plt.show()\n",
        "    print(bin_norms)\n",
        "\n",
        "    fd, hog_image = hog(crop_img_2, \n",
        "                        orientations=9, \n",
        "                        pixels_per_cell=(8,8), \n",
        "                        cells_per_block=(1, 1), \n",
        "                        block_norm = \"L2\",\n",
        "                        visualize=True, \n",
        "                        transform_sqrt = False,\n",
        "                        multichannel=True)\n",
        "    # print(fd)\n",
        "    plt.bar( range(9), fd)\n",
        "\n",
        "\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(8, 8), sharex=True, sharey=True) \n",
        "\n",
        "    ax1.imshow(cv2.cvtColor(crop_img_cell, cv2.COLOR_BGR2RGB))\n",
        "    ax1.set_title('Input image') \n",
        "\n",
        "    # Rescale histogram for better display \n",
        "    hog_image_rescaled = exposure.rescale_intensity(hog_image, in_range=(0, 10)) \n",
        "\n",
        "    ax2.imshow(hog_image_rescaled, cmap=plt.cm.gray) \n",
        "    ax2.set_title('Histogram of Oriented Gradients - rescaled')\n",
        "\n",
        "    # print(hog_image_rescaled)\n",
        "    plt.show()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "erQwBkq35RQ-",
        "colab_type": "text"
      },
      "source": [
        "Gradient is sensitive to light variation. We do not want that => normalization the histogram so that they are not affected by lighting variations\n",
        "\n",
        "> $v = [x, y, z]$, \n",
        "\n",
        "> $\\|v\\| = \\sqrt{x^2 + y^2 + z^2}$\n",
        "\n",
        "> $v_{normalized} =  \\frac{v}{\\|v\\|} = \\Big[ \\frac{x}{\\|v\\|}, \\frac{y}{\\|v\\|}, \\frac{z}{\\|v\\|} \\Big]$\n",
        "\n",
        "The normalization has a stronger helpful effect if applied on a larger scale than the cell only. AS a result, an often good choice is to apply on 4 cells, or a 16x16 block.  Four histograms are taken into account. \n",
        "* one cell (8 x 8) histogram is a vector of size 9 (as 9 bins)\n",
        "* one block (16 x 16) histogram is the concatenation of the 4 histograms, each representing one cell of the block, hence represented by a (36 x 1) vector.\n",
        "* the final HOG feature vector is based on the concatenation of all blocks.\n",
        "If the image as a width of size w*8 pixels, and a height of h*8 pixels, \n",
        "the image dimension in (8*h x 8*w). In such an image, they are :\n",
        "    -   h cells vertically, and w cells horizontally,\n",
        "    -  (h-1) blocks vertically and (w-1) blocks horizontally.\n",
        "\n",
        "For an image of 128 x 128, we then have:\n",
        "* w = 128/8 = 16\n",
        "* h = 128/8 = 16\n",
        "* #cells = 16x16\n",
        "* #Blocks = 15 * 15 = 225 blocks\n",
        "\n",
        "each block having a representative vector (36x1), the resulting vector has dimension (225*36 x 1), or (8100x1)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eXffH7oz8GEJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tfMDFGqsZvzC",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0dyjE33Nno4j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def toy_test_hog(toy_dir=None, toy_mag=None, img=None):\n",
        "    test_tmp = True\n",
        "    if test_tmp:\n",
        "        N_BUCKETS = 9\n",
        "        bin_list = np.zeros(N_BUCKETS)\n",
        "\n",
        "        toy_dir = np.array([[45,45,45,45,0,0,0,0],\n",
        "                            [45,45,45,45,0,0,0,0],\n",
        "                            [45,45,45,45,0,0,0,0],\n",
        "                            [45,45,45,45,0,0,0,0],\n",
        "                            [0,0,0,0,0,0,0,0],\n",
        "                            [0,0,0,0,0,0,0,0],\n",
        "                            [0,0,0,0,0,0,0,0],\n",
        "                            [0,0,0,0,0,0,0,0]])    \n",
        "        # toy_dir = np.array([[0,0,0,0,0,0,0,0],\n",
        "        #                     [0,0,0,0,0,0,0,0],\n",
        "        #                     [0,0,0,0,0,0,0,0],\n",
        "        #                     [0,0,0,0,0,0,0,0],\n",
        "        #                     [0,0,0,0,0,0,0,0],\n",
        "        #                     [0,0,0,0,0,0,0,0],\n",
        "        #                     [0,0,0,0,0,0,0,0],\n",
        "        #                     [0,0,0,0,0,0,0,0]])\n",
        "        toy_dir = np.array([[0,0,0,0,0,0,0,0],\n",
        "                            [0,0,0,0,0,0,0,0],\n",
        "                            [0,0,0,0,0,0,0,0],\n",
        "                            [0,0,0,0,0,0,0,0],\n",
        "                            [0,0,0,0,70,70,70,70],\n",
        "                            [0,0,0,0,70,70,70,70],\n",
        "                            [0,0,0,0,70,70,70,70],\n",
        "                            [0,0,0,0,0,0,0,0]])\n",
        "        toy_mag = np.array([[1,1,1,1,1,1,1,1],\n",
        "                            [1,1,1,1,1,1,1,1],\n",
        "                            [1,1,1,1,1,1,1,1],\n",
        "                            [1,1,1,1,1,1,1,1],\n",
        "                            [1,1,1,1,1,1,1,1],\n",
        "                            [1,1,1,1,1,1,1,1],\n",
        "                            [1,1,1,1,1,1,1,1],\n",
        "                            [1,1,1,1,1,1,1,1]])\n",
        "        # print(mag_normalized.shape)\n",
        "\n",
        "\n",
        "        plot_hog_intermediate(created_img, toy_mag, toy_dir, y_start=0, x_start=0, cell_h=8, cell_w=8)\n",
        "\n",
        "        plt.figure()\n",
        "        for i in range(8):\n",
        "            for j in range(8):\n",
        "                m = toy_mag[i,j]\n",
        "                d = toy_dir[i,j]\n",
        "                # print(str((m,d)))\n",
        "                fill_bins_one_pixel(m,d,bin_list)\n",
        "\n",
        "        print(bin_list)\n",
        "\n",
        "        n = np.linalg.norm(bin_list)\n",
        "        bin_norms = bin_list/n\n",
        "        plt.bar( range(9), bin_norms)\n",
        "        plt.show()\n",
        "        print(bin_norms)\n",
        "\n",
        "    ############################################################\n",
        "\n",
        "        fd, hog_image = hog(cell_img, \n",
        "                        orientations=9, \n",
        "                        pixels_per_cell=(8,8), \n",
        "                        cells_per_block=(1, 1), \n",
        "                        block_norm = \"L2\",\n",
        "                        visualize=True, \n",
        "                        transform_sqrt = False,\n",
        "                        multichannel=True)\n",
        "        print(fd)\n",
        "        plt.figure()\n",
        "        plt.bar( range(9), fd)\n",
        "\n",
        "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(8, 8), sharex=True, sharey=True) \n",
        "        # hog_cropped = hog_image[y_start:y_start + cell_h, x_start:x_start+cell_w]\n",
        "\n",
        "        ax1.imshow(cv2.cvtColor(cell_img, cv2.COLOR_BGR2RGB))\n",
        "        ax1.set_title('Input image') \n",
        "\n",
        "        # Rescale histogram for better display \n",
        "        hog_image_rescaled = exposure.rescale_intensity(hog_image, in_range=(0, 10)) \n",
        "        ax2.imshow(hog_image_rescaled, cmap=plt.cm.gray) \n",
        "        ax2.set_title('Histogram of Oriented Gradients - rescaled')\n",
        "        # print(hog_image_rescaled)\n",
        "        plt.show()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4RE2qpxGt9us",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_image(height, width, special=None):\n",
        "\n",
        "    mat0 = np.ones((height, width), dtype=np.uint8)*255\n",
        "    mat1 = np.ones((height, width), dtype=np.uint8)*255\n",
        "    mat2 = np.ones((height, width), dtype=np.uint8)*255\n",
        "\n",
        "    if special == \"center_black\":\n",
        "        mat0[height//2-1:height//2+1, width//2-1 : width//2+1 ] = 0\n",
        "        mat1[height//2-1:height//2+1, width//2-1 : width//2+1 ] = 0\n",
        "        mat2[height//2-1:height//2+1, width//2-1 : width//2+1 ] = 0\n",
        "    elif special == \"center_gray\":\n",
        "        mat0[height//2-3:height//2+3, width//2-3 : width//2+3 ] = 125\n",
        "        mat1[height//2-3:height//2+3, width//2-3 : width//2+3 ] = 125\n",
        "        mat2[height//2-3:height//2+3, width//2-3 : width//2+3 ] = 125\n",
        "        mat0[height//2-1:height//2+1, width//2-1 : width//2+1 ] = 0\n",
        "        mat1[height//2-1:height//2+1, width//2-1 : width//2+1 ] = 0\n",
        "        mat2[height//2-1:height//2+1, width//2-1 : width//2+1 ] = 0\n",
        "    elif special == \"half\":\n",
        "        # mat0[height//2-3:height//2, width//2-3 : width//2 ] = 125\n",
        "        # mat1[height//2-3:height//2, width//2-3 : width//2 ] = 125\n",
        "        # mat2[height//2-3:height//2, width//2-3 : width//2 ] = 125\n",
        "        mat0[height//2:height, 0 : width ] = 0\n",
        "        mat1[height//2:height, 0 : width ] = 0\n",
        "        mat2[height//2:height, 0 : width ] = 0\n",
        "\n",
        "    elif special == \"diagonal_desc\":\n",
        "        for i in range(height):\n",
        "            for j in range(i,width):\n",
        "                mat0[i,j] = mat1[i,j] = mat2[i,j] = 0\n",
        "    elif special == \"diagonal_asc\":\n",
        "        for i in range(height):\n",
        "            for j in range(width-i-1,width):\n",
        "                mat0[i,j] = mat1[i,j] = mat2[i,j] = 0\n",
        "    elif special == \"diagonal_asc\":\n",
        "        for i in range(height):\n",
        "            for j in range(width-i-1,width):\n",
        "                mat0[i,j] = mat1[i,j] = mat2[i,j] = 0\n",
        "    elif special == \"up_60\":\n",
        "        mat0 = np.ones((height, width), dtype=np.uint8)*0\n",
        "        mat1 = np.ones((height, width), dtype=np.uint8)*0\n",
        "        mat2 = np.ones((height, width), dtype=np.uint8)*0\n",
        "        mat0[0,6:7] = mat1[0,6:7] = mat2[0,6:7] = 255\n",
        "        mat0[1,7] = mat1[1,7] = mat2[1,7] = 135\n",
        "    elif special == \"down_60\":\n",
        "        mat0 = np.ones((height, width), dtype=np.uint8)*255\n",
        "        mat1 = np.ones((height, width), dtype=np.uint8)*255\n",
        "        mat2 = np.ones((height, width), dtype=np.uint8)*255\n",
        "        mat0[0,6:7] = mat1[0,6:7] = mat2[0,6:7] = 0\n",
        "        mat0[1,7] = mat1[1,7] = mat2[1,7] = 125\n",
        "    elif special == \"up_50\":\n",
        "        mat0 = np.ones((height, width), dtype=np.uint8)*0\n",
        "        mat1 = np.ones((height, width), dtype=np.uint8)*0\n",
        "        mat2 = np.ones((height, width), dtype=np.uint8)*0\n",
        "        mat0[0,6:7] = mat1[0,6:7] = mat2[0,6:7] = 255\n",
        "        mat0[1,7] = mat1[1,7] = mat2[1,7] = 210\n",
        "    elif special == \"???\":\n",
        "        mat0 = np.ones((height, width), dtype=np.uint8)*0\n",
        "        mat1 = np.ones((height, width), dtype=np.uint8)*0\n",
        "        mat2 = np.ones((height, width), dtype=np.uint8)*0\n",
        "        mat0[0,6:7] = mat1[0,6:7] = mat2[0,6:7] = 30\n",
        "        mat0[1,7] = mat1[1,7] = mat2[1,7] = 125\n",
        "\n",
        "    image = np.dstack((mat0, mat1, mat2))\n",
        "    return image"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zzqs9qmavSyr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "created_img = create_image(8,8, \"???\")\n",
        "fig, ax1 = plt.subplots(1, 1, figsize=(16, 8), sharex=True, sharey=True)\n",
        "ax1.matshow(created_img)\n",
        "\n",
        "# mag_max, orn_max = compute_gradients(created_img)\n",
        "\n",
        "cell_h = 8\n",
        "cell_w = 8\n",
        "k = cell_h\n",
        "\n",
        "y_start = 0\n",
        "x_start = 0\n",
        "\n",
        "toyhog = MyHog(created_img)\n",
        "toyhog.compute_hog_bins(y_start, x_start, cell_h, cell_w, show=True)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ppCTsJiyHZY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fd, hog_image = skimage_feature_hog(created_img, \n",
        "                orientations=9, \n",
        "                pixels_per_cell=(8,8), \n",
        "                cells_per_block=(1, 1), \n",
        "                block_norm = \"L2\",\n",
        "                visualize=True, \n",
        "                transform_sqrt = False,\n",
        "                multichannel=True)\n",
        "print(fd)\n",
        "plt.figure()\n",
        "plt.bar( range(9), fd)\n",
        "\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(8, 8), sharex=True, sharey=True) \n",
        "# hog_cropped = hog_image[y_start:y_start + cell_h, x_start:x_start+cell_w]\n",
        "\n",
        "ax1.imshow(cv2.cvtColor(created_img, cv2.COLOR_BGR2RGB))\n",
        "ax1.set_title('Input image') \n",
        "\n",
        "# Rescale histogram for better display \n",
        "hog_image_rescaled = exposure.rescale_intensity(hog_image, in_range=(0, 10)) \n",
        "ax2.imshow(hog_image_rescaled, cmap=plt.cm.gray) \n",
        "ax2.set_title('Histogram of Oriented Gradients - rescaled')\n",
        "# print(hog_image_rescaled)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NtYBaU-bzkio",
        "colab_type": "text"
      },
      "source": [
        "Still to do : \n",
        " These [hog] parameters were obtained by experimentation and examining the accuracy of the classifier — you should expect to do this as well whenever you use the HOG descriptor. Running experiments and tuning the HOG parameters based on these parameters is a critical component in obtaining an accurate classifier."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "geHORWA5mMdM",
        "colab_type": "text"
      },
      "source": [
        "2ND - detecting an object of interest in a new image \n",
        "\n",
        "idea: find cropped face in the global idea"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rodK9Dor7oeR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tVlqSL-Y2Y3F",
        "colab_type": "text"
      },
      "source": [
        "Compute local descriptor for all faces_cropped of the training dataset => hog_training{}:\n",
        "\n",
        "=> `hog_training[person_i] = [(fd_0, hog_image_0), ..., (fd_n, hog_image_n)]`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "guoG2jWm4Dv3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "hog_training = {}\n",
        "\n",
        "for person in persons:\n",
        "    hog_training[person] = []\n",
        "    for src_img in training_set[person]:\n",
        "        resized_img = cv2.resize(src_img, (64,64))\n",
        "        fd, hog_image = skimage_feature_hog(resized_img, \n",
        "                                            orientations=9, \n",
        "                                            pixels_per_cell=(8,8), \n",
        "                                            cells_per_block=(2, 2), \n",
        "                                            block_norm = \"L2\",\n",
        "                                            visualize=True, \n",
        "                                            transform_sqrt = True,\n",
        "                                            multichannel=True)\n",
        "        hog_training[person].append((fd, hog_image, resized_img))\n",
        "\n",
        "logging.debug(pretty_return_dict_size(hog_training))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "55hZpBQoGLdD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_local_crop(src_img, center_pixel, crop_shape, show = False):\n",
        "    \"\"\"\n",
        "    src_img:\n",
        "    center_pixel:\n",
        "    crop_shape:\n",
        "    \"\"\"\n",
        "    crop_height =  crop_shape[0]\n",
        "    crop_width = crop_shape[1]\n",
        "    top_= center_pixel[0] - crop_height // 2\n",
        "    bottom_ = center_pixel[0] + crop_height // 2\n",
        "    left_ = center_pixel[1] - crop_width // 2  \n",
        "    right_ = center_pixel[1] + crop_width // 2\n",
        "    if len(src_img.shape)> 2:\n",
        "        crop = src_img[ top_ : bottom_, left_:right_, :]\n",
        "    else:\n",
        "        crop = src_img[ top_ : bottom_, left_:right_]\n",
        "\n",
        "    if show:\n",
        "        cv2_imshow(crop)\n",
        "    return crop\n",
        "\n",
        "def match_hog(src_img, hog_desc, original_face_shape, step = 16, show = False):\n",
        "    \"\"\"\n",
        "    src_img : image to analyse\n",
        "    hog_desc: (fd, hog_image) of the corresponding faces_cropped\n",
        "    original_face_shape: shape of the face used for hog_desc computation\n",
        "    \"\"\"\n",
        "    height = src_img.shape[0] # height of the image to analyze\n",
        "    width  = src_img.shape[1] # width of the image to analyze\n",
        "    \n",
        "    height_face = original_face_shape[0]\n",
        "    width_face = original_face_shape[1]\n",
        "\n",
        "    res_shape = (src_img.shape[0], src_img.shape[1])\n",
        "    res = np.ones(res_shape)*-1\n",
        "\n",
        "    if show:\n",
        "        tmp_image = src_img.copy()\n",
        "        cv2.rectangle(tmp_image, (width_face//2, height_face//2), (width - width_face//2, height - height_face//2), (0, 255, 0))\n",
        "        cv2_imshow(tmp_image)\n",
        "\n",
        "    running_h_idx = range(height_face //2, height - height_face//2+1, step)\n",
        "    running_w_idx = range(width_face //2, width - width_face//2+1, step)\n",
        "\n",
        "    for h_idx in running_h_idx:\n",
        "        for w_idx in running_w_idx:\n",
        "            local_crop = get_local_crop(src_img, (h_idx, w_idx), original_face_shape, False)\n",
        "            local_resized_img = cv2.resize(local_crop, (64,64))\n",
        "            local_fd = skimage_feature_hog(local_resized_img, \n",
        "                                            orientations=9, \n",
        "                                            pixels_per_cell=(8,8), \n",
        "                                            cells_per_block=(2, 2), \n",
        "                                            block_norm = \"L2\",\n",
        "                                            visualize=False, \n",
        "                                            transform_sqrt = True,\n",
        "                                            multichannel=True)\n",
        "            res[h_idx,w_idx]= np.linalg.norm(local_fd-hog_desc[0])\n",
        "            # logging.debug(res[h_idx, w_idx])\n",
        "            if show:\n",
        "                cv2_imshow(local_resized_img)\n",
        "    \n",
        "\n",
        "    # find along height the indexes that are not empty\n",
        "    # find along width the indexes that are not empty\n",
        "    # call function RectBivariateSpline(y, x, Z), from scipy.interpolate.RectBivariateSpline(x, y, z, bbox=[None, None, None, None], kx=3, ky=3, s=0)[source]\n",
        "    return res\n",
        "\n",
        "def fill_matrix_min_neighbours(matrx, size_to_consider = 16, margin = 0):\n",
        "    \"\"\"\n",
        "    fill the gaps in the  computation by taking the min values from closest neighbours\n",
        "    that were computed.\n",
        "    \"\"\"\n",
        "    l = np.argwhere(matrx != -1)\n",
        "    res = matrx.copy()\n",
        "\n",
        "    if len(l)<2:\n",
        "        idx_to_change = res == -1\n",
        "        logging.warning(\"len < 2 --> len(idx_to_change) = \" + str(len(idx_to_change)))\n",
        "        res[idx_to_change] = res.max() + margin\n",
        "        return res\n",
        "\n",
        "    top_left_corner = l[0]\n",
        "    bottom_right_corner = l[-1]\n",
        "\n",
        "    for i in range(top_left_corner[0], bottom_right_corner[0]+size_to_consider//2):\n",
        "        for j in range(top_left_corner[1],bottom_right_corner[1]+size_to_consider//2):\n",
        "            if matrx[i,j] == -1:\n",
        "                local_roi = get_local_crop(matrx, (i,j),(size_to_consider,size_to_consider))\n",
        "                cand = local_roi[ local_roi != -1 ]\n",
        "                res[i,j] = cand.min()\n",
        "    idx_to_change = res == -1\n",
        "    res[idx_to_change] = res.max() + margin\n",
        "    return res"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KUtgffa1HZey",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "idx_of_interest = 5\n",
        "\"\"\"\n",
        " regarding the hog template \n",
        "\"\"\"\n",
        "object_of_interest = training_set[personA][idx_of_interest]\n",
        "image_of_interest = training_set[personA][idx_of_interest]\n",
        "template_hog = hog_training[personA][idx_of_interest]\n",
        "original_face_shape = training_set[personA][idx_of_interest].shape\n",
        "\n",
        "local_crop = get_local_crop(image_of_interest, (original_face_shape[0]//2, original_face_shape[1]//2), original_face_shape, False)\n",
        "local_crop_resized = cv2.resize(local_crop, (64,64))\n",
        "\n",
        "\n",
        "\n",
        "logging.debug(\"object to find: \" +  str(object_of_interest.shape))\n",
        "cv2_imshow(object_of_interest)\n",
        "logging.debug(\"based image to analyze: \" +  str(image_of_interest.shape))\n",
        "cv2_imshow(image_of_interest)\n",
        "logging.debug(\"local_crop to analyze: \" +  str(local_crop.shape))\n",
        "cv2_imshow(local_crop)\n",
        "logging.debug(\"local_crop_resized: \" +  str(local_crop_resized.shape))\n",
        "cv2_imshow(local_crop_resized)\n",
        "\n",
        "#####################################################################################\n",
        "\n",
        "local_fd = skimage_feature_hog(local_crop_resized, \n",
        "                               orientations=9, \n",
        "                               pixels_per_cell=(8,8), \n",
        "                               cells_per_block=(2, 2), \n",
        "                               block_norm = \"L2\",\n",
        "                               visualize=False, \n",
        "                               transform_sqrt = True,\n",
        "                               multichannel=True)\n",
        "            \n",
        "logging.debug(\"Template hog: \" + str(template_hog[0]))\n",
        "logging.debug(\"Computed hog: \" + str(local_fd))\n",
        "\n",
        "res = np.linalg.norm(local_fd-template_hog[0])\n",
        "\n",
        "logging.debug(\"Results: \" + str(res))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qPLRtjqEFjdy",
        "colab_type": "text"
      },
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L06xUrOubbu_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "idx_of_interest = 5\n",
        "\"\"\"\n",
        " regarding the hog template \n",
        "\"\"\"\n",
        "object_of_interest  = training_set[personA][idx_of_interest]\n",
        "template_hog        = hog_training[personA][idx_of_interest]\n",
        "original_face_shape = training_set[personA][idx_of_interest].shape\n",
        "logging.debug(\"object to find:\")\n",
        "cv2_imshow(object_of_interest)\n",
        "\n",
        "counter = 0\n",
        "upper_bound = len(images[personA])\n",
        "for img in images[personA]:\n",
        "    if counter >= upper_bound:\n",
        "        break;\n",
        "    else:\n",
        "        counter+=1\n",
        "\n",
        "    image_of_interest = img\n",
        "    # image_of_interest = faces_cropped[personA][idx_of_interest]\n",
        "\n",
        "    # logging.debug(\"Object to find - shape = \" + str(original_face_shape)) \n",
        "\n",
        "    # logging.debug(\"Image of interest:\")\n",
        "    # cv2_imshow(image_of_interest)\n",
        "\n",
        "    step=16\n",
        "    res = match_hog(image_of_interest, hog_training[personA][idx_of_interest], original_face_shape, step)\n",
        "    new_res = fill_matrix_min_neighbours(res, step)\n",
        "\n",
        "    logging.debug(\"worst match hog results: \" + str(new_res.max()))\n",
        "    logging.debug(\"best match hog results: \" + str(new_res.min()))\n",
        "\n",
        "    b = new_res.copy()\n",
        "    bmax, bmin = b.max(), b.min()\n",
        "    if bmax == bmin and bmax == 0:\n",
        "        logging.info(\"Perfect match!\")\n",
        "    # b = (b - bmin)/(bmax - bmin)\n",
        "    b = (b - bmin)/(bmax)\n",
        "\n",
        "    logging.debug(\"worst match hog results normalized [expexted 1]: \" + str(b.max()))\n",
        "    logging.debug(\"best match hog results [expected 0]: \" + str(b.min()))\n",
        "\n",
        "\n",
        "\n",
        "    fig, (ax1, ax2,ax3) = plt.subplots(1,3 , figsize=(16, 8), sharex=True, sharey=True) \n",
        "\n",
        "    ax1.imshow(cv2.cvtColor(image_of_interest,cv2.COLOR_BGR2RGB))\n",
        "    ax1.set_title(\"Image where to find base face\")\n",
        "\n",
        "    ax2.imshow(new_res)\n",
        "    ax2.set_title(\"Results gaps filled with min neighbour\")\n",
        "\n",
        "    ax3.imshow(b)\n",
        "    ax3.set_title(\"Normalized results of Matching\")\n",
        "    plt.show()\n",
        "    logging.debug(\"=====================================================\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SpA0M6qxccoE",
        "colab_type": "text"
      },
      "source": [
        "In the previous section, still missing:\n",
        "* organization in different steps\n",
        "* normalization and pre-processing (exactly)\n",
        "* block-normalization\n",
        "* validation set in order to find the parameters of the HOG\n",
        "* explain several method to compute the gradients\n",
        "* set up the visualization of the inputs (dataset)\n",
        "* properly right a toy example for each step"
      ]
    }
  ]
}