{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CV_Group9_assignment.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gherbin/ComputerVisionKUL/blob/master/CV_Group9_assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZsHsvp6wb1n2",
        "colab_type": "text"
      },
      "source": [
        "# Hi there! \n",
        "Welcome to this Colab where we'll dig into some Computer Vision fancy stuff!\n",
        "\n",
        "The goals of this notebook is to perform faces classification and identification. To reach that goals we will first:\n",
        "- retrieve training and test images\n",
        "- build two features\n",
        "    1. handcrafted feature: Histogram of Oriented Gradients\n",
        "    2. Learnt from the data: Principal Component Analysis\n",
        "\n",
        "Then, we will train a model based on each feature and compare the classification and identification results.\n",
        "\n",
        "---\n",
        "\n",
        "Several libraries will be extensively used as optimized for some techniques we need. However, at first, some of the key functionalities will be coded as to provide a better view of what really happens being the calls to library functions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SnkjeOT3GuBy",
        "colab_type": "text"
      },
      "source": [
        "## Import all required packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KH_DPxxjTeoI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import cv2\n",
        "import tarfile\n",
        "import zipfile\n",
        "import shutil\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import random\n",
        "import logging\n",
        "\n",
        "from urllib import request\n",
        "from socket import timeout\n",
        "from urllib.error import HTTPError, URLError\n",
        "\n",
        "from google.colab import drive\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "from distutils.dir_util import copy_tree\n",
        "\n",
        "from skimage.feature import hog as skimage_feature_hog\n",
        "from sklearn.decomposition import PCA as sklearn_decomposition_PCA\n",
        "from skimage import exposure\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "from math import sqrt\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "from matplotlib.gridspec import GridSpec\n",
        "from matplotlib.offsetbox import OffsetImage, AnnotationBbox\n",
        "%matplotlib inline\n",
        "\n",
        "from scipy.interpolate import RectBivariateSpline\n",
        "from scipy.linalg import svd as scipy_linalg_svd\n",
        "\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "\n",
        "mpl_logger = logging.getLogger(\"matplotlib\")\n",
        "mpl_logger.setLevel(logging.WARNING)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XQ204pRs2P2i",
        "colab_type": "text"
      },
      "source": [
        "## Parameters\n",
        "\n",
        "As all computerized systems, several parameters help in defining how the system should react. \n",
        "Those parameters are centralized here."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ueU9Z-iMU63f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "base_path = \"/content/sample_data/CV__Group_assignment\"\n",
        "path_datasets = r\"/content/datasets/\"\n",
        "path_discard = r\"/content/discard/\"\n",
        "path_database = r\"/content/DATABASE/\"\n",
        "\n",
        "need_vgg_download = False\n",
        "confirmation_db_renewal = False\n",
        "to_drive_confirmation = False\n",
        "to_drive_confirmation_vgg = False\n",
        "\n",
        "load_from_local_drive = True # allows downloading the source images directly from the archive in github repository (see \"important note\")\n",
        "\n",
        "show = True # similar as global verbose parameter, for images (when custom functions allows it)\n",
        "\n",
        "sq_size = 64 # square size used -> shall be smaller than the output of get_min_size(faces_cropped) # assert sq_size <= min(get_min_size(faces_cropped))\n",
        "\n",
        "color = False\n",
        "\n",
        "if color:\n",
        "    my_color_map = plt.cm.viridis\n",
        "else:\n",
        "    my_color_map = plt.cm.gray\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "83XvyfDBSLQX",
        "colab_type": "text"
      },
      "source": [
        "## Utils functions\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "05JhwSLCSPyI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def pretty_return_dict_size(my_dict):\n",
        "    \"\"\"\n",
        "    returns a string containing the different size of the elements of a dict\n",
        "    \"\"\"\n",
        "    output_list = [\"\\n\"]\n",
        "    for k in my_dict.keys():\n",
        "        output_list.append(str(k))\n",
        "        output_list.append(\":\")\n",
        "        output_list.append(str(len(my_dict[k])))\n",
        "        output_list.append(\"\\n\")\n",
        "    return ''.join(output_list)\n",
        "\n",
        "def show_images_from_dict(my_dict, show_index = False):\n",
        "    \"\"\"\n",
        "    shows the images contained in a dictionary, going through all keys\n",
        "    \"\"\"\n",
        "    for k in my_dict.keys():\n",
        "        logging.debug(\"@------------------- Images of \" + str(k) + \" -------------------@\")\n",
        "        index = 0\n",
        "        for img in my_dict[k]:\n",
        "            if show_index:\n",
        "                logging.debug(\"Image index: \" + str(index))\n",
        "                index+=1\n",
        "            cv2_imshow(img)\n",
        "            logging.debug(\"Shape = \" + str(img.shape))\n",
        "def get_min_size(images_dict):\n",
        "    \"\"\"\n",
        "    returns the minimum size of images contained in the images_dict input\n",
        "    \"\"\"\n",
        "    min_rows, min_cols = float(\"inf\"), float(\"inf\")\n",
        "    max_rows, max_cols = 0, 0\n",
        "    for person in persons:\n",
        "        for src in images_dict[person]:\n",
        "            r, c = src.shape[0], src.shape[1]    \n",
        "            min_rows = min(min_rows, r)\n",
        "            max_rows = max(max_rows, r)\n",
        "            min_cols = min(min_cols, c)\n",
        "            max_cols = max(max_cols, c)\n",
        "    logging.info(\"smallest px numbers (row, cols) = \" + str((min_rows,min_cols)))\n",
        "    return min_rows, min_cols\n",
        "        \n",
        "def my_reshape(image_vector, sq_size, color):\n",
        "    \"\"\"\n",
        "    returns a reshape version of an image represented as an image array, depending of the color parameter.\n",
        "    If color is True, it returns a colored RGB format image of size (sq_size x sq_size) (useable as is by matplotlib)\n",
        "    If color is False, it returns a grayscale image (sq_size x sq_size)\n",
        "    \"\"\"\n",
        "    flattened = image_vector.ndim == 1\n",
        "    if flattened:\n",
        "        if color:\n",
        "            img_reshaped = (np.reshape(image_vector, (sq_size, sq_size, 3))).astype('uint8')\n",
        "            return cv2.cvtColor(img_reshaped, cv2.COLOR_BGR2RGB)\n",
        "        else:\n",
        "            return np.reshape(image_vector, (sq_size, sq_size))\n",
        "    else:\n",
        "        if color:\n",
        "            img_reshaped = (np.reshape(image_vector, (sq_size, sq_size, 3))).astype('uint8')\n",
        "            return cv2.cvtColor(img_reshaped, cv2.COLOR_BGR2RGB)\n",
        "        else:\n",
        "            return image_vector\n",
        "\n",
        "def get_matrix_from_set(images_set, color, sq_size = 64, flatten = True):\n",
        "    \"\"\"\n",
        "    from images_set (training_set or test_set), create and fill in matrix so that it contains the input data.\n",
        "    if flatten, then the matrix contains images represented in 1D\n",
        "    \"\"\"\n",
        "\n",
        "    # init output\n",
        "    matrix = None\n",
        "    nb_faces = sum([len(images_set[x]) for x in images_set if isinstance(images_set[x], list)])\n",
        "    # depending on mode, select appropriate size items. N\n",
        "    \n",
        "    if color and flatten:\n",
        "        matrix = np.empty((nb_faces, sq_size*sq_size*3)) # *3 => color images\n",
        "    elif color and (not flatten):\n",
        "        matrix = np.empty((nb_faces, sq_size, sq_size, 3))\n",
        "    elif (not color) and flatten:\n",
        "        matrix = np.empty((nb_faces, sq_size*sq_size))\n",
        "    elif (not color) and (not flatten):\n",
        "        matrix = np.empty((nb_faces, sq_size,sq_size ))\n",
        "    else:\n",
        "        raise RuntimeError\n",
        "\n",
        "    i = 0\n",
        "    for person in persons:\n",
        "        for src in images_set[person]:\n",
        "            src_rescaled = cv2.resize(src, (sq_size,sq_size))\n",
        "            if color and flatten:\n",
        "                matrix[i,:] = src_rescaled.flatten()\n",
        "            elif color and (not flatten):\n",
        "                matrix[i,:,:,:] = src_rescaled\n",
        "            elif (not color) and flatten:\n",
        "                matrix[i,:] = cv2.cvtColor(src_rescaled, cv2.COLOR_BGR2GRAY).flatten()\n",
        "            elif (not color) and (not flatten):\n",
        "                matrix[i,:,:] = cv2.cvtColor(src_rescaled, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "            i +=1\n",
        "    return matrix\n",
        "\n",
        "def plot_matrix(images_matrix, color, my_color_map, h=8, w=5, transpose = False, return_figure = False):\n",
        "    # if figure is not None:\n",
        "    #     fig = figure\n",
        "    #     fig.set_size_inches(w, h)\n",
        "    # else:\n",
        "    fig = plt.figure(figsize=(w,h)) \n",
        "    fig.subplots_adjust(left=0, right=1, bottom=0, top=1, hspace=0.05, wspace=0.05) \n",
        "    # plot the faces, each image is 64 by 64 pixels \n",
        "\n",
        "    if transpose:\n",
        "        images_matrix_used = images_matrix.T.copy()\n",
        "    else:\n",
        "        images_matrix_used = images_matrix.copy()\n",
        "\n",
        "    i=0\n",
        "    for img_vector in images_matrix_used: \n",
        "        ax = fig.add_subplot(h, w, i+1, xticks=[], yticks=[]) \n",
        "        ax.imshow(my_reshape(img_vector, sq_size, color), cmap = my_color_map, interpolation='nearest') \n",
        "        i+=1\n",
        "    plt.show()\n",
        "\n",
        "    if return_figure:\n",
        "        return fig"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ebQpRYW50fS2",
        "colab_type": "text"
      },
      "source": [
        "## Inputs\n",
        "\n",
        "* The very first input of the system is an archive containing several text file containing each 1000 weblinks to images. This archive is downloaded from [here](http://www.robots.ox.ac.uk/~vgg/data/vgg_face) and extracted locally in `/content/sample_data/CV__Group_assignment` folder. We download and extract it only if needed\n",
        "\n",
        "* To extract faces in the images, we download the Haarcascade model\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BGBkRUir9f7g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if not os.path.isdir(base_path):\n",
        "  os.makedirs(base_path)\n",
        "\n",
        "if need_vgg_download:\n",
        "    vgg_face_dataset_url = \"http://www.robots.ox.ac.uk/~vgg/data/vgg_face/vgg_face_dataset.tar.gz\"\n",
        "    with request.urlopen(vgg_face_dataset_url) as r, open(os.path.join(base_path, \"vgg_face_dataset.tar.gz\"), 'wb') as f:\n",
        "        f.write(r.read())\n",
        "\n",
        "    with tarfile.open(os.path.join(base_path, \"vgg_face_dataset.tar.gz\")) as f:\n",
        "        f.extractall(os.path.join(base_path))\n",
        "\n",
        "\n",
        "trained_haarcascade_url = \"https://raw.githubusercontent.com/opencv/opencv/master/data/haarcascades/haarcascade_frontalface_default.xml\"\n",
        "with request.urlopen(trained_haarcascade_url) as r, open(os.path.join(base_path, \"haarcascade_frontalface_default.xml\"), 'wb') as f:\n",
        "    f.write(r.read())\n",
        "logging.info(\"Downloaded haarcascade_frontalface_default\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MOH3DL2jKk3U",
        "colab_type": "text"
      },
      "source": [
        "# Data Retrieval"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lQPukQmEpoRC",
        "colab_type": "text"
      },
      "source": [
        "This tutorial will extensively use images from four different actors. The images are selected (pseudo-)randomly.\n",
        "\n",
        "The movie stars are (chosen quite randomly as well):\n",
        "1.   personA: Emma Stone\n",
        "2.   personB: Bradley Cooper\n",
        "3.   personC: Jane Levy\n",
        "4.   personD: Marc Blucas\n",
        "\n",
        "\n",
        "Process to get datasets images:\n",
        "1. Randomly pick N images (60 for persons A and B, and 30 for persons C and D) images from the list of 1000 images provided in the textfile. \n",
        "3. Reject some i images that are not appropriated (see rejection step lated) for each person. i may of course be different for all actors.\n",
        "2. Select M images randomly out of the (N-i) images obtained for each persons:\n",
        "    - M=30 for personA and personB (Training and Test),\n",
        "    - M=10 for personC and personD (Test only)\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "**[IMPORTANT NOTE]**\n",
        "\n",
        "If nothing else is set up, getting the N images require to perform a url request on websites we do not control. This is risky, as for any reason, the target website could be modified, not responding, responding too slowly, have removed the picture of interest, ...\n",
        "To prevent such issue, this tutorial provide the code to do things differently.\n",
        "- the first time (with some parameters below properly set), the source images are downloaded from the website (retrieving errors, skipping too slow website, etc.)\n",
        "- the images, downloaded, are then saved and zipped with a logfile\n",
        "- this zip archive is then uploaded on my personal Github account, as a public file\n",
        "\n",
        "It leads to a controlled database constaining the source images, and ensure reproducibility during the different test run.\n",
        "\n",
        "*Three remarks*\n",
        "1. only the original files are stored in the archive in the ZIP. Those files were selected randomly, using a random number generator.\n",
        "2. the curation of the source files, the face cropping, and selection between training and test sets is still done at every run of this notebook.\n",
        "3. the code dedicated to the archiving and saving part will not be detailed (while yet provided) in this notebook, but surely, you are welcome to contact me for more details using geoffroy.herbin@student.kuleuven.be.\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cFh6oMU6phLs",
        "colab_type": "text"
      },
      "source": [
        "Start from clean sheet\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NAMpaW-Ym0oG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "try:\n",
        "    shutil.rmtree(path_database)\n",
        "    shutil.rmtree(path_datasets)\n",
        "    shutil.rmtree(path_discard)\n",
        "except:\n",
        "    pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I4Rj5OSS3fUx",
        "colab_type": "text"
      },
      "source": [
        "Create required folders"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hQw39iLK36m1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "file_info = path_database+ r\"info_retrieved.txt\"\n",
        "\n",
        "try: \n",
        "    os.mkdir(path_database)\n",
        "    os.mkdir(path_datasets) \n",
        "    os.mkdir(path_discard)\n",
        "except OSError as error: \n",
        "    logging.error(error) \n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JuEfFZE42YeO",
        "colab_type": "text"
      },
      "source": [
        "Instead of randomly download from web, download images from a \"clean\" and controlled repository (in [github](https://raw.githubusercontent.com/gherbin/cv_group9_database_replica/master/DATABASE-20200318T142918Z-001.zip) ), dedicated for this notebook. It ensures reproducibility and accessibility to the 180 input images.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TgaFPa0C2YJH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if load_from_local_drive:\n",
        "    \n",
        "    !wget https://raw.githubusercontent.com/gherbin/cv_group9_database_replica/master/DATABASE-20200318T142918Z-001.zip\n",
        "\n",
        "    with zipfile.ZipFile(\"DATABASE-20200318T142918Z-001.zip\", 'r') as zip_ref:\n",
        "        zip_ref.extractall()\n",
        "    !rm -r \"DATABASE-20200318T142918Z-001.zip\"\n",
        "\n",
        "path_, dirs_, files = next(os.walk(path_database))\n",
        "if len(files) == 180+1:\n",
        "    logging.info(\"Successful database retrieval\")\n",
        "elif load_from_local_drive:\n",
        "    logging.error(\"Most Likely problem with database retrieval, number of files = \" + str(len(files)))\n",
        "else:\n",
        "    logging.info(\"No database images retrieved yet\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M09QQcnf38xN",
        "colab_type": "text"
      },
      "source": [
        "###Definition of several data structures\n",
        "\n",
        "`images_size` : dictionary containing the number of images to first get from the web\n",
        "\n",
        "`persons` : list containing the names of the four persons (the names actually are the name of the text file in original database)\n",
        "\n",
        "`images`: dictionary containing the source images. The keys of the dictionary are the names of the four persons of interest"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UEGMZ43b4iaa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "personA = \"Emma_Stone.txt\"\n",
        "personC = \"Jane_Levy.txt\"\n",
        "personB = \"Bradley_Cooper.txt\"\n",
        "personD = \"Marc_Blucas.txt\"\n",
        "persons = [personA, personB, personC, personD]\n",
        "datasets_dict = {}\n",
        "images_size = {}\n",
        "images_size[personA] = 60\n",
        "images_size[personB] = 60\n",
        "images_size[personC] = 30\n",
        "images_size[personD] = 30\n",
        "\n",
        "total_images_size = sum(images_size.values())\n",
        "\n",
        "# Dictionary containing the ids of the pictures downloaded from internet\n",
        "vgg_ids = {}\n",
        "for p in persons:\n",
        "    vgg_ids[p] = []\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8qdfeFWLAL6C",
        "colab_type": "text"
      },
      "source": [
        "If `confirmation_db_renewal` is `True`, the following code picks randomly (based on a seed being the name of the person) the images from the web.\n",
        "\n",
        "For a normal run, if the user does not want to change the original sourced data, `confirmation_db_renewal` should remain `False` (aka *change at your own risk* ;-) )\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I5VsGBAJ3inK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if confirmation_db_renewal:\n",
        "    try:\n",
        "        shutil.rmtree(path_database)\n",
        "    except:\n",
        "        pass \n",
        "    try:\n",
        "        os.mkdir(path_database)\n",
        "    except:\n",
        "        pass \n",
        "\n",
        "    fo = open(file_info, \"w+\")\n",
        "\n",
        "    # images = {}\n",
        "    # images_nominal_indices = {}\n",
        "    for person in persons:\n",
        "        logging.debug(\"Taking care of: \" + str(person))\n",
        "        random.seed(person)\n",
        "        # print(hash(person))\n",
        "        images_ = []\n",
        "        # images_nominal_indices_ = []\n",
        "        prev_index = []\n",
        "\n",
        "\n",
        "        with open(os.path.join(base_path, \"vgg_face_dataset\", \"files\", person), 'r') as f:\n",
        "            lines = f.readlines()       \n",
        "        \n",
        "\n",
        "        while len(images_) < images_size[person]:\n",
        "            index = random.randrange(0, 1000)\n",
        "            logging.debug(\"Index = \" + str(index))\n",
        "            if index in prev_index:\n",
        "                logging.debug(\"Index = \" + str(index) + \" => already there\")\n",
        "                continue\n",
        "            else:\n",
        "                prev_index.append(index)\n",
        "                line = lines[index]\n",
        "                # only curated data\n",
        "                if int(line.split(\" \")[8]) == 1:\n",
        "                    url = line[line.find(\"http://\"): line.find(\".jpg\") + 4]\n",
        "                    logging.debug(\"URL > \\\"\" + str(url))\n",
        "                    try:\n",
        "                        res = request.urlopen(url, timeout = 1)\n",
        "                        img = np.asarray(bytearray(res.read()), dtype=\"uint8\")\n",
        "                        img = cv2.imdecode(img, cv2.IMREAD_COLOR)\n",
        "\n",
        "                        h, w = img.shape[:2]\n",
        "                        cv2_imshow(cv2.resize(img, (w//4, h//4)))\n",
        "                        # images_nominal_indices_.append(index)\n",
        "\n",
        "                        filename = path_database +  str(index) + \"_\" + str(person.split(\".\")[0]) + \".jpg\"\n",
        "\n",
        "                        value = cv2.imwrite(filename, img) \n",
        "                        # logging.debug(\"saved in DB: \" + str(filename))\n",
        "                        images_.append(img)\n",
        "                        fo.write(line)\n",
        "                    except ValueError as e:\n",
        "                            logging.error(\"Value Error >\" + str(e))\n",
        "                    except (HTTPError, URLError) as e:\n",
        "                            logging.error('ERROR RETRIEVING URL >' + str(e))\n",
        "                    except timeout:\n",
        "                            logging.error('socket timed out - URL %s', str(url))\n",
        "                    except cv2.error as e: \n",
        "                            logging.error(\"ERROR WRITING FILE IN DB  >\" + str(e))\n",
        "                    except:\n",
        "                        logging.error(\"Weird exception : \" + str(line))\n",
        "                else:\n",
        "                    logging.debug(\"File not curated => rejected (id = \" + str(index) + \" )\")    \n",
        "                \n",
        "                # images[person] = images_\n",
        "                # images_nominal_indices[person] = images_nominal_indices_\n",
        "\n",
        "    fo.close()\n",
        "else:\n",
        "    logging.warning(\"If you really want to erase and renew the database, please change first the \\\"confirmation\\\" boolean variable, at the beginning of this cell\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P3eNIMAdCLVu",
        "colab_type": "text"
      },
      "source": [
        "From the logfile in the archive, extract the information and fill the dictionary containing all the images `images`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AdFihucjCO6e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open(file_info, 'r') as f: \n",
        "    lines = f.readlines()\n",
        "\n",
        "assert len(lines)==total_images_size, \"amount of lines in file incompatible\" \n",
        "\n",
        "images = {}\n",
        "\n",
        "for p in persons:\n",
        "    images[p] = []\n",
        "\n",
        "\n",
        "images_index = {}\n",
        "for running_index in range(len(lines)):\n",
        "    if running_index in range(0,images_size[personA]):\n",
        "        p = personA\n",
        "    elif running_index in range(images_size[personA],images_size[personA]+images_size[personB]):\n",
        "        p = personB\n",
        "    elif running_index in range(images_size[personA]+images_size[personB],images_size[personA]+images_size[personB]+images_size[personC]):\n",
        "        p = personC\n",
        "    elif running_index in range(images_size[personA]+images_size[personB]+images_size[personC],total_images_size):\n",
        "        p = personD\n",
        "    ind = str(int(lines[running_index].split(\" \")[0])-1)\n",
        "    vgg_ids[p].append(ind)\n",
        "    filename = ind + \"_\" + str(p.split(\".\")[0]) + \".jpg\"\n",
        "    images[p].append(cv2.imread(path_database+filename, cv2.IMREAD_COLOR))\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9v4tuRVO67u1",
        "colab_type": "text"
      },
      "source": [
        "From the sources files, although the images downloaded were part of a curated data, several images need to be removed to be used in the context of this *educative* tutorial. The main reasons are:\n",
        "\n",
        "*   too different from usual representation (make up, masks, ...)\n",
        "*   really poor image quality\n",
        "*   irrelevance and/or error in dataset\n",
        "*   same image already in dataset\n",
        "*   cropped image\n",
        "\n",
        "Considering the tight selection of images to train our model (20 from personA and 20 from personB), and the relatively large global amount of image candidates (1000 for each person), it is acceptable to reject the images we know won't help.\n",
        "\n",
        "From the initial retrieved images, we then remove the undesired images, that we copy in discard images folder, for tracking purposes. We/you may want to use them later.\n",
        "\n",
        "---\n",
        "`datasets_size`: dictionary of the size required per persons ( keys = person names)\n",
        "\n",
        "`to_remove`: dictionary containing the indices to remove, per persons ( keys = person names)\n",
        "\n",
        "`print_images = False` indicates that the remaining images in `images` dictionary will not be printed. \n",
        "\n",
        "---\n",
        "\n",
        "From the remaining images (after rejection), we can apply randomly select the images that are part of the final sets (training and test sets not split yet)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZPKRTSB5r8pg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Dictionary of the size required (see section 3)\n",
        "datasets_size = {}\n",
        "datasets_size[personA] = 30\n",
        "datasets_size[personB] = 30\n",
        "datasets_size[personC] = 10\n",
        "datasets_size[personD] = 10\n",
        "\n",
        "\n",
        "# manually remove images that are not relevant or considered not good enough to be part of the dataset\n",
        "to_remove = {}\n",
        "to_remove[personA] = [0,1,4,8,12,13,16,23,28,34,36,42,44,47,48,49,54]\n",
        "to_remove[personB] = [4,7,11,12,13,16,21,22,23,24,25,26,27,32,36,39,41,46,49,53,55,58]\n",
        "to_remove[personC] = [0,1,6,7,8,11,14,16,17,19,20,21,24]\n",
        "to_remove[personD] = [0,3,5,6,8,10,12,15,16,17,24]\n",
        "\n",
        "# goal is to sort in descending to remove elements from lists without modifying the indexes\n",
        "for p in persons:\n",
        "    to_remove[p].sort(reverse = True)\n",
        "\n",
        "\n",
        "# retrieve images candidates\n",
        "# --------------------------\n",
        "if len(os.listdir(path_datasets) ) == 0 or True:\n",
        "    logging.debug(\"datasets empty - need to retrieve all !\")\n",
        "    # removing images to discard\n",
        "    for person in persons:\n",
        "        for index in to_remove[person]:\n",
        "            img = images[person].pop(index)\n",
        "            logging.debug(\"Removing item \" + str(index) + \" from list \" + str(person))\n",
        "            try:\n",
        "                filename = path_discard +  str(index) +\"_discarded_\" + str(person.split(\".\")[0]) + \".jpg\"\n",
        "                cv2.imwrite(filename, img) \n",
        "            except:\n",
        "                logging.error(\"Error while writing discarded image \" + str(filename))\n",
        "\n",
        "    # randomly select among remaining images\n",
        "    for person in persons:\n",
        "        # build list of indices from remaining images\n",
        "        logging.debug(\"Phase 2 (part 1) -> random indices selection for \" + str(person))\n",
        "\n",
        "        images_ = []\n",
        "        indices = []\n",
        "        new_ids = []\n",
        "        # prev_index = []\n",
        "        random.seed(person)\n",
        "\n",
        "        while len(indices) < datasets_size[person]:       \n",
        "            index = random.randrange(0, len(images[person]))\n",
        "            if index in indices:\n",
        "                logging.debug(\"Index among remaining = \" + str(index) + \" => already there\")\n",
        "                continue\n",
        "            else:\n",
        "                # prev_index.append(index)\n",
        "                indices.append(index)\n",
        "\n",
        "        logging.debug(\"Phase 2 (part 2) -> image selection based on indices\")\n",
        "\n",
        "        for index in indices:\n",
        "            img = images[person][index]\n",
        "            images_.append(img)\n",
        "            filename = path_datasets +  str(vgg_ids[person][index]) + \"_\" + str(person.split(\".\")[0]) + \".jpg\"\n",
        "            logging.debug(\"saved: \" + str(filename))\n",
        "            cv2.imwrite(filename, img) \n",
        "            new_ids.append(vgg_ids[person][index])\n",
        "        images[person] = images_\n",
        "        vgg_ids[person] = new_ids\n",
        "else:\n",
        "    logging.debug(\"folders not empty => can build directly images dictionnary\")\n",
        "\n",
        "# logging.debug(\"Number of images keys=\" + len(images.keys))\n",
        "# logging.debug(\"Number of images values=\" + len(images.values))\n",
        "\n",
        "logging.info(pretty_return_dict_size(images))\n",
        "\"\"\" \n",
        "print images to get to_remove indices\n",
        "\"\"\"\n",
        "print_images = False\n",
        "if print_images:\n",
        "    for person in persons:\n",
        "        counter = 0\n",
        "        for img in images[person]:\n",
        "            h = 0\n",
        "            w = 0\n",
        "            img_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "            faces = faceCascade.detectMultiScale(\n",
        "                img_gray,\n",
        "                scaleFactor=1.13,\n",
        "                minNeighbors=10,\n",
        "                minSize=(30, 30),\n",
        "                flags=cv2.CASCADE_SCALE_IMAGE\n",
        "            )\n",
        "            for (x,y,w,h) in faces:\n",
        "                # faces_cropped[person].append(img[y:y+h, x:x+w])\n",
        "                cv2.rectangle(img, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
        "            logging.debug(\"------------------------------------------------------\")\n",
        "            logging.debug(\"Photo ID = \" + str(counter))\n",
        "            logging.debug(\"size = \" + str((h,w)))\n",
        "            cv2_imshow(img)\n",
        "            counter += 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jvExNNUtK5AU",
        "colab_type": "text"
      },
      "source": [
        "Mount drive and save images, according to the parameter `to_drive_confirmation` value. *Change at your own risk ;-)*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XwJjZ1oaHEV9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Save to drive folders\n",
        "path_drive_DB = r\"/content/drive/My Drive/ComputerVision/DATABASE\"\n",
        "path_drive_Datasets = r\"/content/drive/My Drive/ComputerVision/DATASETS\"\n",
        "\n",
        "# drive folders should be properly set up\n",
        "\n",
        "if to_drive_confirmation:\n",
        "    \n",
        "    logging.warning(\"You need to have a drive mounted for this snippet to run successfully\")\n",
        "\n",
        "    try:\n",
        "        drive.mount('/content/drive')\n",
        "    except:\n",
        "        pass\n",
        "    try:\n",
        "        shutil.rmtree(path_drive_DB)\n",
        "        shutil.rmtree(path_drive_Datasets)\n",
        "    except:\n",
        "        logging.error(\"Error in rmtree\") \n",
        "\n",
        "\n",
        "    try: \n",
        "        os.mkdir(path_drive_DB) \n",
        "        os.mkdir(path_drive_Datasets)\n",
        "    except OSError as error: \n",
        "        logging.error(error) \n",
        "    \n",
        "    logging.debug(\"Saving database in drive : start\")\n",
        "\n",
        "    fromDirectory = path_database\n",
        "    toDirectory = path_drive_DB\n",
        "    copy_tree(fromDirectory, toDirectory)\n",
        "\n",
        "    logging.debug(\"Saving datasets in drive : start\")\n",
        "\n",
        "    fromDirectory = path_datasets\n",
        "    toDirectory = path_drive_Datasets\n",
        "    copy_tree(fromDirectory, toDirectory)\n",
        "\n",
        "    logging.debug(\"Saving: done !\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z5K7VyKZupB3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "path_vgg = r\"/content/sample_data/CV__Group_assignment\"\n",
        "path_drive_vgg = r\"/content/drive/My Drive/ComputerVision/CV__Group_assignment\"\n",
        "\n",
        "# drive folders should be properly set up\n",
        "\n",
        "if to_drive_confirmation_vgg:\n",
        "    \n",
        "    logging.warning(\"You need to have a drive mounted for this snippet to run successfully\")\n",
        "\n",
        "    try:\n",
        "        drive.mount('/content/drive')\n",
        "    except:\n",
        "        pass\n",
        "    try:\n",
        "        shutil.rmtree(path_drive_vgg)\n",
        "    except:\n",
        "        logging.error(\"Error in rmtree\") \n",
        "\n",
        "    try: \n",
        "        os.mkdir(path_drive_vgg) \n",
        "    except OSError as error: \n",
        "        logging.error(error) \n",
        "    \n",
        "    logging.debug(\"Saving database in drive : start\")\n",
        "\n",
        "    fromDirectory = path_vgg\n",
        "    toDirectory = path_drive_vgg\n",
        "    copy_tree(fromDirectory, toDirectory)\n",
        "\n",
        "    logging.debug(\"Saving: done !\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9HilePrqbDWC",
        "colab_type": "text"
      },
      "source": [
        "From the raw images saved in the `images` dictionary, the faces are extracted using the *HaarCascade* method.\n",
        "\n",
        "The following code is based on the tutorial: [How to detect faces using Haar Cascade](https://www.digitalocean.com/community/tutorials/how-to-detect-and-extract-faces-from-an-image-with-opencv-and-python)\n",
        "\n",
        "The faces are saved in a new dictionary: `faces_cropped`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G582IgL-fvi1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "faceCascade = cv2.CascadeClassifier(os.path.join(base_path, \"haarcascade_frontalface_default.xml\"))\n",
        "faces_cropped = {}\n",
        "\n",
        "with open(file_info, 'r') as f: \n",
        "    lines = f.readlines()\n",
        "\n",
        "for person in persons:\n",
        "\n",
        "    faces_cropped[person] = []\n",
        "\n",
        "    for img in images[person]:\n",
        "        img_ = img.copy()\n",
        "        img_gray = cv2.cvtColor(img_, cv2.COLOR_BGR2GRAY)\n",
        "        faces = faceCascade.detectMultiScale(\n",
        "            img_gray,\n",
        "            scaleFactor=1.13,\n",
        "            minNeighbors=10,\n",
        "            minSize=(30, 30),\n",
        "            flags=cv2.CASCADE_SCALE_IMAGE\n",
        "        )\n",
        "        for (x,y,w,h) in faces:\n",
        "            faces_cropped[person].append(img[y:y+h, x:x+w])\n",
        "            cv2.rectangle(img_, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
        "\n",
        "        # h, w = img_.shape[:2]\n",
        "        # draw_box(lines, int(vgg_ids[person][running_index])+1, img_, person)\n",
        "        # cv2_imshow(cv2.resize(img_, (w // 2, h // 2)))\n",
        "\n",
        "logging.info(\"Faces extracted and saved in dictionnary faces_cropped\")\n",
        "logging.info(pretty_return_dict_size(faces_cropped))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zVrtCWGgRFKX",
        "colab_type": "text"
      },
      "source": [
        "At this point, `faces_cropped` dictionary contains 30 cropped faces for personA and B, and 10 images for personC and personD.\n",
        "\n",
        "The following code selects randomly (based on a seed) the 20 images part of the training set for personA and personB. The other faces (10 for each person) are then part of the test set.\n",
        "\n",
        "---\n",
        "\n",
        "* `training_set`: dictionary containing faces cropped (original size) part of the training set\n",
        "* `test_set`: dictionary containing faces cropped (original size) part of the test set\n",
        "---\n",
        "\n",
        "At this point, there is not (yet) dedicated validation sets. It is discussed later on.\n",
        "\n",
        "All the training will be done on the training set faces, without any tailoring or dedicated fitting on the test set images. Indeed, metrics on the test set faces indicate how well our model will generalize. It's therefore important to not influence our model with the data of the test set.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ylnkm0G5SCcR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "training_sets_size = {}\n",
        "training_sets_size[personA] = 20\n",
        "training_sets_size[personB] = 20\n",
        "training_sets_size[personC] = 0\n",
        "training_sets_size[personD] = 0\n",
        "\n",
        "test_sets_size = {}\n",
        "test_sets_size[personA] = 10\n",
        "test_sets_size[personB] = 10\n",
        "test_sets_size[personC] = 10\n",
        "test_sets_size[personD] = 10\n",
        "\n",
        "training_set = {}\n",
        "test_set = {}\n",
        "for person in persons:\n",
        "    image_ = faces_cropped[person]\n",
        "    training_set_ = []\n",
        "    random.seed(person)\n",
        "    init_set = set(range(0, len(image_)))\n",
        "\n",
        "    indices_training = random.sample(init_set, training_sets_size[person])\n",
        "    indices_test = list(init_set - set(indices_training))\n",
        "\n",
        "    training_set[person] = [faces_cropped[person][i] for i in indices_training] \n",
        "    test_set[person] = [faces_cropped[person][i] for i in indices_test]\n",
        "\n",
        "logging.info(\"Faces saved in dictionnary training_set: \")\n",
        "logging.info(pretty_return_dict_size(training_set))\n",
        "logging.info(\"Faces saved in dictionnary test_set: \")\n",
        "logging.info(pretty_return_dict_size(test_set))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "pAvlX6WBio8R",
        "colab": {}
      },
      "source": [
        "# show_images_from_dict(training_set)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-pOSPIokbVSV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# show_images_from_dict(test_set)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j73x0LDhZX9k",
        "colab_type": "text"
      },
      "source": [
        "### Faces of the training set\n",
        "- 20 faces of Emma Stone, personA\n",
        "- 20 faces of Bradley Cooper, personB\n",
        "\n",
        "PersonA and PersonB are quite different, A being a female, and B a male. Furthermore, the images within a class are somehow dissimilar as well\n",
        "- different viewpoints (front, left, right)\n",
        "- different lightening conditions\n",
        "- not same hair color\n",
        "- beard/no beard (personB)\n",
        "- not same (limited) background\n",
        "\n",
        "However, a similar characteristic is that both of the actors are most of the time smiling on the faces extracted. Sometimes showing their teeth, sometimes not.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cPlaQzrHZncq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#  Visualisation \n",
        "training_set_matrix = get_matrix_from_set(training_set, color = True, sq_size = sq_size,flatten = True)\n",
        "plot_matrix(training_set_matrix, color = True, my_color_map = plt.cm.viridis, h=4, w=10, transpose = False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T84lBuROhHe3",
        "colab_type": "text"
      },
      "source": [
        "### Faces of the test set\n",
        "Test images are needed for four persons:\n",
        "- 10 faces of Emma Stone, personA\n",
        "- 10 faces of Bradley Cooper, personB\n",
        "- 10 faces of Jane Levy, personC\n",
        "- 10 faces of Marc Blucas, personD\n",
        "\n",
        "(A - C) and (B - D) respectively share some characteristics:\n",
        "* both female / male\n",
        "* same kind of skin tone\n",
        "* visually quite similar (especially for A and C)\n",
        "\n",
        "Within each groups, as for the training set, the faces are taken from different viewpoints, lightening conditions, ...\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kpceMYvAhH_Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_set_matrix = get_matrix_from_set(test_set, color = True, sq_size=sq_size, flatten = True)\n",
        "plot_matrix(test_set_matrix, color = True, my_color_map = plt.cm.viridis, h=4, w=10, transpose = False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fq0JMIBgOhih",
        "colab_type": "text"
      },
      "source": [
        "# Feature Representations\n",
        "\n",
        "A feature representation of an object is intuitively a piece of *information*, of a reduced dimension with respect to the object, and that captures the object.\n",
        "It tells what defines the object, and allows differentiating different objects.\n",
        "\n",
        "In the context of an image, a good feature needs to be:\n",
        "- **robust**: the same feature extracted from the same object on an image should be *close*, even if the lightening condition change, the view point change, ...\n",
        "- **discriminative**: different images, representing different object, should lead to different representation in the feature space. As a toy example, the size of an image is not a good feature to detect a person, as several person can be represented in images of the same size.\n",
        "Two types \n",
        "\n",
        "We will look at two features:\n",
        "1. **HOG**: Histogram of oriented gradients. This is a handcrafted feature, extracted using a specific algorithm \n",
        "2. **PCA**: Principal Component Analysis. This is a feature learnt from the data.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yTaT6m7T4lF6",
        "colab_type": "text"
      },
      "source": [
        "## Histogram of Oriented Gradients - HOG\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kFIgA2EaDhSQ",
        "colab_type": "text"
      },
      "source": [
        "As this is a tutorial of Computer Vision, let's look first at what is visually / intuitively the HOG on a real image - one of the Emma Stone (personA) faces. The execution of the following code snippet shows on the left the input face, and on the right, the results.\n",
        "\n",
        "Then, we'll see the details of the algorithm, and its specificities (parameters)\n",
        "\n",
        "This section is extensively inspired by [this course](https://www.learnopencv.com/histogram-of-oriented-gradients/), while the technique has been introduced by [this paper](http://lear.inrialpes.fr/people/triggs/pubs/Dalal-cvpr05.pdf), which I strongly advise to read!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PrC1L60wOadz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "First example - Emma stone first image\n",
        "\"\"\"\n",
        "src = faces_cropped[personA][0]\n",
        "# cv2_imshow(src)\n",
        "\n",
        "#1 resizing\n",
        "resized_img = cv2.resize(src, (sq_size, sq_size))\n",
        "#2 computing HOG\n",
        "fd, hog_image = skimage_feature_hog(resized_img, \n",
        "                    orientations=9, \n",
        "                    pixels_per_cell=(8,8), \n",
        "                    cells_per_block=(2, 2), \n",
        "                    block_norm = \"L2\",\n",
        "                    visualize=True, \n",
        "                    transform_sqrt = True,\n",
        "                    multichannel=True)\n",
        "\n",
        "\"\"\"\n",
        "Plotting results\n",
        "\"\"\"\n",
        "\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 8), sharex=True, sharey=True) \n",
        "ax1.imshow(cv2.cvtColor(resized_img, cv2.COLOR_BGR2RGB))\n",
        "ax1.set_title('Input image') \n",
        "\n",
        "# Rescale histogram for better display \n",
        "hog_image_rescaled = exposure.rescale_intensity(hog_image, in_range=(0, 10)) \n",
        "\n",
        "ax2.imshow(hog_image_rescaled, cmap=plt.cm.gray) \n",
        "ax2.set_title('Histogram of Oriented Gradients - rescaled')\n",
        "logging.debug(\"HOG Rescaled: \" + str(hog_image_rescaled.min()) + \" -> \" + str(hog_image_rescaled.max()) )\n",
        "\n",
        "# ax3.imshow(hog_image, cmap=plt.cm.gray) \n",
        "# ax3.set_title('Histogram of Oriented Gradients')\n",
        "# logging.debug(\"HOG: \" + str(hog_image.min()) + \" -> \" + str(hog_image.max()) )\n",
        "plt.show()\n",
        "\n",
        "# logging.debug(hog_image_rescaled.shape)\n",
        "logging.debug(\"fd shape = \" + str(fd.shape))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b_b7lnLnmuDk",
        "colab_type": "text"
      },
      "source": [
        "### HOG - What is that ?\n",
        "\n",
        "HOG is a feature descriptor that extracts information from an image (or more precisely, a patch) based on the gradients in this image. More specifically, it builds a vector representing the weighted distribution of the gradients orientation across the images.\n",
        "\n",
        "#### Why is it interesting ?\n",
        "Let's remember that our goal is to perform image classification and identification. \n",
        "A face can be recognized through the inherent shapes: circular of face, shapes of the eyes, the nose, potentially the glasses, etc. The *edge* information is therefore useful! It is even more useful than the colors... Intuitively, you can think about recognizing someone familiar with only some contours of one face.\n",
        "\n",
        "![Drawing Obama](http://www.drawingskill.com/wp-content/uploads/2/Barack-Obama-Drawing-Pics.jpg)\n",
        "\n",
        "It is easy to recognize the former US President, while no color information is given. Intuitively, HOG gives the same information:\n",
        "- magnitude of gradient is large around the edges and corners\n",
        "- orientation gives the shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "upmrGJDTJfRt",
        "colab_type": "text"
      },
      "source": [
        "#### How to compute the HOG of an image ?\n",
        "\n",
        "In a nutshell:\n",
        "- The gradients are first computed on each pixel. \n",
        "- The gradients orientation and magnitude are used to build an histogram for a cell. The size of a cell is typically 8 x 8 pixels. \n",
        "- Those histogram are normalized \n",
        "- All the histograms computed on the images are then concatenated in a *long* vector, yet much smaller than original image. \n",
        "\n",
        "In the upcoming sections, we will detail the process, as well as the code and parameters required at each step."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kotEMzlaJdvw",
        "colab_type": "text"
      },
      "source": [
        "The following code snippet is a homemade class required to compute the HOG. The results obtained with this code will be compared with the infamous skimage library optimized for the HOG descriptor. \n",
        "\n",
        "You can simply run the snippet and come back later on to see the details of the implementation.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "stIpfAn9FIIz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MyHog():\n",
        "    \n",
        "    def __init__(self, img):\n",
        "        self.img = img # image of the size 64x64; 64x128; 128x128; ... => resized image of the original\n",
        "        self.mag_max, self.orn_max = self.compute_gradients()\n",
        "\n",
        "    def compute_gradients(self):\n",
        "        gx = cv2.Sobel(self.img, cv2.CV_32F, 1, 0, ksize = 1)\n",
        "        gy = cv2.Sobel(self.img, cv2.CV_32F, 0, 1, ksize = 1)\n",
        "\n",
        "        mag, angle = cv2.cartToPolar(gx, gy, angleInDegrees=True)\n",
        "        orn = angle.copy()\n",
        "\n",
        "        logging.debug(\"mag shape :\" + str(mag.shape))\n",
        "        logging.debug(\"orn shape :\" + str(orn.shape))\n",
        "\n",
        "        # constructing matrices of max dimension\n",
        "        mag_max = np.zeros((mag.shape[0], mag.shape[1]))\n",
        "        orn_max = np.zeros((orn.shape[0], orn.shape[1]))\n",
        "        for i in range(mag.shape[0]):\n",
        "            for j in range(mag.shape[1]):\n",
        "                mag_max[i,j] = mag[i,j].max()\n",
        "                idx = np.argmax(mag[i,j])\n",
        "                orn_max[i,j] = orn[i,j,idx] \n",
        "\n",
        "        # mag_max = mag_max.T    \n",
        "        # orn_max = orn_max.T \n",
        "\n",
        "        return mag_max, orn_max\n",
        "\n",
        "    def get_cells_mag_orn(self, y_start, x_start, cell_h, cell_w):\n",
        "        \"\"\"\n",
        "        returns the cell magnitude, orientation and \"clipped\" orientation \n",
        "        ( where 0 -> 360 is mapped into 0 -> 180)\n",
        "        \"\"\"\n",
        "        cell_mag = np.zeros((cell_h,cell_w))\n",
        "        cell_orn = np.zeros((cell_h,cell_w))\n",
        "        for i in range(cell_h):\n",
        "            for j in range(cell_w):\n",
        "                cell_mag[i,j] = self.mag_max[y_start+i, x_start+j]\n",
        "                cell_orn[i,j] = round(self.orn_max[y_start+i,x_start+j])\n",
        "        \n",
        "        cell_orn_clipped = cell_orn.copy() \n",
        "        cell_orn_clipped = ((cell_orn_clipped) + 90 ) % 360\n",
        "        for i in range(cell_h):\n",
        "            for j in range(cell_w):\n",
        "                if 0 <= cell_orn_clipped[i,j] < 180:\n",
        "                    cell_orn_clipped[i,j] = 180 - cell_orn_clipped[i,j]\n",
        "                elif 180 <= cell_orn_clipped[i,j] <=360:\n",
        "                    cell_orn_clipped[i,j] = 180 - cell_orn_clipped[i,j] % 180\n",
        "\n",
        "        return cell_mag.T, cell_orn.T, cell_orn_clipped.T\n",
        "    \n",
        "    def fill_bins_one_pixel(self, mag, orn, bin_list, implementation_type = \"skimage\"):\n",
        "        \"\"\"\n",
        "        # mag: magnitude of the gradient of 1 px\n",
        "        # orn: orientation of the gradient of 1 px\n",
        "        bin_list: reference, list of bins that is incremented\n",
        "        \"\"\"\n",
        "        N_BUCKETS = len(bin_list)\n",
        "        assert N_BUCKETS == 9, \"N_BUCKETS is not 9!!!\"\n",
        "        size_bin = 20.\n",
        "        if implementation_type == \"learnopencv\":\n",
        "            if orn >= 160:\n",
        "                left_bin = 8\n",
        "                right_bin = 9\n",
        "                left_val= mag * (right_bin * 20 - orn) / 20\n",
        "                right_val = mag * (orn - left_bin * 20) / 20\n",
        "                left_bin = 8\n",
        "                right_bin = 0\n",
        "            else:\n",
        "                left_bin = int(orn / size_bin)\n",
        "                right_bin = (int(orn / size_bin) + 1) % N_BUCKETS\n",
        "                left_val= mag * (right_bin * 20 - orn) / 20\n",
        "                right_val = mag * (orn - left_bin * 20) / 20\n",
        "            \n",
        "            assert left_val >= 0, \"leftval = \" + str(left_val) + \", \" + str(\"mag = \") + str(mag) + \" & orn = \" + str(orn)\n",
        "            assert right_val >= 0, \"rightval = \" + str(right_val) + \", \" + str(\"mag = \") + str(mag) + \" & orn = \" + str(orn)\n",
        "\n",
        "            # print(left_val)\n",
        "            # print(right_val)\n",
        "\n",
        "            bin_list[left_bin] += left_val\n",
        "            bin_list[right_bin] += right_val\n",
        "\n",
        "        elif implementation_type == \"skimage\":\n",
        "            # easiest \n",
        "            \"\"\"\n",
        "            this implementation mimics the one from skimage\n",
        "            \"\"\"\n",
        "\n",
        "            if 0 <= orn <= 10:\n",
        "                bin_list[4] += mag\n",
        "            elif 10 < orn <= 30:\n",
        "                bin_list[3] += mag\n",
        "            elif 30 < orn <= 50:\n",
        "                bin_list[2] += mag\n",
        "            elif 50 < orn <= 70:\n",
        "                bin_list[1] += mag\n",
        "            elif 70 < orn <= 90:\n",
        "                bin_list[0] += mag\n",
        "            elif 90 < orn <= 110:\n",
        "                bin_list[8] += mag \n",
        "            elif 110 < orn <= 130:\n",
        "                bin_list[7] += mag\n",
        "            elif 130 < orn <= 150:\n",
        "                bin_list[6] += mag\n",
        "            elif 150 < orn <= 170:\n",
        "                bin_list[5] += mag \n",
        "            elif 170 < orn <= 180:\n",
        "                bin_list[4] += mag \n",
        "            else:\n",
        "                raise RuntimeError(\"Impossible ! > \" + str(orn))\n",
        "        else:\n",
        "            raise NotImplementedError\n",
        "\n",
        "\n",
        "\n",
        "    def compute_hog_bins(self, y_start, x_start, cell_h, cell_w, show_src = True, show_results=True, figsize = (12,4)):\n",
        "        \"\"\"\n",
        "        y_start: y value of the top left pixel\n",
        "        x_start: x value of the top left pixel\n",
        "        cell_h : height of the cell in which HOG is computed\n",
        "        cell_w : width of the cell in which HOG is computed\n",
        "        \"\"\"\n",
        "        cell_img = self.img[y_start:y_start + cell_h, x_start:x_start+cell_w]\n",
        "\n",
        "        if show_src:\n",
        "            tmp = self.img.copy()\n",
        "            cv2.rectangle(tmp, (x_start-1, y_start-1), (x_start+cell_w+1, y_start+cell_h+1), (0,255,0))\n",
        "\n",
        "            fig, ax = plt.subplots(1,1, figsize = (figsize[1],figsize[1]))\n",
        "            ax.imshow(cv2.cvtColor(tmp, cv2.COLOR_BGR2RGB))\n",
        "            ax.set_title(\"Selection of a cell\")\n",
        "\n",
        "            plt.show()\n",
        "\n",
        "        # construction of the magnitude and orn matrices\n",
        "        cell_mag, cell_orn, cell_orn_clipped = self.get_cells_mag_orn(y_start, x_start, cell_h, cell_w)\n",
        "\n",
        "        number_of_bins = 9\n",
        "        bin_list = np.zeros(number_of_bins)\n",
        "        for i in range(cell_h):\n",
        "            for j in range(cell_w):\n",
        "                # m = round(mag_normalized[y_start+j,x_start+i].max())\n",
        "                m = cell_mag[i,j]\n",
        "                d = cell_orn_clipped[i,j]\n",
        "                # print(\"m,d =\" + str((m,d)))\n",
        "                self.fill_bins_one_pixel(m,d,bin_list)\n",
        "        \n",
        "        # logging.debug(\"Bins computed:\" + str(bin_list))\n",
        "        n = np.linalg.norm(bin_list)\n",
        "        bin_norms = bin_list/n\n",
        "        # logging.debug(\"Bins normalized:\" + str(bin_norms))\n",
        "             \n",
        "        if show_results:\n",
        "            fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=figsize, sharex=True, sharey=True) \n",
        "\n",
        "            # ax1 => just to visually represent the arrows\n",
        "            for i in range(cell_h):\n",
        "                for j in range(cell_w):\n",
        "\n",
        "                    radius = cell_mag[i,j] / (cell_mag.max() - cell_mag.min())\n",
        "                    angle_ = cell_orn[i,j]\n",
        "                    \n",
        "                    orn_value_clipped = cell_orn_clipped[i,j]\n",
        "\n",
        "                    mag_value = round(cell_mag[i,j])\n",
        "                    \n",
        "                    ax1.arrow(i, j, radius*np.cos(np.deg2rad(angle_)), radius*np.sin(np.deg2rad(angle_)), head_width=0.15, head_length=0.15, fc='b', ec='b')\n",
        "                    ax2.text(i, j, str(orn_value_clipped.astype(np.int64)), fontsize=10,va='center', ha='center')\n",
        "                    ax3.text(i, j, str(mag_value.astype(np.int64)), fontsize=10,va='center', ha='center')\n",
        "\n",
        "            ax1.imshow(cv2.cvtColor(cell_img, cv2.COLOR_BGR2RGB))\n",
        "            ax1.set_title('Input image') \n",
        "\n",
        "            ax2.matshow(cell_orn_clipped, alpha=0)\n",
        "            ax2.set_title('Orientation values')\n",
        "\n",
        "            ax3.set_title('Magnitude values')\n",
        "            intersection_matrix = np.ones(cell_mag.shape)\n",
        "            ax3.matshow(cell_mag, alpha = 0)\n",
        "           \n",
        "            plt.show()\n",
        "        return bin_list, bin_norms"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oMB6GhIcLi2C",
        "colab_type": "text"
      },
      "source": [
        "###HOG How-to, Step1: Preprocessing the image\n",
        "\n",
        "Usually, the size of an image is not appropriate to perform the HOG computation. The easiest thing is just to resize the image to an appropriate size. In this tutorial, we use a multiple of 8 and a square image. Considering the smallest face cropped, we select 64 x 64 pixels.\n",
        "\n",
        "Furthermore, as often in image processing, the largest the image, the more resource consuming it is. Keeping a reasonably small image helps in having a decent computation time.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o42F21fRN7Fe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "logging.info(\"Toy Example\")\n",
        "\n",
        "img = faces_cropped[personA][0].copy()\n",
        "resized_img = cv2.resize(img, (64,64))\n",
        "logging.info(\"Shape of source  face: \" + str(img.shape))\n",
        "logging.info(\"Shape of resized face: \" + str(resized_img.shape))\n",
        "\n",
        "\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(8, 4), sharex=False, sharey=False) \n",
        "\n",
        "ax1.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
        "ax1.set_title('Input image') \n",
        "\n",
        "ax2.imshow(cv2.cvtColor(resized_img, cv2.COLOR_BGR2RGB))\n",
        "ax2.set_title('Resized image') \n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ufx19W7JePys",
        "colab_type": "text"
      },
      "source": [
        "###HOG How-to, Step2: Computing the gradient for all pixels\n",
        "\n",
        "Computing the gradient in horizontal ($x$) and vertical ($y$) directions can be done using a pass of the Sobel Filter, part of the *openCV* library. \n",
        "This is implemented in the `MyHog.compute_gradients()` function (see implementation of `MyHog` class, above. From these gradients in $x$ and $y$ we can derive the magnitudes and orientations in all pixels using the formulas:\n",
        "> $\n",
        "\\begin{align} \n",
        "mag &= \\sqrt{g_x^2 + g_y^2} \\\\ \n",
        "orn &= atan(\\frac{g_y}{g_x}) \n",
        "\\end{align}\n",
        "$\n",
        "\n",
        "This is implemented using *openCV* library with `cartToPolar`.\n",
        "\n",
        "#####*Grayscale or Colored image*\n",
        "> For grayscale image, every pixel has one value so that this computation is straightforward. For colored image, a pixel has three values (one for Red, one for Green, one for Blue). In the HOG algorithm, the gradients is computed for all three channels, and the final magnitude is the maximum of the three, and the orientation is the one corresponding to the magnitude.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hQiWXisWJItc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "Create an object MyHog, which takes a resized image as input, and compute its \n",
        "gradients.\n",
        "\"\"\"\n",
        "myhog = MyHog(resized_img)\n",
        "\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(8, 4), sharex=True, sharey=True) \n",
        "\n",
        "ax1.imshow(myhog.mag_max, cmap = plt.cm.jet)\n",
        "ax1.set_title('Max of magnitude') \n",
        "\n",
        "ax2.imshow(myhog.orn_max, cmap = plt.cm.jet)\n",
        "ax2.set_title('Max of Orientation') \n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IQxPKrGhizSK",
        "colab_type": "text"
      },
      "source": [
        "As visible on the magnitude and orientation plots above, only essential information regarding the edges is kept. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lSvgxKzkiiWi",
        "colab_type": "text"
      },
      "source": [
        "###HOG How-to, Step3: Compute histograms for cells\n",
        "\n",
        "The histograms are first computed for small cells. It has two major benefits\n",
        "1. the representation is more compact: Suppose we take cells of $8 \\times 8$ pixels. The gradient of each pixel is described using 2 numbers (magnitude and orientation), leading to 128 numbers. Considering an histogram applied on such a cell allows to represent those 128 numbers by a tiny array of typically 9 numbers. In total a colored image of $64 \\times 64$ pixels is represented using $9*8*8$ vector.\n",
        "2. the representation is less sensitive to noise, as applying a cell is equivalent to a low-pass filter. Higher frequency outliers are therefore of less importance.\n",
        "\n",
        "\n",
        "The choice of the cell size is a design choice that can be modified. In a later section, we will modify this parameter to see how it can affect the results. \n",
        "In the paper that first presented the technique, a cell of $8 \\times 8$ pixels was used - we will continue with this(hyper-)parameter. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OziV9UK_rMGp",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "> *You may try yourself to modify the cell size or the x_start or y_start values, to see the influence on the histogram computed for that particular cell*\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bkapi9krhA_l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "Size of a cell\n",
        "\"\"\"\n",
        "cell_h = 8\n",
        "cell_w = 8\n",
        "\n",
        "\"\"\"\n",
        "starting point of the cell on the image\n",
        "\"\"\"\n",
        "y_start = 3 * cell_h - 1\n",
        "x_start = 2 * cell_w - 1\n",
        "\n",
        "hog_val, hog_val_normalized = myhog.compute_hog_bins(y_start, x_start, cell_h, cell_w, show_results=True, figsize=(16,5))\n",
        "\n",
        "fig, ax = plt.subplots(1,1, figsize = (2*5, 5))\n",
        "ax.bar([\"]70-90]\",\"]50-70]\", \"]30-50]\", \"]10-30]\",\"]10-\\n180]\",\"]150-\\n170]\",\"]130-\\n150]\",\"]110-\\n130]\", \"]90-\\n110]\"], hog_val_normalized)\n",
        "ax.set_title(\"Histogram computed with MyHog (homemade)\")\n",
        "plt.show()\n",
        "logging.info(\"MyHog bins   normalized  : {}, {}, {}, {}, {}, {}, {}, {}, {}\".format(*np.round(hog_val_normalized,2)))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2BFakzS91ki5",
        "colab_type": "text"
      },
      "source": [
        "**Legend of the above images**\n",
        "1. source image with *cell* visible in flashy green,\n",
        "2. details of the gradients computations\n",
        "    - *leftmost*: cell enlarged with an arrow indicating the gradient: length of the arrow represent the magnitude, and orientation is the gradient orientation computed on that pixel\n",
        "    - *middle*: matrix (shape == cell) containing the orientations computed (unsigned)\n",
        "    - *rightmost*: matrix (shape == cell) containung the magnitude computed\n",
        "3. histogram computed (`keyword = \"skimage\"`)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NbKkGHW9ru-I",
        "colab_type": "text"
      },
      "source": [
        "####In details\n",
        "\n",
        "The details of building the histogram for a cell is not complicated:\n",
        "- consider N bins. N is a design parameter, and each of the bins represent a range of gradient orientations. I have chosen N = 9, following the [introducing paper](http://lear.inrialpes.fr/people/triggs/pubs/Dalal-cvpr05.pdf), as it induces a granularity fine enough to observe change in the picture.\n",
        "> The HOG is usually applied using **unsigned gradients**. The numbers on the orientation matrix are between 0 and 180 instead of 0 and 360 degrees. Concretely, an angle $\\alpha [deg] $ and $(180 + \\alpha) [deg]$ contribute to the same bin. Empirically, it has been observed that it wasn't decreasing performance in the detection. Of course, nothing forbids to use signed gradients. \n",
        "- for a particular pixel:\n",
        "    - the bin is selected according to the orientation of the gradient; \n",
        "    - the value that goes in the bin is based on the magnitude. Different methods were proposed by the [introducing paper](http://lear.inrialpes.fr/people/triggs/pubs/Dalal-cvpr05.pdf), and are also explained in details for instance in [vidha blog](https://www.analyticsvidhya.com/blog/2019/09/feature-engineering-images-introduction-hog-feature-descriptor/). The class `MyHog` above contains two implementation: either the magnitude is split proportionnaly between two bins (as described in [learnopencv](https://www.learnopencv.com/histogram-of-oriented-gradients/), or -- as done in *openCV* library --, the whole magnitude is assigned to the closest bin. \n",
        "\n",
        "While this step is not difficult, let's realize in image how it's done!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jtmUfkv3XqlQ",
        "colab_type": "text"
      },
      "source": [
        "####Creation of dedicated images\n",
        "\n",
        "The following function allows creating images \"on demand\", in order to better understand the histogram computation. The parameter \"special\" indicate what type of image is required."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4RE2qpxGt9us",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "CREATE_IMAGE returns an image created based on a keyword. \n",
        "\"\"\"\n",
        "def create_image(height, width, special=None):\n",
        "\n",
        "    mat0 = np.ones((height, width), dtype=np.uint8)*255\n",
        "    mat1 = np.ones((height, width), dtype=np.uint8)*255\n",
        "    mat2 = np.ones((height, width), dtype=np.uint8)*255\n",
        "\n",
        "    if special == \"center_black\":\n",
        "        mat0[height//2-1:height//2+1, width//2-1 : width//2+1 ] = 0\n",
        "        mat1[height//2-1:height//2+1, width//2-1 : width//2+1 ] = 0\n",
        "        mat2[height//2-1:height//2+1, width//2-1 : width//2+1 ] = 0\n",
        "    elif special == \"center_gray\":\n",
        "        mat0[height//2-3:height//2+3, width//2-3 : width//2+3 ] = 125\n",
        "        mat1[height//2-3:height//2+3, width//2-3 : width//2+3 ] = 125\n",
        "        mat2[height//2-3:height//2+3, width//2-3 : width//2+3 ] = 125\n",
        "        mat0[height//2-1:height//2+1, width//2-1 : width//2+1 ] = 0\n",
        "        mat1[height//2-1:height//2+1, width//2-1 : width//2+1 ] = 0\n",
        "        mat2[height//2-1:height//2+1, width//2-1 : width//2+1 ] = 0\n",
        "    elif special == \"90\":\n",
        "        mat0[0:height, width//2 : width ] = 0\n",
        "        mat1[0:height, width//2 : width ] = 0\n",
        "        mat2[0:height, width//2 : width ] = 0\n",
        "    elif special == \"180\":\n",
        "        mat0[height//2:height, 0 : width ] = 0\n",
        "        mat1[height//2:height, 0 : width ] = 0\n",
        "        mat2[height//2:height, 0 : width ] = 0\n",
        "    elif special == \"135\":\n",
        "        for i in range(height):\n",
        "            for j in range(i,width):\n",
        "                mat0[i,j] = mat1[i,j] = mat2[i,j] = 0\n",
        "    elif special == \"45\":\n",
        "        for i in range(height):\n",
        "            for j in range(width-i-1,width):\n",
        "                mat0[i,j] = mat1[i,j] = mat2[i,j] = 0\n",
        "    elif special == \"28_34_37\":\n",
        "        mat0[4, 6:8] = 200\n",
        "        mat0[5, 4:8] = 150\n",
        "        mat0[6, 2:8] = 100\n",
        "        mat0[7, 0:8] = 50\n",
        "        mat1 = mat0.copy()\n",
        "        mat2 = mat0.copy()\n",
        "    elif special == \"up_01\":\n",
        "        mat0[4, 4:8] = 250\n",
        "        mat0[5, 0:8] = 50\n",
        "        mat0[6, 0:8] = 50\n",
        "        mat0[7, 0:8] = 50\n",
        "        mat1 = mat0.copy()\n",
        "        mat2 = mat0.copy()\n",
        "    elif special == \"up_10\":\n",
        "        mat0[4, 4:8] = 220\n",
        "        mat0[5, 0:8] = 50\n",
        "        mat0[6, 0:8] = 50\n",
        "        mat0[7, 0:8] = 50\n",
        "        mat1 = mat0.copy()\n",
        "        mat2 = mat0.copy()\n",
        "    elif special == \"up_11\":\n",
        "        mat0[4, 4:8] = 225\n",
        "        mat0[5, 0:8] = 50\n",
        "        mat0[6, 0:8] = 50\n",
        "        mat0[7, 0:8] = 50\n",
        "        mat1 = mat0.copy()\n",
        "        mat2 = mat0.copy()\n",
        "    elif special == \"up_15\":\n",
        "        mat0[4, 4:8] = 200\n",
        "        mat0[5, 0:8] = 50\n",
        "        mat0[6, 0:8] = 50\n",
        "        mat0[7, 0:8] = 50\n",
        "        mat1 = mat0.copy()\n",
        "        mat2 = mat0.copy()\n",
        "    elif special == \"up_27\":\n",
        "        mat0[4, 4:8] = 150\n",
        "        mat0[5, 0:8] = 50\n",
        "        mat0[6, 0:8] = 50\n",
        "        mat0[7, 0:8] = 50\n",
        "        mat1 = mat0.copy()\n",
        "        mat2 = mat0.copy()\n",
        "    elif special == \"up_152\":\n",
        "        mat0 = np.ones((height, width), dtype=np.uint8)*0\n",
        "        mat1 = np.ones((height, width), dtype=np.uint8)*0\n",
        "        mat2 = np.ones((height, width), dtype=np.uint8)*0\n",
        "        mat0[0,6:7] = mat1[0,6:7] = mat2[0,6:7] = 255\n",
        "        mat0[1,7] = mat1[1,7] = mat2[1,7] = 135\n",
        "    elif special == \"down_153\":\n",
        "        mat0 = np.ones((height, width), dtype=np.uint8)*255\n",
        "        mat1 = np.ones((height, width), dtype=np.uint8)*255\n",
        "        mat2 = np.ones((height, width), dtype=np.uint8)*255\n",
        "        mat0[0,6:7] = mat1[0,6:7] = mat2[0,6:7] = 0\n",
        "        mat0[1,7] = mat1[1,7] = mat2[1,7] = 125\n",
        "    elif special == \"up_141\":\n",
        "        mat0 = np.ones((height, width), dtype=np.uint8)*0\n",
        "        mat1 = np.ones((height, width), dtype=np.uint8)*0\n",
        "        mat2 = np.ones((height, width), dtype=np.uint8)*0\n",
        "        mat0[0,6:7] = mat1[0,6:7] = mat2[0,6:7] = 255\n",
        "        mat0[1,7] = mat1[1,7] = mat2[1,7] = 210\n",
        "    elif special == \"up_111\":\n",
        "        mat0 = np.ones((height, width), dtype=np.uint8)*0\n",
        "        mat1 = np.ones((height, width), dtype=np.uint8)*0\n",
        "        mat2 = np.ones((height, width), dtype=np.uint8)*0\n",
        "        mat0[0,0:7] = mat1[0,0:7] = mat2[0,0:7] = 100\n",
        "        mat0[1,7] = mat1[1,7] = mat2[1,7] = 255\n",
        "\n",
        "    image = np.dstack((mat0, mat1, mat2))\n",
        "    return image"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J-N7rgcjwvB7",
        "colab_type": "text"
      },
      "source": [
        "####Examples of histogram computed on created images\n",
        "In order to understand how the bins are fulled, let's look at a few of simple images. Those images are $8 \\times 8$, meaning 1 cell == 1 image\n",
        "\n",
        "* pure 90° gradient\n",
        "* pure 180° gradient\n",
        "* diagonal: 45°\n",
        "* diagonal: 135°\n",
        "\n",
        "For each case, we plot:\n",
        "- 1) the arrows representing the gradients, \n",
        "- 2) the matrices of the magnitude and orientation values, \n",
        "- 3) the resulting histograms\n",
        "\n",
        "**and** we log:\n",
        "\n",
        "- the raw values of the histogram bins\n",
        "- the normalized values of the histogram bins, using *L2-Normalization*:\n",
        "$\\begin{align}\n",
        "bins\\_values &= [x_1, x_2, x_3, x_4, x_5, x_6, x_7, x_8, x_9] \\\\\n",
        "\\|bins\\_values\\| &= \\sqrt{x_1^2 + x_2^2 + x_3^2 + x_4^2 + x_5^2 + x_6^2 + x_7^2 + x_8^2 + x_9^2 } \\\\\n",
        "bins\\_values_{normalized} &=  \\frac{v}{\\|v\\|} \\\\ \n",
        "&= \\Bigg[ \\frac{x_1}{\\|v\\|}, \\frac{x_2}{\\|v\\|}, \\frac{x_3}{\\|v\\|},  \\frac{x_4}{\\|v\\|}, \\frac{x_5}{\\|v\\|}, \\frac{x_6}{\\|v\\|},  \\frac{x_7}{\\|v\\|}, \\frac{x_8}{\\|v\\|}, \\frac{x_9}{\\|v\\|} \\Bigg]\n",
        "\\end{align}$\n",
        "\n",
        "\n",
        "\n",
        "#### Validation of intuition\n",
        "To prove ourselves our implementation and understanding is correct, we will confront the results with the infamous library `skimage`, using `skimage.feature.hog` with the same parameters as the handmade function: 9 bins, an $8\\times 8$ cell, and 1 cell per *block* (we discuss the *blocks* in the next section). Two parameters are still unknown: transform_sqrt and multichannel \n",
        "- `transform_sqrt`: if True, then the sqrt operator is applied to the global image first. It can give better results. We can safely leave it to False for the purpose of this exercices with the HOG bins...\n",
        "- `multichannel`: simply indicates if the image is grayscale (`multichannel = False`) or in color (`multichannel = True`) \n",
        "\n",
        "<!--Note: we briefly discussed the `block_norm` parameter, but more to come in the next step.-->\n",
        "\n",
        "####Finally...\n",
        "Coming back to the very first example of the HOG, we saw the Emma Stone picture with weird white-ish pictograms describing her face... Well, thanks to the `skimage` library, it's very easy to get this image, and we show it for the toy example we are studying now. It allows grabbing the full overview of how, eventually, the complete histogram and visualization is computed.\n",
        "\n",
        "> *Of course, don't hesitate to change yourself the list of images that are analyzed, considering the list implemented. You find the keywords accepted in the special list*\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zzqs9qmavSyr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "Creation of the images\n",
        "\"\"\"\n",
        "special = [\"up_01\",\"45\",\"90\", \"135\",\"180\",\"up_10\",\"up_11\",\"up_15\", \"up_27\", \"28_34_37\", \"up_111\", \"up_141\", \"up_152\", \"down_153\",\"center_black\", \"center_gray\"]\n",
        "list_as_example = [\"90\", \"180\", \"45\", \"135\", \"28_34_37\"]\n",
        "\n",
        "# created_img = create_image(cell_h,cell_w, \"45\")\n",
        "\n",
        "\"\"\"\n",
        "Homemade implementation of the histogram\n",
        "and\n",
        "Validation with an optimized library\n",
        "\"\"\"\n",
        "for keyword in list_as_example:\n",
        "    logging.info(\"Considering image: \" + str(keyword))\n",
        "\n",
        "    # creation of the simple image\n",
        "    created_img = create_image(cell_h, cell_w, keyword)\n",
        "\n",
        "    # creation of MyHog object\n",
        "    toyhog=MyHog(created_img)\n",
        "\n",
        "    # compute bins using MyHog\n",
        "    y_start_loc = 0\n",
        "    x_start_loc = 0\n",
        "    bins, bins_normalized = toyhog.compute_hog_bins(y_start_loc, x_start_loc, cell_h, cell_w, show_src=False, show_results=True, figsize = (14,4))\n",
        "\n",
        "    # compute bins using Skimage \n",
        "    fd, hog_image = skimage_feature_hog(created_img, \n",
        "                orientations=9, \n",
        "                pixels_per_cell=(8,8), \n",
        "                cells_per_block=(1, 1), \n",
        "                block_norm = \"L2\",\n",
        "                visualize=True, \n",
        "                transform_sqrt = False,\n",
        "                multichannel=True)\n",
        "\n",
        "    # plot results from Skimage\n",
        "    plt.figure()\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 4), sharex=False, sharey=False) \n",
        "\n",
        "    # Rescale hog for better display \n",
        "    hog_image_rescaled = exposure.rescale_intensity(hog_image, in_range=(0, 10)) \n",
        "\n",
        "\n",
        "    xlabels = [\"]70-90]\",\"]50-70]\", \"]30-50]\", \"]10-30]\",\"]10-\\n180]\",\"]150-\\n170]\",\"]130-\\n150]\",\"]110-\\n130]\", \"]90-\\n110]\"]\n",
        "    x = np.arange(9)\n",
        "    w = 0.2\n",
        "    ax1.bar( x-w, bins_normalized,  width=2*w, align='center',color=\"b\")\n",
        "    ax1.bar( x+w, fd, width=2*w, align='center',color=\"r\")\n",
        "   \n",
        "    ax1.set_title(\"Histogram computed by Skimage.feature.hog\")\n",
        "    ax1.legend([\"MyHog (homemade)\", \"Skimage.feature.hog\"])\n",
        "    # start, end = ax.get_xlim()\n",
        "    # ax.xaxis.set_ticks(np.arange(start, end, 1))\n",
        "    # ax1.set_xticklabels(xlabels)\n",
        "    ax1.set_xticks(x)\n",
        "    ax1.set_xticklabels(xlabels)\n",
        "\n",
        "    ax2.imshow(hog_image_rescaled, cmap=plt.cm.gray) \n",
        "    ax2.set_title('Histogram of Oriented Gradients - visual')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "\n",
        "    logging.info(\"MyHog bins     computed  : {}, {}, {}, {}, {}, {}, {}, {}, {}\".format(*np.round(bins,2)))\n",
        "    logging.info(\"MyHog bins   normalized  : {}, {}, {}, {}, {}, {}, {}, {}, {}\".format(*np.round(bins_normalized,2)))\n",
        "    logging.info(\"Skimage bins normalized  : {}, {}, {}, {}, {}, {}, {}, {}, {}\".format(*np.round(fd,2)))\n",
        "    logging.info(\"***\"*30)\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "co65Rf5VLdZ8",
        "colab_type": "text"
      },
      "source": [
        "> Notice: as a reminder, the purpose of the `MyHog` class (or any of the other class from this tutorial) is definitely not to reproduce exactly the behavior of a well-known and optimized library, but solely to break the magic behind using a library function without understanding the algorithm behind. As a result, the HOG computed may differ in several ways."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B5u575_gKGKy",
        "colab_type": "text"
      },
      "source": [
        "####Coming back to our initial cell...\n",
        "Emma Stone cell defined above can now be shown in terms of HOG, helped by the `skimage` library.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HH2aQX3NRd4P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "Retrieving the cell defined above\n",
        "\"\"\"\n",
        "cell_img=resized_img[y_start:y_start + cell_h, x_start:x_start+cell_w]\n",
        "\n",
        "\"\"\"\n",
        "computing HOG of the cell using same parameters\n",
        "\"\"\"\n",
        "fd, hog_image = skimage_feature_hog(cell_img, \n",
        "                    orientations=9, \n",
        "                    pixels_per_cell=(8,8), \n",
        "                    cells_per_block=(1, 1), \n",
        "                    block_norm = \"L2\",\n",
        "                    visualize=True, \n",
        "                    transform_sqrt = False,\n",
        "                    multichannel=True)\n",
        "\n",
        "\n",
        "\n",
        "plt.figure()\n",
        "\n",
        "fig, (ax0, ax1, ax2) = plt.subplots(1, 3, figsize=(12, 4), sharex=False, sharey=False) \n",
        "# hog_cropped = hog_image[y_start:y_start + cell_h, x_start:x_start+cell_w]\n",
        "ax0.imshow(cv2.cvtColor(resized_img, cv2.COLOR_BGR2RGB))\n",
        "ax0.set_title('Input image') \n",
        "\n",
        "ax1.imshow(cv2.cvtColor(cell_img, cv2.COLOR_BGR2RGB))\n",
        "ax1.set_title('Extracted cell') \n",
        "\n",
        "# Rescale histogram for better display \n",
        "hog_image_rescaled = exposure.rescale_intensity(hog_image, in_range=(0, 10)) \n",
        "ax2.imshow(hog_image_rescaled, cmap=plt.cm.gray) \n",
        "ax2.set_title('Histogram of Oriented Gradients - rescaled')\n",
        "# print(hog_image_rescaled)\n",
        "plt.show()\n",
        "\n",
        "logging.info(\"Skimage bins normalized  : {}, {}, {}, {}, {}, {}, {}, {}, {}\".format(*np.round(fd,2)))\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jiAjftDx2Nxm",
        "colab_type": "text"
      },
      "source": [
        "This is the end of the Step3: the computation of the histogram for one cell! A careful eye will have seen the parameters `cells_per_block` and `block_norm` of the library method. This is linked to Step4: Block Normalization!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OXLInz6551Ak",
        "colab_type": "text"
      },
      "source": [
        "###HOG How-to, Step4: Block normalization\n",
        "\n",
        "In the Step3, we have extensively seen how to compute the histogram of gradients for a cell. We are almost at the end of the feature representation build up, but there are yet one normalization step. \n",
        "> In the previous step, we actually already normalized the histogram values using *L2-Normalization*. This is a simple case of the Block normalization where there is 1 cell per block. In general, we can define to perform Block normalization on more than 1 block. A common value is 4, as discussed in the [introducing paper](http://lear.inrialpes.fr/people/triggs/pubs/Dalal-cvpr05.pdf). \n",
        "\n",
        "####<u>Why do we need normalization ?</u>\n",
        " \n",
        "When normalized, a histogram becomes independant from the lighting variation. \n",
        "Indeed, illumination has the impact of increasing/decreasing the values of the pixels in a cell. \n",
        "\n",
        "Using normalization, a change on the pixel value has no impact if all the pixels in a cell are subject to the same change. \n",
        "> let's say a low illumination make the pixel values divided by two. Having a normalized histogram on the cell will not be affected by such a change, as in the end, the absolute value is not important: only the relative value of one pixel to others matter. This is the very essence of the normalization.\n",
        "\n",
        "As a result, normalizing the histogram makes it quite independant of the lighting condition (provided that in a cell, all the pixels have the same lighting condition, which seems a sensible assumption).\n",
        "\n",
        "####<u>Normalizing by block</u> \n",
        "A nice visualization of the normalization by block of multiple cells is given in [learnopencv](https://www.learnopencv.com/wp-content/uploads/2016/12/hog-16x16-block-normalization.gif) where we see in green the different cells, and in blue a block of 4 cells. \n",
        "Using a block normalization - so, normalizing multiple cells at ones, and slide the normalization window across the image - is introduced in the [introducing paper](http://lear.inrialpes.fr/people/triggs/pubs/Dalal-cvpr05.pdf) which shows some benefits in terms of missing rate. Typically, 4 cells per blocks is often used. In a later section (see Classification), a grid search tends to try out other block sizes.\n",
        "\n",
        "####<u>What normalization ?</u>\n",
        "Several normalization can be considered: *L1*, *L2*, *L2-Hys*, ...\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jc4AH7fJp63w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "Definition of the block size (in number of px)\n",
        "1 cell = 8 x 8\n",
        "1 block => 16 x 16 => 4 cells\n",
        "\"\"\"\n",
        "block_w = 16\n",
        "block_h = 16\n",
        "\n",
        "x_cells = np.arange(0,64,8)\n",
        "y_cells = np.arange(0,64,8)\n",
        "\n",
        "# credit: https://stackoverflow.com/questions/44816682/drawing-grid-lines-across-the-image-uisng-opencv-python\n",
        "def draw_grid(img, line_color=(0, 255, 0), thickness=1, type_=8, pxstep=8):\n",
        "    '''(ndarray, 3-tuple, int, int) -> void\n",
        "    draw gridlines on img\n",
        "    line_color:\n",
        "        BGR representation of colour\n",
        "    thickness:\n",
        "        line thickness\n",
        "    type:\n",
        "        8, 4 or cv2.LINE_AA\n",
        "    pxstep:\n",
        "        grid line frequency in pixels\n",
        "    '''\n",
        "    x = pxstep\n",
        "    y = pxstep\n",
        "    while x < img.shape[1]:\n",
        "        cv2.line(img, (x, 0), (x, img.shape[0]), color=line_color, lineType=type_, thickness=thickness)\n",
        "        x += pxstep\n",
        "\n",
        "    while y < img.shape[0]:\n",
        "        cv2.line(img, (0, y), (img.shape[1], y), color=line_color, lineType=type_, thickness=thickness)\n",
        "        y += pxstep\n",
        "\n",
        "def draw_one_block(img, origin=(0,0), line_color=(255,0, 0), block_size = 16, thickness=1, type_=8):\n",
        "    cv2.rectangle(img, origin, (origin[0]+block_size, origin[1]+block_size), line_color, thickness=thickness, lineType =type_)\n",
        "\n",
        "def draw_all_blocks(img, thickness):\n",
        "    color_list = [(255,0,0), (255,255,0), (255,0,255)]\n",
        "    x = 0\n",
        "    y = 0\n",
        "    counter = 0\n",
        "    while x < img.shape[1]-8:\n",
        "        # cv2.line(img, (x, 0), (x, img.shape[0]), color=line_color, lineType=type_, thickness=thickness)\n",
        "        # draw_one_block(img, (x,y))\n",
        "        \n",
        "        \n",
        "        while y < img.shape[0]-8:\n",
        "            # cv2.line(img, (0, y), (img.shape[1], y), color=line_color, lineType=type_, thickness=thickness)\n",
        "            lc = color_list[counter%3]\n",
        "            draw_one_block(img, (x,y),line_color=lc, thickness=thickness)\n",
        "            counter += 1\n",
        "            y += 8\n",
        "        y=0\n",
        "        x += 8\n",
        "    return counter\n",
        "\n",
        "\"\"\"\n",
        "creating a green grid covering the resized image\n",
        "\"\"\"\n",
        "\n",
        "grid_cells_img = resized_img.copy()\n",
        "draw_grid(grid_cells_img, type_=8)\n",
        "\n",
        "\"\"\"\n",
        "Creating the three first blocks\n",
        "\"\"\"\n",
        "\n",
        "first_block_img = grid_cells_img.copy()\n",
        "second_block_img = grid_cells_img.copy()\n",
        "third_block_img = grid_cells_img.copy()\n",
        "\n",
        "draw_one_block(first_block_img, origin=(0,0), line_color=(255,0,0), thickness=2)\n",
        "draw_one_block(second_block_img, origin=(8,0), line_color=(255,255,0),thickness=2)\n",
        "draw_one_block(third_block_img, origin=(16,0), line_color=(255,0,255),thickness=2)\n",
        "\n",
        "\"\"\"\n",
        "Creating all the blocks on top of the cells\n",
        "\"\"\"\n",
        "\n",
        "blocks_img = grid_cells_img.copy()\n",
        "counter = draw_all_blocks(blocks_img, thickness=1)\n",
        "\n",
        "\"\"\"\n",
        "Vizualization\n",
        "\"\"\"\n",
        "\n",
        "fig, (ax0, ax1, ax2, ax3) = plt.subplots(1, 4, figsize=(16, 4), sharex=False, sharey=False) \n",
        "ax0.imshow(cv2.cvtColor(grid_cells_img, cv2.COLOR_BGR2RGB))\n",
        "ax0.set_title('Cells') \n",
        "\n",
        "ax1.imshow(cv2.cvtColor(first_block_img, cv2.COLOR_BGR2RGB))\n",
        "ax1.set_title('first block') \n",
        "\n",
        "ax2.imshow(cv2.cvtColor(second_block_img, cv2.COLOR_BGR2RGB))\n",
        "ax2.set_title('second block') \n",
        "\n",
        "ax3.imshow(cv2.cvtColor(third_block_img, cv2.COLOR_BGR2RGB))\n",
        "ax3.set_title('third block') \n",
        "\n",
        "fig, ax = plt.subplots(1,1,figsize=(4,4))\n",
        "ax.imshow(cv2.cvtColor(blocks_img, cv2.COLOR_BGR2RGB))\n",
        "ax.set_title(\"All Blocks\")\n",
        "\n",
        "plt.show()\n",
        "\n",
        "logging.info(\"In total, there are: \" + str(counter) + \" blocks possible in the picture\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1LybYTHG0fxi",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "As shown in the previous example, on the image chosen, there are $49$ blocks possible of size $(16 \\times 16)$ pixels.\n",
        "\n",
        "###HOG How-to, Step5: concatenation\n",
        "\n",
        "After the normalization, the only step remaining is the concatenation of the computed vectors into a larger one, that represent the input image. This will be the feature representation of the image, based on the *oriented* gradients in that image.\n",
        "\n",
        "###What size is this feature representation ?\n",
        "- one cell is represented by $9$ numbers (histogram)\n",
        "- four histograms are normalized together, leading to a $(36,1)$ vector\n",
        "- there are $49$ such vectors representing the image.\n",
        "    If the image as a width of size $(w*8)$ pixels, and a height of $(h*8)$ pixels, the image dimension is $(8*h \\times  8*w)$. In such an image, they are :\n",
        "    *   h cells vertically, and w cells horizontally,\n",
        "    *  (h-1) blocks vertically and (w-1) blocks horizontally.\n",
        "\n",
        "The length of the final vector is then $36 \\cdot 49$ numbers, or a $(1764,1)$ vector.\n",
        "\n",
        "Of course, this is still large... But much more compact that our initial $(64,64,3)$ array of $12288$ numbers\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eXffH7oz8GEJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fd, hog_image = skimage_feature_hog(resized_img, \n",
        "                    orientations=9, \n",
        "                    pixels_per_cell=(8,8), \n",
        "                    cells_per_block=(2, 2), \n",
        "                    block_norm = \"L2\",\n",
        "                    visualize=True, \n",
        "                    transform_sqrt = True,\n",
        "                    multichannel=True)\n",
        "\n",
        "\"\"\"\n",
        "Plotting results\n",
        "\"\"\"\n",
        "\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 8), sharex=True, sharey=True) \n",
        "ax1.imshow(cv2.cvtColor(resized_img, cv2.COLOR_BGR2RGB))\n",
        "ax1.set_title('Input image') \n",
        "\n",
        "# Rescale histogram for better display \n",
        "hog_image_rescaled = exposure.rescale_intensity(hog_image, in_range=(0, 10)) \n",
        "ax2.imshow(hog_image_rescaled, cmap=plt.cm.gray) \n",
        "ax2.set_title('Histogram of Oriented Gradients - rescaled')\n",
        "logging.debug(\"HOG Rescaled: \" + str(hog_image_rescaled.min()) + \" -> \" + str(hog_image_rescaled.max()) )\n",
        "\n",
        "plt.show()\n",
        "\n",
        "# logging.debug(hog_image_rescaled.shape)\n",
        "logging.info(\"Shape of the HOG feature : \" + str(fd.shape))\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GuWXw_8xuTWb",
        "colab_type": "text"
      },
      "source": [
        "<!--histogram construction is based on the gradient computed - both magnitude and orientation (as defined)\n",
        "\n",
        "- the bin is selected according to the orientation (direction) of the gradient; \n",
        "- the value that goes in the bin is based on the magnitude\n",
        "\n",
        "For instance on the toy example:\n",
        "first pixel has:\n",
        "* mag = 6; orn = 45°. So, the vote of this pixel goes for 75% in the bin of 40°, and 25% in the bin of 60°, as closer to 40°. As a result, we add 4.5 to bin nb 3, and 1.5 to bin nb 4. -->"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "erQwBkq35RQ-",
        "colab_type": "text"
      },
      "source": [
        "###HOG: Exec summary\n",
        "\n",
        "* one cell (typ 8 x 8) of an image is represented by a histogram \n",
        "    * the orientation and magnitude of the gradient are computed on each pixel\n",
        "    * orientation of the gradient indicate a bin\n",
        "    * magnitude indicate the amount to place into the bin\n",
        "* the histogram is a vector of size 9 (as 9 bins)\n",
        "* one block (16 x 16) histogram is the concatenation of the 4 histograms, each representing one cell of the block, hence represented by a (36 x 1) vector.\n",
        "* the final HOG feature vector is based on the concatenation of all blocks.\n",
        "\n",
        "\n",
        "For an image of 64 x 64, it follows:\n",
        "* 8 cells along the width\n",
        "* 8 cells along the height\n",
        "* Number of cells = 64 cells\n",
        "* Number of blocks = 7 * 7 = 49 blocks\n",
        "\n",
        "Each block has a representative vector of dimension $(36 \\times 1)$, and the resulting vector has dimension $(49*36 \\times 1)$, or $(1764,)$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tfMDFGqsZvzC",
        "colab_type": "text"
      },
      "source": [
        "This ends the first part of Histogram of Oriented Gradient, where I showed in detail how to compute such a feature representation, and the meaning of the different parameters.\n",
        "\n",
        "As many other parameters, the \"hyper-parameters\" of a HOG method should always be assessed according to a specific problem. \n",
        "- Insofar, the parameters have mainly be considered equal to their suggested value by the litterature, in this [HOG introduction](http://lear.inrialpes.fr/people/triggs/pubs/Dalal-cvpr05.pdf)\n",
        "- Later, in this tutorial, we will optimize a classifier using a systematic method and a cross-validation set. \n",
        "\n",
        "Recall that we shall **never** use the test set to fine tune our model parameters."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NtYBaU-bzkio",
        "colab_type": "text"
      },
      "source": [
        "<!--Still to do : \n",
        " These [hog] parameters were obtained by experimentation and examining the accuracy of the classifier — you should expect to do this as well whenever you use the HOG descriptor. Running experiments and tuning the HOG parameters based on these parameters is a critical component in obtaining an accurate classifier.-->"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "geHORWA5mMdM",
        "colab_type": "text"
      },
      "source": [
        "### Detecting an object of interest in a new image\n",
        "\n",
        "In this part, the goal is to use the HOG feature descriptor in order to detect if an object is present or not in a new image.\n",
        "\n",
        "> we are not building a classifier or identificator (yet!) The goal is *just* to detect which region of an image have structures that correspond well to the ones of the feature descriptor.\n",
        "\n",
        "\n",
        "To do so, the steps are:\n",
        "1. Select an object\n",
        "2. Compute its HOG feature description\n",
        "3. Select a new image\n",
        "4. pre-process this image in terms of dimensions\n",
        "5. compute the image HOG at every location\n",
        "6. Assess the matching between the descriptor and the image\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pDIZoHwMn_o-",
        "colab_type": "text"
      },
      "source": [
        "####Select an object and compute its hog\n",
        "\n",
        "As we have built a training set and a test set, let's just pick randomly one image of the training set. We can do even better and compute the HOG for all images in training set using the parameters already seen. We store the results in the dictionary `hog_training`. As *usual*, the keys are the persons names.\n",
        "\n",
        "Later, we can select any of those as the `image_of_interest`\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "guoG2jWm4Dv3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "hog_training = {}\n",
        "\n",
        "for person in persons:\n",
        "    hog_training[person] = []\n",
        "    for src_img in training_set[person]:\n",
        "        resized_img = cv2.resize(src_img, (sq_size,sq_size))\n",
        "        fd, hog_image = skimage_feature_hog(resized_img, \n",
        "                                            orientations=9, \n",
        "                                            pixels_per_cell=(8,8), \n",
        "                                            cells_per_block=(2, 2), \n",
        "                                            block_norm = \"L2\",\n",
        "                                            visualize=True, \n",
        "                                            transform_sqrt = True,\n",
        "                                            multichannel=True)\n",
        "        hog_training[person].append((fd, hog_image, resized_img))\n",
        "\n",
        "logging.debug(pretty_return_dict_size(hog_training))\n",
        "logging.info(\"hog_training dictionary contains the HOG descriptors (resized) for all faces of the training set\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KUtgffa1HZey",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "Let's pick an image of interest, by its index [0 -> 19]\n",
        "\"\"\"\n",
        "\n",
        "idx_of_interest = 2\n",
        "image_of_interest = training_set[personA][idx_of_interest]\n",
        "hog_of_interest = hog_training[personA][idx_of_interest]\n",
        "\n",
        "\"\"\"\n",
        "Visualization of the image and its hog selected as image_of_interest\n",
        "\"\"\"\n",
        "fig, (ax0, ax1) = plt.subplots(1,2,figsize = (8,4), sharex=False, sharey=False)\n",
        "\n",
        "ax0.imshow(cv2.cvtColor(image_of_interest, cv2.COLOR_BGR2RGB))\n",
        "ax0.set_title(\"Image of interest from training set\")\n",
        "\n",
        "ax1.imshow(hog_of_interest[1])\n",
        "ax1.set_title(\"Visualization of the HOG of interest\")\n",
        "\n",
        "logging.info(\"Shape of the descriptor       : \" + str(hog_of_interest[0].shape))\n",
        "logging.info(\"Shape of the descriptor (visu): \" + str(hog_of_interest[1].shape))\n",
        "logging.info(\"Shape of the image of interest: \" + str(image_of_interest.shape))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WnlkuONE-Qcm",
        "colab_type": "text"
      },
      "source": [
        "####Select a new image\n",
        "This is the image we want to apply the descriptor matching.\n",
        "Put in another words, we want to verify, using a distance metric such as the euclidean distance, if the HOG descriptor of my *image_of_interest* presents similarities with a region in a new image.\n",
        "\n",
        "Let's just pick a certain number of images from our **original** images downloaded, the ones that are not cropped yet, nor resized. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hBs2r9_HEWEt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "person_ = personA # can be {personA, personB, personC, personD}\n",
        "number_of_candidates = 3\n",
        "random.seed(\"7/4/2020\")\n",
        "images_candidates = random.sample(images[person_], number_of_candidates)\n",
        "\n",
        "fig = plt.figure(figsize=(12,4))\n",
        "i=0\n",
        "\n",
        "for img in images_candidates:\n",
        "    ax=fig.add_subplot(1,number_of_candidates, i+1)\n",
        "    ax.imshow( cv2.cvtColor(img, cv2.COLOR_BGR2RGB) )\n",
        "    i+=1\n",
        "plt.show()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l89dOjWmEXGU",
        "colab_type": "text"
      },
      "source": [
        "The different images selected don't have the same size."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c3PgFiwYIrQf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "logging.info(\"Shapes of image on which to look for the image_of_interest: \")\n",
        "for img in images_candidates:\n",
        "    logging.info(\"Image shape: \" + str(img.shape))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9yFQMKcxKMhf",
        "colab_type": "text"
      },
      "source": [
        "####Matching\n",
        "In this larger step, we want to see if there is a match between the descriptor of interest and the image candidate. \n",
        "\n",
        "Several problems arise:\n",
        "- The descriptor of interest has a fixed (designed) size of $(1764,1)$\n",
        "- The different image candidates have different sizes,\n",
        "- The image candidates contains more information than just a face\n",
        "\n",
        "*Why not just cropping the faces on the image and resize it?*\n",
        "\n",
        "We could of course do that - but that is not really the purpose :-) Rather, we want to assess, in **every location** of the image candidate, if there is a chance to see a pattern such as the descriptor provided of the image of interest.\n",
        "\n",
        "*Simpler case*\n",
        "Let's consider first that if the image candidate contains a face (=object), this face has approximately the same size as the original face of interest\n",
        "\n",
        "One way to proceed is to slide, across the image candidate, a window of the size of the object to detect. \n",
        "- at *each* location, crop the part of the image candidate in the sliding window\n",
        "> sliding at every each location is cumbersome and time consuming. The parameter `step`actually defines the amount of pixels that is skipped in both directions during the sliding.\n",
        "- compute the HOG of this crop\n",
        "- compare (using Euclidean distance for instance) this descriptor with the descriptor of the object to find\n",
        "- go to the next location\n",
        "\n",
        "At the end, the result is a score attributed to every location, indicating the correspondance between the object to detect, and the area in the image. \n",
        "\n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IqjJB-ImN0I7",
        "colab_type": "text"
      },
      "source": [
        "*Helper functions*\n",
        "- `get_local_crop`: realize the cropping before computing the HOG, ensuring the appropriate size to compute HOG\n",
        "- `match_hog`: homemade matching between an object of interest and an image candidate, implementing this \"sliding\" accross the image\n",
        "- `fill_matrix_min_neighbours`: compensate the effect of the `step`parameter in the plot"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "55hZpBQoGLdD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_local_crop(src_img, center_pixel, crop_shape, show = False):\n",
        "    \"\"\"\n",
        "    src_img:\n",
        "    center_pixel:\n",
        "    crop_shape:\n",
        "    \"\"\"\n",
        "    crop_height =  crop_shape[0]\n",
        "    crop_width = crop_shape[1]\n",
        "    top_= center_pixel[0] - crop_height // 2\n",
        "    bottom_ = center_pixel[0] + crop_height // 2\n",
        "    left_ = center_pixel[1] - crop_width // 2  \n",
        "    right_ = center_pixel[1] + crop_width // 2\n",
        "    if len(src_img.shape)> 2:\n",
        "        crop = src_img[ top_ : bottom_, left_:right_, :]\n",
        "    else:\n",
        "        crop = src_img[ top_ : bottom_, left_:right_]\n",
        "\n",
        "    if show:\n",
        "        cv2_imshow(crop)\n",
        "    return crop\n",
        "\n",
        "def match_hog(src_img, hog_desc, original_face_shape, win_stride = (8,8), show = False):\n",
        "    \"\"\"\n",
        "    src_img : image to analyse\n",
        "    hog_desc: (fd, hog_image) of the corresponding faces_cropped\n",
        "    original_face_shape: shape of the face used for hog_desc computation\n",
        "    \"\"\"\n",
        "    height = src_img.shape[0] # height of the image to analyze\n",
        "    width  = src_img.shape[1] # width of the image to analyze\n",
        "    \n",
        "    height_face = original_face_shape[0]\n",
        "    width_face = original_face_shape[1]\n",
        "\n",
        "    res_shape = (src_img.shape[0], src_img.shape[1])\n",
        "    res = np.ones(res_shape)*-1\n",
        "\n",
        "    if show:\n",
        "        tmp_image = src_img.copy()\n",
        "        cv2.rectangle(tmp_image, (width_face//2, height_face//2), (width - width_face//2, height - height_face//2), (0, 255, 0))\n",
        "        cv2_imshow(tmp_image)\n",
        "\n",
        "    running_h_idx = range(height_face //2, height - height_face//2+1, win_stride[0])\n",
        "    running_w_idx = range(width_face //2, width - width_face//2+1, win_stride[1])\n",
        "\n",
        "    for h_idx in running_h_idx:\n",
        "        for w_idx in running_w_idx:\n",
        "            local_crop = get_local_crop(src_img, (h_idx, w_idx), original_face_shape, False)\n",
        "            local_resized_img = cv2.resize(local_crop, (64,64))\n",
        "            local_fd = skimage_feature_hog(local_resized_img, \n",
        "                                            orientations=9, \n",
        "                                            pixels_per_cell=(8,8), \n",
        "                                            cells_per_block=(2, 2), \n",
        "                                            block_norm = \"L2\",\n",
        "                                            visualize=False, \n",
        "                                            transform_sqrt = True,\n",
        "                                            multichannel=True)\n",
        "\n",
        "            # computing euclidean distance here !!            \n",
        "            res[h_idx,w_idx]= np.linalg.norm(local_fd-hog_desc[0])\n",
        "            if show:\n",
        "                cv2_imshow(local_resized_img)\n",
        "\n",
        "    return res\n",
        "\n",
        "def fill_matrix_min_neighbours(matrx, win_stride = (16,16), margin = 0):\n",
        "    \"\"\"\n",
        "    fill the gaps in the  computation by taking the min values from closest neighbours\n",
        "    that were computed.\n",
        "    \"\"\"\n",
        "    l = np.argwhere(matrx != -1)\n",
        "    res = matrx.copy()\n",
        "\n",
        "    if len(l)<2:\n",
        "        idx_to_change = res == -1\n",
        "        logging.warning(\"len < 2 --> len(idx_to_change) = \" + str(len(idx_to_change)))\n",
        "        res[idx_to_change] = res.max() + margin\n",
        "        return res\n",
        "\n",
        "    top_left_corner = l[0]\n",
        "    bottom_right_corner = l[-1]\n",
        "\n",
        "    for i in range(top_left_corner[0], bottom_right_corner[0]+win_stride[0]//2):\n",
        "        for j in range(top_left_corner[1],bottom_right_corner[1]+win_stride[1]//2):\n",
        "            if matrx[i,j] == -1:\n",
        "                local_roi = get_local_crop(matrx, (i,j),win_stride)\n",
        "                cand = local_roi[ local_roi != -1 ]\n",
        "                res[i,j] = cand.min()\n",
        "    idx_to_change = res == -1\n",
        "    res[idx_to_change] = res.max() + margin\n",
        "    return res"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L07yM3Z15tJN",
        "colab_type": "text"
      },
      "source": [
        "*Details*\n",
        "\n",
        "For all the image in images_candidates list, \n",
        "- compute the matching on every required location (point spaced by win_stride)\n",
        "- fill the non computed point with min neighbour\n",
        "- normalized (L2) to get values between 0 -> 1\n",
        "- show the results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L06xUrOubbu_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for img in images_candidates:\n",
        "    win_stride=(16,16)\n",
        "    res = match_hog(img, hog_of_interest, image_of_interest.shape, win_stride)\n",
        "    new_res = fill_matrix_min_neighbours(res, win_stride)\n",
        "\n",
        "    logging.debug(\"worst match hog results: \" + str(new_res.max()))\n",
        "    logging.debug(\"best match hog results: \" + str(new_res.min()))\n",
        "\n",
        "    b = new_res.copy()\n",
        "    bmax, bmin = b.max(), b.min()\n",
        "    if bmax == bmin and bmax == 0:\n",
        "        logging.info(\"Perfect match!\")\n",
        "    b = (b - bmin)/(bmax - bmin)\n",
        "    # b = (b - bmin)/(bmax)\n",
        "\n",
        "    logging.debug(\"worst match hog results normalized [expexted 1]: \" + str(b.max()))\n",
        "    logging.debug(\"best match hog results [expected 0]: \" + str(b.min()))\n",
        "\n",
        "\n",
        "\n",
        "    fig, (ax1, ax2) = plt.subplots(1,2 , figsize=(16, 8), sharex=True, sharey=True) \n",
        "\n",
        "    ax1.imshow(cv2.cvtColor(img,cv2.COLOR_BGR2RGB))\n",
        "    ax1.set_title(\"Image where to find base face\")\n",
        "\n",
        "    # ax2.imshow(new_res, cmap=\"jet\")\n",
        "    # ax2.set_title(\"Results gaps filled with min neighbour\")\n",
        "\n",
        "    ax2.imshow(b, cmap=\"jet\")\n",
        "    ax2.set_title(\"Normalized results of Matching\")\n",
        "\n",
        "    plt.show()\n",
        "    logging.debug(\"=====================================================\")\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UtorNRBtUiEr",
        "colab_type": "text"
      },
      "source": [
        "####Matching - part2\n",
        "\n",
        "As showed in the results above, it works ! Several observations nonetheless:\n",
        "1. the borders are red (indicating  large distance): this is because of the sliding that considers the full object of interest is required in the image\n",
        "\n",
        "2. It works even if not the exact same shape!\n",
        "    * provided that a threshold is chosen, one could use this to recognize an object\n",
        "    * only *LOCAL* description is given: only local object shape and appearances (to some extend) can be represented. \n",
        "    * Because of locality and limit of expressiveness, the face of Bradley Cooper and Emma Stone may look alike, in terms of HOG descriptors.\n",
        "\n",
        "3. Only *ONE* size of the object is verified. If the object to find is of the same size as the object in the image, the descriptors will be very similar, and the distance small. However, if the object to find is smaller or larger in the image candidate, the HOG descriptor won't match at all. \n",
        "To solve this problem, one can perform multiscale detection, where the object is scaled several times to ensure to really detect the object, if it is present.\n",
        "\n",
        "A nice overview of this multiscaling can be found in [pyimagesearch](https://www.pyimagesearch.com/2015/11/16/hog-detectmultiscale-parameters-explained/)\n",
        "\n",
        "4. The detection of object with this sliding window takes time, and is even more time / resource consuming if used with multiscale analysis. Several techniques should be set in place (also discussed in [pyimagesearch](https://www.pyimagesearch.com/2015/11/16/hog-detectmultiscale-parameters-explained/)) such as:\n",
        "    - reducing the size of the image candidate, without losing too much information\n",
        "    - adjusting the HOG parameters so that the computation time is optimized for a **specific application**\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BbGWtNSulkEt",
        "colab_type": "text"
      },
      "source": [
        "###HOG Conclusion\n",
        "\n",
        "In this first feature representation building, we studied in details how the descriptor is built and computed, and the different parameters that comes in play, and in particular\n",
        "- the cell size\n",
        "- the block size\n",
        "- the normalization method\n",
        "\n",
        "We have then used the HOG descriptor to try and detect if an object is in an image, by computing on (almost) every position of the image the descriptor and assessing the distance (as similarity metric) with the face of interest descriptor.\n",
        "Doing so, we have then seen that the techniques works well to find region of a similar shape in the image, hence the locality of the descriptor. We also covered some of the challenges related to the images'size, locality of the descriptors, and complexity of the computations.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FMBCO6xyhskK",
        "colab_type": "text"
      },
      "source": [
        "##  Principal Component Analysis - **PCA**\n",
        "\n",
        "We have extensively covered HOG. Similarly, we will go through the PCA technique. First, we will give some intuition; then we will go through the maths, as this technique rely heavily on the computing, then we will apply PCA on the training set and try and observe some results.\n",
        "\n",
        "When discussing PCA, we will mainly focus of the technique applied on images. There are plenty of blogs out there precisely defining and tailoring the techniques for all kinds of application. This tutorial is just an example of PCA applied to face images.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "As for the previous HOG technique, homemade code is fully provided. This gives the details on the implementation and insight about how things are calculated. For this part of the tutorial, most of the examples are done with this homemade code. A section is dedicated to a demo of a library tool as well. For the classification and identification parts, however, library optimized code will be used as it's the purpose of those libraries. \n",
        "Don't hesitate to try out different parameters in the provided function, and please report any bug to geoffroy.herbin@student.kuleuven.be\n",
        "\n",
        "---\n",
        "\n",
        "- How to convert my image (you can work in color if you like) dataset to a 2D matrix?\n",
        "- Can you exploit the dimensionality of this data matrix to make your computations more effective?\n",
        "- Mean subtraction or not?\n",
        "- Shall we use eigenvalue or singular value decomposition?\n",
        "- How many non-zero eigenvalues/singular values should we have?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Rr9R4fwgtK_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MyPCA:\n",
        "    \"\"\"\n",
        "    homemade class to perform PCA using several methods and compare\n",
        "    Three methods are implemented\n",
        "    - method = \"svd\": singular value decomposition technique\n",
        "    - method = \"eigen\": nominal eigenvalue decomposition technique is implemented\n",
        "    - method = \"eigen_fast\": eigenvalue decomposition using dimensionality \n",
        "                            reduction is implemented\n",
        "    \"\"\"\n",
        "    def __init__(self, method = \"svd\"):\n",
        "        self.method = method\n",
        "        self.eigenvalues = None\n",
        "        self.eigenvectors = None\n",
        "        self.X_mean = None\n",
        "\n",
        "    def fit(self, data):\n",
        "        X = data.copy()\n",
        "        n, m = X.shape\n",
        "        assert n < m, \"n is not smaller than m -> you most likely need \" + \\\n",
        "                    \"to transpose your input data\"\n",
        "        self.X_mean = np.mean(X, axis = 0)\n",
        "        X -= self.X_mean\n",
        "\n",
        "        self.eigenvalues = None\n",
        "        self.eigenvectors = None\n",
        "\n",
        "        if self.method == \"svd\":\n",
        "            # singular value decomposition\n",
        "            U, S, Vt = np.linalg.svd(X)\n",
        "            self.eigenvalues = S**2 / (n - 1)\n",
        "            self.eigenvectors = Vt.T[:,0:n]\n",
        "\n",
        "        elif self.method == \"eigen_fast\":\n",
        "\n",
        "            # compute small covariance matrix\n",
        "            D = np.dot(X, X.T) / (n - 1)\n",
        "\n",
        "            # eigen decomposition\n",
        "            LD, W = np.linalg.eig(D)\n",
        "\n",
        "            order_D = np.argsort(LD)[::-1]\n",
        "            LD_sorted = LD[order_D]\n",
        "            W_sorted = W[:,order_D]\n",
        "            \n",
        "            eigenVectors_sorted_tmp = np.dot(X.T, W_sorted)\n",
        "            eigenVectors_sorted = np.empty(eigenVectors_sorted_tmp.shape)\n",
        "            for i in range(n):\n",
        "                v = eigenVectors_sorted_tmp[:,i]\n",
        "                eigenVectors_sorted[:,i] = v / np.linalg.norm(v)\n",
        "            \n",
        "            self.eigenvalues = LD_sorted\n",
        "            self.eigenvectors = eigenVectors_sorted\n",
        "                \n",
        "        elif self.method ==\"eigen\":\n",
        "            # compute covariance matrix\n",
        "\n",
        "            Cov = np.dot(X.T, X) / (n - 1)\n",
        "\n",
        "            # eigen decomposition\n",
        "            LC, V =np.linalg.eig(Cov)\n",
        "\n",
        "            # sort in appropriate order and keep only relevant component\n",
        "            order = np.argsort(LC)[::-1]\n",
        "            LC_sorted = LC[order][0:n]\n",
        "            V_sorted = V[:,order][:,0:n]\n",
        "\n",
        "            self.eigenvalues = LC_sorted\n",
        "            self.eigenvectors = V_sorted.real\n",
        "        else:\n",
        "            raise RuntimeError(\"method value unknown\")\n",
        "        \n",
        "    def projectPC(self, X, k):\n",
        "        Vk = self.eigenvectors[:, 0:k]\n",
        "        # logging.debug(\"Reduce X \" + str(X.shape) + \"using k = \" + str(k) + \" components\")\n",
        "        # logging.debug(\"Vk (4096 x k)= \" + str(Vk.shape))\n",
        "        X_reduced = np.dot(X, Vk)\n",
        "        # logging.debug(\"X_reduced (n x k) = \" + str(X_reduced.shape))\n",
        "\n",
        "        return X_reduced\n",
        "    \n",
        "    def reconstruct(self, X_reduced, k, show = False):\n",
        "        if len(X_reduced.shape) > 1:\n",
        "            X_reduced = X_reduced[:,0:k]\n",
        "        else:\n",
        "            X_reduced = X_reduced[0:k]\n",
        "        \n",
        "        Vkt = self.eigenvectors[:, 0:k].T\n",
        "        # logging.debug(\"Reduce using X_reduced = \" + str(X_reduced.shape) )\n",
        "        # logging.debug(\"Reconstruct using k = \" + str(k) + \" components\")\n",
        "        # logging.debug(\"Vkt (k x 4096)= \" + str(Vkt.shape))\n",
        "        X_hat_centered = np.dot(X_reduced, Vkt)\n",
        "        # logging.debug(\"X_hat_centered.shape (20,4096): \" + str(X_hat_centered.shape))\n",
        "        if show:\n",
        "            self.show_data(X_hat_centered, add_mean = True)\n",
        "        return X_hat_centered \n",
        "\n",
        "    def compute_error(self, X, X_hat):\n",
        "        return sqrt(mean_squared_error(X, X_hat))\n",
        "\n",
        "    def show_principal_components(self,k, figsize=(10,4)):\n",
        "        w = figsize[0]\n",
        "        h = figsize[1]\n",
        "        fig = plt.figure(figsize=(w,h)) \n",
        "        fig.subplots_adjust(left=0, right=1, bottom=0, top=1, hspace=0.05, wspace=0.05) \n",
        "        logging.debug(\"self.eigenvectors.shape = \" + str(self.eigenvectors.shape) )\n",
        "        for i in range(k):\n",
        "            pc = self.eigenvectors[:,i]\n",
        "            assert pc.shape[0]==(sq_size**2)*1 or pc.shape[0]==(sq_size**2)*3, \"Not proper shape (expected (sq_size**2) (*3)) \" + str(pc.shape)\n",
        "            \n",
        "            ax = fig.add_subplot(h, w, i+1, xticks=[], yticks=[]) \n",
        "            if color:\n",
        "                pc_img = (np.reshape(pc.real, (sq_size, sq_size, 3))*255).astype(\"uint8\") \n",
        "                ax.imshow(pc_img, interpolation='nearest') \n",
        "            else:\n",
        "                pc_img = np.reshape(pc.real, (sq_size, sq_size))\n",
        "                ax.imshow(pc_img  , cmap=plt.cm.gray, interpolation='nearest')\n",
        "\n",
        "        plt.show()\n",
        "\n",
        "    def compute_explained_variance(self, show=True):\n",
        "        sum_all_eigenValues = sum(self.eigenvalues)\n",
        "        logging.debug(\"\\nSum of all eigenValues: \" + str(sum_all_eigenValues))\n",
        "\n",
        "        explained_variance      = [(value / sum_all_eigenValues)*100 for value in self.eigenvalues]\n",
        "        cum_explained_variance  = np.cumsum(explained_variance)\n",
        "        logging.debug(\"Cum explained variance : \\n\" + str(cum_explained_variance))\n",
        "        if show:\n",
        "            fig = plt.figure(figsize=(12, 6))\n",
        "            ax1 = fig.add_subplot(121)\n",
        "            ax1.bar(range(len(self.eigenvalues)), self.eigenvalues)\n",
        "            ax1.set_xlabel('eigenvalues')\n",
        "            ax1.set_ylabel('values')\n",
        "\n",
        "            ax2 = fig.add_subplot(122)\n",
        "            ax2.bar(range(len(explained_variance)), explained_variance)\n",
        "            ax2.plot(range(len(cum_explained_variance)), cum_explained_variance, color='green', linestyle='dashed', marker='o', markersize=5)\n",
        "\n",
        "            ax2.set_xlabel('eigenvalues')\n",
        "            ax2.set_ylabel('% information ')\n",
        "            ax2.legend( labels = [\"Cumulative Expl. Var.\", \"Explained Variance\"])\n",
        "            ax2.grid()\n",
        "            plt.show()\n",
        "        return explained_variance, cum_explained_variance\n",
        "\n",
        "    def show_data(self, X, add_mean = False):\n",
        "        \n",
        "        # copy so that adding the mean does not modify the original centered\n",
        "        # data X\n",
        "        X_ = X.copy()\n",
        "\n",
        "        if len(X.shape) > 1:\n",
        "            self._show_data(X_, add_mean)\n",
        "        else:\n",
        "            fig = plt.figure(figsize=(3,3))\n",
        "            if add_mean:\n",
        "                    X_ += self.X_mean\n",
        "            # img = np.reshape(X_, (sq_size, sq_size))\n",
        "            img = my_reshape(X_, sq_size, color)\n",
        "\n",
        "            ax = fig.add_subplot(1, 1, 1, xticks=[], yticks=[]) \n",
        "            ax.imshow(img, cmap = my_color_map, interpolation='nearest') \n",
        "\n",
        "            plt.show()\n",
        "\n",
        "\n",
        "    def _show_data(self, X, add_mean = False):\n",
        "        fig = plt.figure(figsize=(8,8)) \n",
        "        fig.subplots_adjust(left=0, right=1, bottom=0, top=1, hspace=0.05, wspace=0.05) \n",
        "        i=0\n",
        "        for x in X:\n",
        "            if add_mean:\n",
        "                    x += self.X_mean\n",
        "            # assert x.shape[0]==(sq_size**2)*3, \"Not proper shape (expected (sq_size**2)*3) \" + str(x.shape)\n",
        "            ax = fig.add_subplot(8, 5, i+1, xticks=[], yticks=[]) \n",
        "            img = my_reshape(x, sq_size, color) \n",
        "            ax.imshow(img, cmap = my_color_map, interpolation='nearest') \n",
        "\n",
        "            i+=1\n",
        "        plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gQjyu0m88ZJP",
        "colab_type": "text"
      },
      "source": [
        "### Basic idea\n",
        "\n",
        "If you are completely unfamiliar with the principal components analysis, the thread in [PCA intuition](https://stats.stackexchange.com/questions/2691/making-sense-of-principal-component-analysis-eigenvectors-eigenvalues) contains a wonderful layered explanation of what the PCA is, and what can its use be.\n",
        "Another very nice introduction is given in [medium](https://medium.com/@aptrishu/understanding-principle-component-analysis-e32be0253ef0)\n",
        "\n",
        "Essentially, the PCA technique extracts the information from the data to get the main directions intrinsically present in the data. To realize that, PCA uses the covariance, which is a *measure of the extent to which corresponding elements from two sets of ordered data move in the same direction* (definition extracted from [medium website](https://medium.com/@aptrishu/understanding-principle-component-analysis-e32be0253ef0))\n",
        "\n",
        "\n",
        "Based on this covariance information, the PCA  *finds a new set of dimensions (or a set of basis of views) such that all the dimensions are orthogonal (and hence linearly independent) and ranked according to the variance of data along them. It means more important principle axis occurs first* (extracted from [medium website](https://medium.com/@aptrishu/understanding-principle-component-analysis-e32be0253ef0))\n",
        "If you're more of a visual person, the plot [here](https://i.stack.imgur.com/lNHqt.gif) shows the first principal component of a cloud of points, with an animation that shows how the variance is minimized.\n",
        "\n",
        "It is really an extraction of information from the data, hence an unsupervised technique, that is used to reduce the dimensionality of the problem. Applying PCA on images can therefore be used as a feature representation!\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zIFK9vVWiyuf",
        "colab_type": "text"
      },
      "source": [
        "### PCA How-to, Step1: Pre-processing\n",
        "\n",
        "The PCA is applied on *hyper-points*: points in our high-dimensional space. We reprensent each of those points by a vector. Applied on images, we then need to convert our sets of images into a useable format. \n",
        "\n",
        "The representation chosen is a $(n \\times p)$ matrix where\n",
        "- n is the number of images\n",
        "- p is the dimension of the vector\n",
        "\n",
        "The matrix containing the training data is then a $(40 \\times p)$ matrix, and -- in our specific problem insofar -- the matrix containing the test data is a $(40 \\times p)$ matrix.\n",
        "\n",
        "#### Resizing\n",
        "All the images (*hyper-points*) shall have the same dimensions to start with. There is therefore a first step of resizing all the images into a commonly appropriate size, keeping in mind that the faces are square images. The parameter indicating the lenght of this square side is defined as `sq_size` in this notebook. \n",
        "\n",
        "`sq_size = 64` indicates that the images are resized as a $(64 \\times 64)$ matrix, containing 1 or 3 channels depending on their colorscale. \n",
        "\n",
        "\n",
        "#### Color or Grayscale ?\n",
        "In the litterature, we find both implementation, and I decide not to choose at this point. A grayscale image contains only one channel, a colored image contains three. \n",
        "> to change from grayscale to color, just change the boolean parameter `color` in the beginning of this notebook.\n",
        "\n",
        "The grayscale image is then simply converted to a vector by flattening the matrix, concatenating each row one after the others. \n",
        "\n",
        "The colored image is converted applying the same technique on the three channels, then concatenating the three resulting vectors into one, longer, vector.\n",
        "\n",
        "####Resulting Dimensionality\n",
        "From a training set colored image of dimensions $( h, w )$\n",
        "- resize to $(64,64)$\n",
        "- convert to vector:\n",
        "    * if grayscale, convert to $(1, 4096)$\n",
        "    * if color, convert to ($1, 12288)$\n",
        "\n",
        "The resulting matrix representing the training set has a shape $(40,4096)$ or $(40,12288)$ depending if grayscale or color."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Erjs7Drnf9w9",
        "colab_type": "text"
      },
      "source": [
        "####Process the training set as a data matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C293QFgzgezY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "Build useable training set from (hyper-)parameters\n",
        "\"\"\"\n",
        "\n",
        "# column 0 = first image\n",
        "# column 1 = second image\n",
        "# ...\n",
        "m_src = get_matrix_from_set(training_set, color, sq_size = sq_size, flatten = True)\n",
        "\n",
        "logging.debug(\" m_src: original matrix\")\n",
        "logging.debug(\" m_src: shape = \" + str(m_src.shape))\n",
        "\n",
        "plot_matrix(m_src, color, my_color_map, h=4, w=10, transpose = False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "foS_IqQljwTu",
        "colab_type": "text"
      },
      "source": [
        "#### Process the test set as a data matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_tDYQ-ODjvAV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "Build useable test set from (hyper-)parameters\n",
        "\"\"\"\n",
        "m_test_src = get_matrix_from_set(test_set, color, sq_size = sq_size, flatten = True)\n",
        "\n",
        "\n",
        "logging.debug(\" m_test_src: original matrix\")\n",
        "logging.debug(\" m_test_src: shape = \" + str(m_test_src.shape))\n",
        "\n",
        "\n",
        "plot_matrix(m_test_src, color, my_color_map,h = 4, w=10,transpose = False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JlzOen9a4Nq_",
        "colab_type": "text"
      },
      "source": [
        "###PCA How-to, Step2: Centering the Data\n",
        "\n",
        "Centering the data is essential in order to have, eventually, the eigenvectors sorted according to the eigenvalues properly meaning what we desire, aka the directions of max variances in the data.\n",
        "\n",
        "The following code shows:\n",
        "1. (left) a plot of the data (training) matrix \n",
        "2. (middle) the mean image\n",
        "3. (right) a plot of the data where the mean image is substracted: the data are now centered\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8uVi6DotjZF9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = m_src.copy()\n",
        "X_mean = np.mean(X, axis = 0)\n",
        "Xc = X - X_mean\n",
        "\n",
        "fig = plt.figure(figsize=(18, 5))\n",
        "\n",
        "Xs = np.arange(0, X.shape[0])\n",
        "Ys = np.arange(0, X.shape[1])\n",
        "Xs, Ys = np.meshgrid(Xs, Ys)\n",
        "  \n",
        "ax1 = fig.add_subplot(131,projection='3d')\n",
        "surf1 = ax1.plot_surface(Xs, Ys, X.T, cmap=plt.cm.jet, antialiased=True)\n",
        "ax1.set_xlabel('image index')\n",
        "ax1.set_ylabel('vector index')\n",
        "ax1.set_zlabel('value')\n",
        "ax1.set_title('Original data')\n",
        "ax2 = fig.add_subplot(132)\n",
        "ax2.imshow(my_reshape(X_mean, sq_size, color), cmap = my_color_map, interpolation='nearest') \n",
        "ax2.set_title(\"Mean image\")\n",
        "# ax2.imshow(X_mean.reshape(sq_size,sq_size), cmap=plt.cm.gray)\n",
        "\n",
        "# ax2.plot(np.arange(0, m_src.shape[0]), m_src_means, 'bo')\n",
        "# ax2.plot(np.arange(0, m_src.shape[1]), m_src_max, 'go')\n",
        "# ax2.plot(np.arange(0, m_src.shape[1]), m_src_min, 'ro')\n",
        "\n",
        "ax3 = fig.add_subplot(133,projection='3d')\n",
        "surf3 = ax3.plot_surface(Xs, Ys, Xc.T, cmap=plt.cm.jet, antialiased=True)\n",
        "ax3.set_xlabel('image index')\n",
        "ax3.set_ylabel('vector index')\n",
        "ax3.set_zlabel('value')\n",
        "ax3.set_title(\"Centered data\")\n",
        "\n",
        "logging.info(\"Visualization of the images - centered:\")\n",
        "plot_matrix(Xc, color, my_color_map, h=4, w=10,transpose = False)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oOG5NzkvKzIo",
        "colab_type": "text"
      },
      "source": [
        "###PCA How-to, Step3: Decomposition\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vc6kwotfxtH_",
        "colab_type": "text"
      },
      "source": [
        "####*Canonical - EigenDecomposition*\n",
        "####0. <u>Data </u>\n",
        "Let's take our initial data matrix X, a $(n \\times p)$ matrix of data where n is the number of images, and p is the number of variables. In our case, the number of variables is the number of pixels of one image (or three times this number, if color image)\n",
        "\n",
        "First, we have to center the data, hence substracting the mean image. This is done in the previous step. In the following text, $X$ is assumed centered. \n",
        "\n",
        "\n",
        "\n",
        "####1. <u>Covariance Matrix </u>\n",
        "\n",
        "Then, we can compute the covariance matrix, that indicates how a variable (= pixel intensity) varies with respect to other pixels. The Covariance matrix indicates how the variables evolve with respect to each others. \n",
        "\n",
        "$C = \\frac{X^T \\cdot X}{n-1}$ and has dimension $(p \\times p)$. This is a pretty large matrix already.\n",
        "\n",
        "####2. <u>Eigenvalues and EigenVectors </u>\n",
        "Having computed the covariance matrix C, we can compute its eigenvectors and eigenvalues, indicating the main directions and their strength of how the data evolve. The eigendecomposition is expressed as:\n",
        "\n",
        "$C = V * L * V^T$ where\n",
        "- $L$ is a diagonal matrix of eigenvalues\n",
        "- $V$ is the $(p \\times p)$ matrix of eigenvectors\n",
        "\n",
        "\n",
        "<u>*Mathematical Trick: Exploiting the dimensionality of the matrix*</u>\n",
        "\n",
        "Computing the eigenvalues and eigenvectors of $C$ can be pretty cumbersome, as $C$ is a large matrix $(p \\times p)$.\n",
        "\n",
        "Recalling our algebra skills, given the dimension of $X$, only a limited amount of eigenvalues are non zero: there are only $(n-1)$ non zero eigenvalues. There is no need to compute the $p$ eigenvalues and related $(p \\times p)$ eigenvectors matrix as (all) the information is contained in only $(n-1)$ eigenvalues.\n",
        "\n",
        "As a result, to speed up the computation and take advantage of this property, instead of computing the eigenvalues and eigenvectors of $C = \\frac{X^T \\cdot X}{n-1}$ of size $(p \\times p)$, let's rather compute the $n$ eigenvalues and corresponding eigenvectors of the matrix \n",
        "$D = \\frac{X \\cdot X^T}{n-1}$ \n",
        "\n",
        "of size $(n \\times n)$, such that:\n",
        "\n",
        "$D = W * L * W^T$\n",
        "- the eigenvalues computed are the same of $C$\n",
        "- the corresponding eigenvectors of C, in matrix $V$, are related such that $V = X^T \\cdot W$\n",
        "\n",
        "This way, it takes advantage of the dimensions of the problem.\n",
        "\n",
        "####3. <u>Principal Components</u>\n",
        "\n",
        "The principal components are defined as the eigenvectors $V$. The eigenvectors can be sorted according to the value of their associated eigenvalue, in decreasing order.\n",
        "\n",
        "The eigenvector that has the largest eigenvalue associated is called \"first principal component\", the second largest is called \"second principal component\", and so on.\n",
        "\n",
        "In the context of faces analysis, the eigenvectors are often called **eigenfaces**, as they can be reshaped as an image, and displayed (provided the appropriate number conversion so that it's in a range visible)\n",
        "\n",
        "By projecting the original data $X$ on the new directions, the eigenfaces, we get *new coordinates* that yet *fully describe the original data*. \n",
        "\n",
        "Furthermore, the number of eigenvectors on which we project the data is reduced with respect to original problem dimensionality.   \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4HE5XZ2h1kmX",
        "colab_type": "text"
      },
      "source": [
        "####*Singular Value Decomposition*\n",
        "\n",
        "The idea behind the SVD is essentially mathematical, and help in computing the eigenvectors and eigenvalues in a different way. From a matrix X, centered, (as in previous section), one can compute its decomposition $X = U \\cdot S \\cdot V^T$ \n",
        "where $U$ is a unitary matrix, $S$ is diagonal, containing what's called the singular values $s_i$. \n",
        "One can see that $V$, right singular vectors, are related to eigenvectors of the covariance matrix from previous section.\n",
        "\n",
        "Indeed, computing this covariance matrix gives:\n",
        "\n",
        "\\begin{align}\n",
        "Cov &= \\frac{X^T \\cdot X}{n-1} \\\\\n",
        "    &= \\frac{V \\cdot S \\cdot U^T \\cdot U \\cdot S \\cdot V}{n-1} \\\\\n",
        "    &= V \\cdot \\frac{S^2}{n-1} V^T\n",
        "\\end{align}\n",
        "\n",
        "There is therefore a link between the singular values ($s_i$) and the eigenvalues ($\\lambda_i$):\n",
        "$$\\lambda_i = \\frac{s_i^2}{n-1}$$ and the right singular vectors are the eigenvectors $V$.\n",
        "\n",
        "Note that in practice, most of the implementation of the PCA algorithm uses singular value decomposition, and starts by centering the data - such as the library function `sklearn.decomposition.PCA` that we will extensively use later on."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T1OfmaOV4tkL",
        "colab_type": "text"
      },
      "source": [
        "####Decomposition and Eigenfaces\n",
        "\n",
        "The following lines of codes create an object of type `MyPCA`, and compute the decompositions following two methods: \"svd\" and \"eigen_fast\". \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P9c9kJzRhMVK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nb_training_faces = sum(training_sets_size.values())\n",
        "\n",
        "\"\"\"\n",
        "Ensuring reset of object if cell is rerun\n",
        "\"\"\"\n",
        "my_pca = None\n",
        "my_pca2 = None\n",
        "\n",
        "\"\"\"\n",
        "Getting the source matrix\n",
        "\"\"\"\n",
        "X = m_src.copy()\n",
        "\n",
        "\"\"\"\n",
        "Creating the MyPCA objects\n",
        "-> solving according to SVD\n",
        "-> solving according to EigenDecomposition (with Math Trick)\n",
        "\"\"\"\n",
        "my_pca = MyPCA(\"svd\")\n",
        "my_pca.fit(X)\n",
        "print(\"All principal components, using SVD\")\n",
        "my_pca.show_principal_components(k=nb_training_faces)\n",
        "\n",
        "my_pca2 = MyPCA(\"eigen_fast\")\n",
        "my_pca2.fit(X)\n",
        "print(\"All principal components, using eigendecomposition\")\n",
        "my_pca2.show_principal_components(k=nb_training_faces)\n",
        "\n",
        "# my_pca3 = MyPCA(\"eigen\")\n",
        "# my_pca3.fit(X)\n",
        "# print(\"All principal components, using nominal eigendecomposition\")\n",
        "# my_pca3.show_principal_components(k=nb_training_faces)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sWe-_cjtjRXe",
        "colab_type": "text"
      },
      "source": [
        "The two plots above show the same information: the eigenfaces, but computed in two different ways: using SVD and using eigendecomposition. Several things are important to be noticed:\n",
        "- the eigenfaces are very much alike. Actually, they are exactly the same (despite the last one, see next point) considering the well-known sign ambiguity related to decomposition, see [Standord course, sect. 5.3, Properties of eigenvectors](https://graphics.stanford.edu/courses/cs205a-13-fall/assets/notes/chapter5.pdf). Long story short, white and black may be reversed without any issue in the eigenfaces, on each image independantly. \n",
        "    * In the commonly used library implementation, there is often an `sign_flip`function implemented to ensure repeatability of the results of the decomposition. See [documentation](https://kite.com/python/docs/sklearn.utils.extmath.svd_flip)\n",
        "- the last eigenface seems to differ... Definitely, this isn't an issue. \n",
        "    * recall that there are only (n - 1) non-zero eigenvalue. The eigenface associated to this eigenvalue is therefore multiplied by 0, and doesn't play a role. \n",
        "\n",
        "If interested, you may uncomment the last lines in order to create a PCA with parameter `method = \"eigen\"`, and see the result of the true eigendecomposition, without the mathematical trick. Or you can trust me that the result is the same (time to run ~50 seconds)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CRu22Qk0KBeM",
        "colab_type": "text"
      },
      "source": [
        "####Reconstructing on k components"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ZZagj0-bU7a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "Selecting X as the input image we want to reconstruct\n",
        "\"\"\"\n",
        "X = m_src.copy()[9,:]\n",
        "\n",
        "logging.debug(\"Shape of input X = \" + str((X.shape)))\n",
        "\n",
        "\"\"\"\n",
        "Centering the data\n",
        "\"\"\"\n",
        "X_mean = np.mean(m_src.copy(), axis=0)\n",
        "Xc = X - X_mean\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "Computing X_reduced, projection of Xc on the principal components space\n",
        "\"\"\"\n",
        "X_reduced = my_pca.projectPC(Xc, k=nb_training_faces)\n",
        "logging.debug(\"Shape of X_reduced = \" + str(X_reduced.shape))\n",
        "\n",
        "\"\"\"\n",
        "Reconstructing progressively\n",
        "Based on k first components only\n",
        "\"\"\"\n",
        "k=0\n",
        "fig = plt.figure(figsize=(3,3))\n",
        "img = my_reshape(X_mean, sq_size, color)\n",
        "ax = fig.add_subplot(1, 1, 1, xticks=[], yticks=[]) \n",
        "ax.imshow(img, cmap = my_color_map, interpolation='nearest') \n",
        "plt.show()\n",
        "logging.info(\"Principal components used = \" + str(k) + \";\\nReconstruction error = \" + str(np.round(my_pca.compute_error(Xc+X_mean, X_mean),2)))\n",
        "\n",
        "for k in [1,2,3,5,8,10,12,15,20,25,30,40]:\n",
        "    X_hat_centered = my_pca.reconstruct(X_reduced, k, show=True)\n",
        "    logging.info(\"\\nAbove: \\nPrincipal components used = \" + str(k) + \n",
        "                 \";\\nReconstruction error = \" + \n",
        "                 str(np.round(my_pca.compute_error(Xc, X_hat_centered),2)) + \n",
        "                 \"\\n\"+\"___\"*30)\n",
        "\n",
        "\n",
        "# my_pca.show_data(X)\n",
        "expl_var, cum_expl_var = my_pca.compute_explained_variance(show = True)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ddRvRypsHa81",
        "colab_type": "text"
      },
      "source": [
        "*Discussions*\n",
        "\n",
        "On the above results and images, several things can be observed:\n",
        "1. First the different reconstruction of one selected face. As expected, the reconstruction decreases as the number of components used (k) increases. This also match the intuition behind the explained variance.\n",
        "2. Second, the ultimate error remaining is 0, indicating no information was lost when considering the 40 components. That confirms the mathematical theory.\n",
        "3. The last two graphs show on the left, the eigenvalues, and on the right, the cumulative explained variance. \n",
        "    * the eigenvalues are sorted from the most important to the least important, confirming the right curve of the cumulative expl. var. However, the descent of the values is not fast (not exponential). Furthermore, there is no big drop off in the values.\n",
        "    * this indicates that the choice of an **optimal number p** of components such that the dimensionality of the feature space is reduced but still informative is not obvious. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wrk9j2nrK9Bq",
        "colab_type": "text"
      },
      "source": [
        "####Choice of optimal $p$\n",
        "\n",
        "\n",
        "$p$ is defined as the optimal number of components used in the reduced space so that:\n",
        "1. the dimension is reduced (lower than n)\n",
        "2. the reconstructed information is *close* to input data. It means the reconstruction is still informative.\n",
        "Selecting only the first $p$ components has the effect of removing small variances. This can be important for some application, if there are little variance between different classes.\n",
        "\n",
        "\n",
        "As said above:\n",
        "- there is no clear drop-off in the eigenvalues,\n",
        "- there is no exponential decrease if the eigenvalues.\n",
        "\n",
        "It makes the choice of an optimal $p$ complicated. \n",
        "\n",
        "To choose, we will do:\n",
        "1. compute the reconstruction loss (error):\n",
        "    * for all training examples\n",
        "    * for all possible choice of p\n",
        "2. plot this RMSE, in absolute value, \n",
        "3. plot, in percentage, the ratio between the reconstruction loss using p-components and the RMSE between mean image and input image. This gives the notion of percentage of reconstruction error -- that can actually be related to the cumulative explained variance !\n",
        "2. define a threshold of 95% of the information kept (5% of error tolerated)\n",
        "\n",
        "That will lead to a sensible choice of $p$. This is however purely arbitrary.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7jHGWbSAi5MF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train = m_src.copy()\n",
        "X_train_mean = np.mean(X_train, axis = 0)\n",
        "X_train_centered = X_train - X_train_mean\n",
        "\n",
        "my_pca = None\n",
        "my_pca = MyPCA(\"svd\")\n",
        "my_pca.fit(X_train)\n",
        "\n",
        "n = X_train.shape[0]\n",
        "\"\"\"\n",
        "np array containing all the rmse computed\n",
        "\n",
        "if n = 40, max number of components, then rmse has a size 40x40 (0 -> 39)\n",
        "> a row matches the rmse of one image wrt the dimension reconstructed. Last column should be 0 (or close, ~e-14)\n",
        "\"\"\"\n",
        "rmse = np.empty((n,n+1))\n",
        "rmse_pc = np.empty((n,n+1)) # rmse in percentage\n",
        "rmse_base = np.empty((n,))\n",
        "for i in range(n):\n",
        "    rmse_base[i]=my_pca.compute_error(X_train[i], X_train_mean)\n",
        "\n",
        "\n",
        "index_image = 0\n",
        "for img_center_vector in X_train_centered:\n",
        "    # logging.debug(\"projecting on \" + str(n) + \" principal components\")\n",
        "    X_centered_reduced = my_pca.projectPC(img_center_vector, n)\n",
        "    for k in range(n+1):\n",
        "        # from 1 to n, included\n",
        "        if k == 0:\n",
        "            rmse[index_image, k] = rmse_base[index_image]\n",
        "            rmse_pc[index_image, k] = 100 * rmse[index_image,k] / rmse_base[index_image]\n",
        "        else:\n",
        "            # logging.debug(\"reconstructing using \" + str(p) + \" principal components\")\n",
        "            X_hat_centered = my_pca.reconstruct(X_centered_reduced, k)\n",
        "            rmse[index_image,k] = my_pca.compute_error(img_center_vector, X_hat_centered)\n",
        "            rmse_pc[index_image, k] = 100 * rmse[index_image,k] / rmse_base[index_image]\n",
        "        \n",
        "    index_image += 1\n",
        "\n",
        "fig = plt.figure(figsize = (16,16))\n",
        "ax1 = fig.add_subplot(2,2,1)\n",
        "\n",
        "for idx in range(n):\n",
        "    rmse_ = rmse[idx,:]\n",
        "    ax1.plot([i for i in range(0,n+1)],rmse_, '-')\n",
        "ax1.set_title(\"reconstruction errors (RMSE) for all images\")\n",
        "ax1.set_xlabel(\"reconstruction dimension(s) \\'p\\' \")\n",
        "ax1.set_ylabel(\"RMSE\")\n",
        "\n",
        "rmse_mean = np.mean(rmse, axis = 0)\n",
        "ax2 = fig.add_subplot(2,2,2)\n",
        "ax2.plot([i for i in range(0, n+1)], rmse_mean, \"ro-\")\n",
        "ax2.set_title(\"Mean of RMSE for all images\")\n",
        "ax2.set_xlabel(\"reconstruction dimension(s) \\'p\\' \")\n",
        "ax2.set_ylabel(\"mean of RMSEs\")\n",
        "\n",
        "ax1.set_ylim((0,80))\n",
        "ax2.set_ylim((0,80))\n",
        "ax1.grid()\n",
        "ax2.grid()\n",
        "\n",
        "ax3 = fig.add_subplot(2,2,3)\n",
        "\n",
        "for idx in range(n):\n",
        "    rmse_ = rmse_pc[idx,:]\n",
        "    ax3.plot([i for i in range(0,n+1)],rmse_, '-')\n",
        "ax3.set_title(\"reconstruction errors (RMSE) for all images, in %\")\n",
        "ax3.set_xlabel(\"reconstruction dimension(s) \\'p\\' \")\n",
        "ax3.set_ylabel(\"RMSE\")\n",
        "\n",
        "rmse_pc_mean = np.mean(rmse_pc, axis = 0)\n",
        "ax4 = fig.add_subplot(2, 2,4)\n",
        "ax4.plot([i for i in range(0, n+1)], rmse_pc_mean, \"ro-\")\n",
        "ax4.set_title(\"Mean of RMSE for all images, in %\")\n",
        "ax4.set_xlabel(\"reconstruction dimension(s) \\'p\\' \")\n",
        "ax4.set_ylabel(\"mean of RMSEs\")\n",
        "\n",
        "ax3.set_ylim((0,110))\n",
        "ax4.set_ylim((0,110))\n",
        "ax3.grid()\n",
        "ax4.grid()\n",
        "\n",
        "# print(rmse_pc_mean)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X6RE5kJ1bgWw",
        "colab_type": "text"
      },
      "source": [
        "Following the explained process of defining $p$, the threshold is set at $p = 35$. This is definitely not an extraordinary result, but yet constitutes somehow a reduction, and based on a reasonable choice.\n",
        "\n",
        "We can now visualize the training image reconstructed based on the first 35 components."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6tMlcQwwcbQ0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "Define optimal p\n",
        "\"\"\"\n",
        "p = 35\n",
        "\n",
        "\"\"\"\n",
        "Selecting X as the input image we want to reconstruct\n",
        "\"\"\"\n",
        "X = m_src.copy()\n",
        "logging.debug(\"Shape of input X = \" + str((X.shape)))\n",
        "\n",
        "\"\"\"\n",
        "Centering the data\n",
        "\"\"\"\n",
        "X_mean = np.mean(m_src.copy(), axis=0)\n",
        "Xc = X - X_mean\n",
        "\n",
        "\"\"\"\n",
        "Creating data structure for outputs\n",
        "\"\"\"\n",
        "X_hat=np.empty(X.shape)\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "Computing X_reduced, projection of Xc on the principal components space\n",
        "\"\"\"\n",
        "X_reduced = my_pca.projectPC(Xc, k=p)\n",
        "logging.debug(\"Shape of X_reduced = \" + str(X_reduced.shape))\n",
        "\n",
        "\"\"\"\n",
        "Reconstructing based on p first components only\n",
        "\"\"\"\n",
        "X_hat_centered = my_pca.reconstruct(X_reduced, p, show=False)\n",
        "X_hat = X_hat_centered + X_mean\n",
        "fig1 = plt.figure()\n",
        "plot_matrix(X_hat, color, my_color_map, h=4, w=10, transpose=False)\n",
        "\n",
        "\"\"\"\n",
        "Visualize original input, for comparison purpose\n",
        "\"\"\"\n",
        "fig2=plt.figure()\n",
        "fig2 = plot_matrix(X, color, my_color_map,h=4, w=10, transpose=False,return_figure=True)\n",
        "\n",
        "\n",
        "for axis in ['top','bottom','left','right']:\n",
        "    fig2.axes[1].spines[axis].set_linewidth(2)\n",
        "    fig2.axes[1].spines[axis].set_color('white')\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WPCAoAaGHgLy",
        "colab_type": "text"
      },
      "source": [
        "On the above two series of images, the top one is the reconstructed using 35 components, and the above one plots the original data.\n",
        "- overall, the quality seems good enough to fully recognize all the faces pretty well, confirming that most of the variance is kept, and the remaining construction loss is small.\n",
        "- several images, however, clearly show this loss (analysis in grayscale):\n",
        "    * image[0,6] contains visibly some extra riddles on the bottom left of the face \n",
        "    * image[2,1] is appears difformed\n",
        "    * image[3,6] shows reminiscence of hair on Bradley Cooper's forehead.\n",
        "    * ...\n",
        "\n",
        "\n",
        "Luckily, it still show some loss in the reconstruction, confirming the previous results established. Nonetheless, the quality is considered good enough to go on with the $p=35$ selected. \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S2Kkdwd456FK",
        "colab_type": "text"
      },
      "source": [
        "####Plot on first two Principal Components\n",
        "PCA is often used as dimensionality reduction technique to represent high dimensional data. Let's use that to show the faces on the first two principal components base.\n",
        "\n",
        "*We do that first with the training images, reconstructed using p=35, for information. Later, we will apply the same thing on some test images*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QCv5xuzT0VTx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "my_pca = None\n",
        "\n",
        "X = m_src.copy()\n",
        "my_pca = MyPCA(\"svd\")\n",
        "my_pca.fit(X)\n",
        "\n",
        "data = m_src.copy()\n",
        "data_mean = np.mean(m_src.copy(), axis = 0)\n",
        "data_centered = data - data_mean\n",
        "\n",
        "# my_pca_svd.show_data(X)\n",
        "\n",
        "# reduce the image to the principal components (k=2)\n",
        "data_projected = my_pca.projectPC(data_centered, k=2)\n",
        "logging.debug(\"shape of data_projected = \" + str(data_projected.shape))\n",
        "\n",
        "\n",
        "plt.figure() \n",
        "fig, ax = plt.subplots(1, 1, figsize=(14, 14), sharex=True, sharey=True)\n",
        "\n",
        "eig1 = data_projected[:,0]\n",
        "eig2 = data_projected[:,1]\n",
        "ax.plot(eig1, eig2, 'bo')\n",
        "ax.set_xlabel(\"First Component\")\n",
        "ax.set_ylabel(\"Second Component\")\n",
        "\n",
        "for (x_, y_), img_vector_ in zip(data_projected, X_hat):\n",
        "    img = my_reshape(img_vector_, sq_size, color)\n",
        "    ab = AnnotationBbox(OffsetImage(img, cmap = my_color_map), (x_, y_), frameon=False)\n",
        "    ax.add_artist(ab)\n",
        "\n",
        "ax.grid()\n",
        "plt.show()\n",
        "\n",
        "logging.info(\"\\nIn order to better visualize the plot on top, here is the same view\\nwere personA is in red, and personB is in green\")\n",
        "\n",
        "plt.figure() \n",
        "fig, ax = plt.subplots(1, 1, figsize=(8, 8), sharex=True, sharey=True)\n",
        "ax.plot(eig1[0:20], eig2[0:20], 'ro')\n",
        "ax.plot(eig1[20:40], eig2[20:40], 'go')\n",
        "plt.show()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L6P-anlyeL3Z",
        "colab_type": "text"
      },
      "source": [
        "###Demo scikit learn\n",
        "\n",
        "Of course, everything that has been done so far regarding PCA can be achieved using dedicated  - and optimized - libraries. For that purpose, we can use the `sklearn.decomposition` package that, among other things, implement the PCA using the *SVD* decomposition that we've looked at. \n",
        "\n",
        "A difference to note is the use, internally, of the `svd_flip(u, vt)`, a function that ensures the vectors to be deterministic, hence solving the sign ambiguity inherent to matrix decomposition, as already discussed above.\n",
        "\n",
        "The following part first performs the same operation as we've implemented before to get familiarized.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lhJkmJSQBtYk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "Let's start --again-- from the training set, processed as a matrix\n",
        "\"\"\"\n",
        "logging.info(\"shape of input matrix (n x p) = \" + str(m_src.shape))\n",
        "plot_matrix(m_src, color, my_color_map, h=4, w=10, transpose = False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2l2rE-lgCT29",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "Create pca sklearn object, and compute decomposition\n",
        "\"\"\"\n",
        "X = m_src.copy()\n",
        "n_components = 40\n",
        "logging.info(\"Shape input data: \"+str(X.shape))\n",
        "pca_ = sklearn_decomposition_PCA(n_components=n_components) \n",
        "pca_.fit(X)\n",
        "\n",
        "\"\"\"\n",
        "Get the eigenfaces\n",
        "\"\"\"\n",
        "\n",
        "eigenfaces = pca_.components_\n",
        "logging.debug(\"eigenfaces shape = \" + str(eigenfaces.shape))\n",
        "\n",
        "\"\"\"\n",
        "Visualize the eigenfaces, just as we did before\n",
        "\"\"\"\n",
        "plot_matrix(eigenfaces, color, my_color_map, h=4, w=10, transpose=False)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sg0VOAO_DQMD",
        "colab_type": "text"
      },
      "source": [
        "Without any surprise, we find again the same principal components as before.\n",
        "\n",
        "Continuing with sklearn library, we can reconstruct the original data based on the first $p$ components. Hopefully, the results are the same as the ones obtained with the homemade PCA. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5RLJgwgzJrO1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from numpy.testing import assert_array_almost_equal\n",
        "p=35\n",
        "pca_ = sklearn_decomposition_PCA(n_components=p) \n",
        "pca_.fit(X)\n",
        "X_reduced_sk = pca_.transform(X)\n",
        "X_reconstructed_sk = pca_.inverse_transform(X_reduced_sk)\n",
        "\n",
        "print(X_reconstructed_sk.shape)\n",
        "plot_matrix(X_reconstructed_sk, color, my_color_map, h=4, w=10)\n",
        "logging.info(\"Difference between Homemade PCA and Scikit-learn PCA: \" + str(np.linalg.norm(X_hat - X_reconstructed_sk)))\n",
        "logging.info(\" ==> OK!\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TqITs-xBjIIE",
        "colab_type": "text"
      },
      "source": [
        "### Projection of Test Faces reconstructed\n",
        "\n",
        "Using the same kind of plot as before, we can reconstruct, **in the same eigenface base** the test set images using $p$ first components. \n",
        "\n",
        "faces (from the 4 persons) on the first two principal components\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qneLYLvEoAeD",
        "colab_type": "text"
      },
      "source": [
        "We need to apply the sames steps that were applied to the training set, to the test set. that is:\n",
        "\n",
        "\n",
        "1.   centering the data: substracting the mean **of the training set**\n",
        "2.   projecting the resulting centered data on the p first principal components computed by applying PCA on the training data. We won't fit the PCA to the test images, we *just* project the test data using the already-found principal components\n",
        "3.   reconstructing the original data based on those p first principal components, and plot the result of the 40 images on the 2-first components space.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1uNKTkyvi5Sb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "scatter_plot_3D = False\n",
        "scatter_plot_3times = False\n",
        "\n",
        "\"\"\"\n",
        "Once again, start by locally copying the data structure\n",
        "> data_test as new data\n",
        "> data_train_mean for centering\n",
        "\"\"\"\n",
        "\n",
        "data_test = m_test_src.copy()\n",
        "data_train_mean = np.mean(m_src.copy(), axis = 0)\n",
        "\n",
        "data_test_centered = data_test - data_train_mean\n",
        "\n",
        "data_test_projected = my_pca.projectPC(data_test_centered, k=p)\n",
        "logging.debug(\"shape of data_projected = \" + str(data_test_projected.shape))\n",
        "\n",
        "\"\"\"\n",
        "scatter plot in 3D - 3 first components\n",
        "\"\"\"\n",
        "if scatter_plot_3D:\n",
        "    fig = plt.figure(figsize=(8,8))\n",
        "    ax = fig.add_subplot(111, projection='3d')\n",
        "    eig1 = data_test_projected[:,0]\n",
        "    eig2 = data_test_projected[:,1]\n",
        "    eig3 = data_test_projected[:,2]\n",
        "    ax.scatter(eig1[0:10], eig2[0:10], eig3[0:10], 'b')\n",
        "    ax.scatter(eig1[10:20], eig2[10:20], eig3[10:20], 'r')\n",
        "    ax.scatter(eig1[20:30], eig2[20:30], eig3[20:30], 'g')\n",
        "    ax.scatter(eig1[30:40], eig2[30:40], eig3[30:40], 'y')\n",
        "    plt.show()\n",
        "\n",
        "if scatter_plot_3times:\n",
        "    fig = plt.figure(figsize=(24,8))\n",
        "\n",
        "    eig1 = data_test_projected[:,0]\n",
        "    eig2 = data_test_projected[:,1]\n",
        "    eig3 = data_test_projected[:,2]\n",
        "\n",
        "\n",
        "    ax1 = fig.add_subplot(1,3,1)\n",
        "    eig1 = data_test_projected[:,0]\n",
        "    eig2 = data_test_projected[:,1]\n",
        "    eig3 = data_test_projected[:,2]\n",
        "    ax1.plot(eig1[0:10], eig2[0:10], 'bo')\n",
        "    ax1.plot(eig1[10:20], eig2[10:20], 'ro')\n",
        "    ax1.plot(eig1[20:30], eig2[20:30], 'go')\n",
        "    ax1.plot(eig1[30:40], eig2[30:40], 'yo')\n",
        "\n",
        "    ax2 = fig.add_subplot(1,3,2)\n",
        "    ax2.plot(eig1[0:10], eig3[0:10], 'bo')\n",
        "    ax2.plot(eig1[10:20], eig3[10:20], 'ro')\n",
        "    ax2.plot(eig1[20:30], eig3[20:30], 'go')\n",
        "    ax2.plot(eig1[30:40], eig3[30:40], 'yo')\n",
        "\n",
        "    ax3 = fig.add_subplot(1,3,3)\n",
        "    ax3.plot(eig2[0:10], eig3[0:10], 'bo')\n",
        "    ax3.plot(eig2[10:20], eig3[10:20], 'ro')\n",
        "    ax3.plot(eig2[20:30], eig3[20:30], 'go')\n",
        "    ax3.plot(eig2[30:40], eig3[30:40], 'yo')\n",
        "\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YlOKOtPlRGT2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "eig1 = data_test_projected[:,0]\n",
        "eig2 = data_test_projected[:,1]\n",
        "\n",
        "data_test_reconstructed = my_pca.reconstruct(data_test_projected, p, show=False)\n",
        "\n",
        "plt.figure() \n",
        "fig, ax = plt.subplots(1, 1, figsize=(16, 16), sharex=True, sharey=True)\n",
        "ax.grid()\n",
        "ax.plot(eig1, eig2, 'bo')\n",
        "for x_, y_, img_vector_ in zip(eig1, eig2, data_test_reconstructed):\n",
        "    img = my_reshape(img_vector_ + data_train_mean, sq_size, color)\n",
        "    ab = AnnotationBbox(OffsetImage(img, cmap = my_color_map), (x_, y_), frameon=False)\n",
        "    ax.add_artist(ab)\n",
        "\n",
        "\n",
        "\n",
        "plt.figure() \n",
        "fig, ax = plt.subplots(1, 1, figsize=(8, 8), sharex=True, sharey=True)\n",
        "eig1 = data_test_projected[:,0]\n",
        "eig2 = data_test_projected[:,1]\n",
        "ax.plot(eig1[0:10], eig2[0:10], 'bo')\n",
        "ax.plot(eig1[10:20], eig2[10:20], 'ro')\n",
        "ax.plot(eig1[20:30], eig2[20:30], 'go')\n",
        "ax.plot(eig1[30:40], eig2[30:40], 'yo')\n",
        "ax.legend([\"personA\", \"personB\", \"personC\", \"personD\"])\n",
        "plt.show()\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jelojYy7RGMf",
        "colab_type": "text"
      },
      "source": [
        "The plots of the test reconstructions confirms the intuition behing the eigenfaces:\n",
        "- Emma Stone, in blue, is mainly on the top; while Bradley Cooper, in red, in mainly on the bottom. They are well separated according to the 2nd eigenface, which seems related to the shape of the face, with a very dark part on the bottom right.\n",
        "- Jane Leny, in green, a woman that resembles to Emma Stone for a human, is mainly on the top of the plot,\n",
        "- Marc Blucas, in yellow, is a white man similar to Bradley Cooper. While half of the points are located in the lower left half, where most of Bradley cooper images also are, the rest of Marc Blucas images is in the middle of blue and green points.\n",
        "\n",
        "Two two first eigenfaces, or principal components, already gives us some of the important information present in the data, even if their cumulative explained variance - fitted for the training set inputs - was actually not that high!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6MNLtaC9of-J",
        "colab_type": "text"
      },
      "source": [
        "Although we will certainly not modify any hyper-parameters based on the test set images, it is still interesting to reproduce the metrics that we built for the training set and the choice of an optimal $p$. It is interesting to answer such questions as:\n",
        "- what is the final relative reconstruction error ?\n",
        "- How does it evolve with p ?\n",
        "- Was $p$ a nice choice, regarding the test sets ?\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s5TqYKddlAip",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "np array containing all the rmse computed\n",
        "\n",
        "if n = 40, max number of components, then rmse has a size 40x40 (0 -> 39)\n",
        "> a row matches the rmse of one image wrt the dimension reconstructed. \n",
        "Last column should NOT be 0 as we compute construction of test images (= with loss)\n",
        "\"\"\"\n",
        "n = m_test_src.shape[0]\n",
        "\n",
        "rmse_test = np.empty((n,n+1))\n",
        "rmse_test_pc = np.empty((n,n+1)) # rmse in percentage\n",
        "rmse_test_base = np.empty((n,))\n",
        "for i in range(n):\n",
        "    rmse_test_base[i]=my_pca.compute_error(data_test[i], X_train_mean)\n",
        "\n",
        "\n",
        "index_image = 0\n",
        "for img_center_vector in data_test_centered:\n",
        "    X_test_centered_reduced = my_pca.projectPC(img_center_vector, n)\n",
        "    for k in range(n+1):\n",
        "        # from 1 to n, included\n",
        "        if k == 0:\n",
        "            rmse_test[index_image, k] = rmse_test_base[index_image]\n",
        "            rmse_test_pc[index_image, k] = 100 * rmse_test[index_image,k] / rmse_test_base[index_image]\n",
        "        else:\n",
        "            # logging.debug(\"reconstructing using \" + str(p) + \" principal components\")\n",
        "            X_test_hat_centered = my_pca.reconstruct(X_test_centered_reduced, k)\n",
        "            rmse_test[index_image,k] = my_pca.compute_error(img_center_vector, X_test_hat_centered)\n",
        "            rmse_test_pc[index_image, k] = 100 * rmse_test[index_image,k] / rmse_test_base[index_image]\n",
        "        \n",
        "    index_image += 1\n",
        "\n",
        "\"\"\"\n",
        "Visualization !\n",
        "\"\"\"\n",
        "\n",
        "fig = plt.figure(figsize = (16,16))\n",
        "ax1 = fig.add_subplot(2,2,1)\n",
        "\n",
        "for idx in range(n):\n",
        "    rmse_ = rmse_test[idx,:]\n",
        "    ax1.plot([i for i in range(0,n+1)],rmse_, '-')\n",
        "ax1.set_title(\"reconstruction errors (RMSE) for all test images\")\n",
        "ax1.set_xlabel(\"reconstruction dimension(s) \\'p\\' \")\n",
        "ax1.set_ylabel(\"RMSE\")\n",
        "\n",
        "rmse_test_mean = np.mean(rmse_test, axis = 0)\n",
        "ax2 = fig.add_subplot(2,2,2)\n",
        "ax2.plot([i for i in range(0, n+1)], rmse_test_mean, \"ro-\")\n",
        "ax2.set_title(\"Mean of RMSE for all test images\")\n",
        "ax2.set_xlabel(\"reconstruction dimension(s) \\'p\\' \")\n",
        "ax2.set_ylabel(\"mean of RMSEs\")\n",
        "\n",
        "ax1.set_ylim((0,80))\n",
        "ax2.set_ylim((0,80))\n",
        "ax1.grid()\n",
        "ax2.grid()\n",
        "\n",
        "ax3 = fig.add_subplot(2,2,3)\n",
        "\n",
        "for idx in range(n):\n",
        "    rmse_ = rmse_test_pc[idx,:]\n",
        "    ax3.plot([i for i in range(0,n+1)],rmse_, '-')\n",
        "ax3.set_title(\"reconstruction errors (RMSE) for all test images, in %\")\n",
        "ax3.set_xlabel(\"reconstruction dimension(s) \\'p\\' \")\n",
        "ax3.set_ylabel(\"RMSE\")\n",
        "\n",
        "rmse_test_pc_mean = np.mean(rmse_test_pc, axis = 0)\n",
        "ax4 = fig.add_subplot(2, 2,4)\n",
        "ax4.plot([i for i in range(0, n+1)], rmse_test_pc_mean, \"ro-\")\n",
        "ax4.set_title(\"Mean of RMSE for all test images, in %\")\n",
        "ax4.set_xlabel(\"reconstruction dimension(s) \\'p\\' \")\n",
        "ax4.set_ylabel(\"mean of RMSEs\")\n",
        "\n",
        "ax3.set_ylim((0,110))\n",
        "ax4.set_ylim((0,110))\n",
        "ax3.grid()\n",
        "ax4.grid()\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MLl_-HFCZZ_3",
        "colab_type": "text"
      },
      "source": [
        "*Observations*\n",
        "- the reconstruction loss, computed in the same fashion as for the training set images, decreases also as $p$ increases\n",
        "- the slope seems to become near 0 as $p$ is ~35. It comforts us with the choice of $p=35$\n",
        "- using all the components, the remaining error in the reconstruction is still 60% of the error of the base error. there is \"no way\" to do better.\n",
        "> as a reminder, the \"base\" error is the RMSE between the input image, and the mean image of the training set. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Djz9YO4Guhr",
        "colab_type": "text"
      },
      "source": [
        "# Exploit Feature Representations\n",
        "- use of feature representations that we have constructed before\n",
        "- evaluate each feature representation for classification and identification\n",
        "- discuss each method + why it is appropriate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GO2I8rHUHRuC",
        "colab_type": "text"
      },
      "source": [
        "## Classification - howto\n",
        "\n",
        "- pre-process data\n",
        "    1. Resizing to appropriate size => selected parameter\n",
        "    2. color or grayscale => selected parameter\n",
        "    \n",
        "- compute feature reprensentation \n",
        "    1. HOG\n",
        "    2. PCA\n",
        "- train the corresponding classifier using training sets\n",
        "    1. training using HOG feature\n",
        "    2. training using PCA feature\n",
        "- apply the classifier on unseen images (test)\n",
        "    * apply same preprocessing as for training (exact same)\n",
        "    * train\n",
        "    * compute metrics\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yLF5zzo4IFBZ",
        "colab_type": "text"
      },
      "source": [
        "### Preprocess the data\n",
        "\n",
        "- get raw training data: faces cropped data for personA and personB\n",
        "- resize the raw images to a common image size\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nk6XzxXHH1q3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "logging.info(min(get_min_size(training_set), get_min_size(test_set)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gqBussWgOwAd",
        "colab_type": "text"
      },
      "source": [
        "The smallest face crop that we have in training set and test set is (70,70), so we can without issue rescale all our face crops to (64,64) as part of the data pre-processing."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_kb7kzbG3cTB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train = get_matrix_from_set(training_set, color, sq_size = sq_size, flatten = False)\n",
        "X_test = get_matrix_from_set(test_set, color, sq_size = sq_size, flatten = False)\n",
        "y_train = np.zeros((40,))\n",
        "y_train[20:40] = 1\n",
        " \n",
        "\"\"\"\n",
        "For now, set up \"0\" for personC; \"1\" for personD !\n",
        "\"\"\"\n",
        "y_test = np.zeros((40,))\n",
        "y_test[10:20] = 1\n",
        "y_test[30:40] = 1\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hr-i3JSN7nYn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "logging.info(\"Training set (horizontal axis):\")\n",
        "logging.info(\"Test set PersonA [0 -> 19]:\")\n",
        "plot_matrix(X_train[0:20,:], color, my_color_map, h=1, w=20, transpose = False)\n",
        "logging.info(\"Test set PersonA [20 -> 39]:\")\n",
        "plot_matrix(X_train[20:40,:], color, my_color_map, h=1, w=20, transpose = False)\n",
        "\n",
        "logging.info(\"Test set (vertical axis):\")\n",
        "logging.info(\"Test set PersonA [0 -> 9]:\")\n",
        "plot_matrix(X_test[0:10, :], color, my_color_map, h=1, w=10, transpose = False)\n",
        "logging.info(\"Test set PersonB [10 -> 19]:\")\n",
        "plot_matrix(X_test[10:20, :], color, my_color_map, h=1, w=10, transpose = False)\n",
        "logging.info(\"Test set PersonC [20 -> 29]:\")\n",
        "plot_matrix(X_test[20:30, :], color, my_color_map, h=1, w=10, transpose = False)\n",
        "logging.info(\"Test set PersonD [30 -> 39]:\")\n",
        "plot_matrix(X_test[30:40, :], color, my_color_map, h=1, w=10, transpose = False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K2w_3OUNVmOx",
        "colab_type": "text"
      },
      "source": [
        "## HOG Classification\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ztVlTGrVE0M",
        "colab_type": "text"
      },
      "source": [
        "### Construction of Transformers\n",
        "followed method of https://kapernikov.com/tutorial-image-classification-with-scikit-learn/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tddO9f8APPda",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "\n",
        "class HogTransformer(BaseEstimator, TransformerMixin):\n",
        "    \"\"\"\n",
        "    Expects an array of 2D arrays (1 channel images)\n",
        "    Calculates hog features for each image\n",
        "    \"\"\"\n",
        "    def __init__(self, y=None, \n",
        "                 orientations = 9, \n",
        "                 pixels_per_cell = (8,8), \n",
        "                 cells_per_block = (2,2),\n",
        "                 block_norm = \"L2-Hys\", \n",
        "                 transform_sqrt = False,\n",
        "                 multichannel = False):\n",
        "        self.y = y\n",
        "        self.orientations = orientations\n",
        "        self.pixels_per_cell = pixels_per_cell\n",
        "        self.cells_per_block = cells_per_block\n",
        "        self.block_norm = block_norm\n",
        "        self.transform_sqrt = transform_sqrt\n",
        "        self.multichannel = multichannel # default is grayscale\n",
        "    \n",
        "    def fit(self, X, y=None):\n",
        "        logging.debug(\"[HOGTransformer.fit] X.Shape \" + str(X.shape))\n",
        "        return self\n",
        "    \n",
        "    def transform(self, X, y=None):\n",
        "        logging.debug(\"[HOGTransformer.transform] X.Shape \" + str(X.shape))\n",
        "\n",
        "        def local_hog(X):\n",
        "            if self.multichannel:\n",
        "                X_ = X.copy().T\n",
        "                logging.debug(\"TO CHECK IF TRANSPOSE STILL NEEDED ?\")\n",
        "                logging.debug(\"[HOGTransformer.transform] (1) X.Shape \" + str(X.shape))\n",
        "                logging.debug(\"[HOGTransformer.transform] (2) X_.Shape \" + str(X_.shape))\n",
        "            else:\n",
        "                X_ = X.copy()\n",
        "            # logging.debug(\"[HOGTransformer.transform.local_HOG]\" )\n",
        "            # cv2_imshow(X_)\n",
        "            return skimage_feature_hog(X_,\n",
        "                                       orientations = self.orientations, \n",
        "                                       pixels_per_cell = self.pixels_per_cell,\n",
        "                                       cells_per_block = self.cells_per_block,\n",
        "                                       block_norm = self.block_norm,\n",
        "                                       visualize = False, \n",
        "                                       transform_sqrt = self.transform_sqrt, \n",
        "                                       feature_vector = True, \n",
        "                                       multichannel = self.multichannel)\n",
        "\n",
        "        try: \n",
        "            # tmp = [str(image.shape) for image in X]\n",
        "            # logging.debug( str(tmp) )\n",
        "            return np.array([local_hog(image) for image in X])\n",
        "        except ValueError as ve:\n",
        "            logging.error(str(ve))\n",
        "        except NameError as ne:\n",
        "            logging.error(str(ne))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ajQ4PN5asVjc",
        "colab_type": "text"
      },
      "source": [
        "HOG feature\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "mef1tejVsYN6",
        "colab": {}
      },
      "source": [
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.model_selection import cross_val_predict\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# resizify = ResizeTransformer(sq_size=sq_size)\n",
        "# grayify = RGB2GrayTransformer(color=color)\n",
        "\n",
        "# scalify = StandardScaler()\n",
        "hogify = HogTransformer(    orientations = 9, \n",
        "                            pixels_per_cell = (16,16), \n",
        "                            cells_per_block = (2,2),\n",
        "                            block_norm = \"L2-Hys\", \n",
        "                            transform_sqrt = False,\n",
        "                            multichannel = color)\n",
        "\n",
        "\n",
        "\n",
        "# X_train_gray = grayify.fit_transform(X_train)\n",
        "X_train_hog = hogify.fit_transform(X_train)\n",
        "# X_train_prepared = scalify.fit_transform(X_train_hog)\n",
        "X_train_prepared = X_train_hog"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gGSuC0Lr1up7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(X_train_prepared.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BSnOwwFICgPZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sgd_clf = SGDClassifier(random_state=42, max_iter=1000, tol=1e-6, verbose = 1)\n",
        "sgd_clf.fit(X_train_prepared, y_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "183bDFIGELk1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "X_test_ab = X_test[0:20,:]\n",
        "y_test_ab = y_test[0:20]\n",
        "print(\"X_test shape -> \" + str(X_test.shape))\n",
        "print(\"X_test_ab shape -> \" + str(X_test_ab.shape))\n",
        "print(\"ytest [10]      -> \" + str(sum(y_test_ab)))\n",
        "X_test_hog = hogify.transform(X_test_ab)\n",
        "X_test_prepared = X_test_hog\n",
        "\n",
        "y_pred = sgd_clf.predict(X_test_prepared)\n",
        "logging.info(\"Percentage correct: \" + str( 100* np.sum(y_pred == y_test_ab)/len(y_test_ab) ))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mkYFiPjIKHan",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "label_names = [\"Emma Stone\", \"Bradly Cooper\"]\n",
        "\n",
        "cmx = confusion_matrix(y_test_ab, y_pred)\n",
        "cmx\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r1vzcfScKwJZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
        " \n",
        "def plot_confusion_matrix(cmx, vmax1=None, vmax2=None, vmax3=None):\n",
        "    cmx_norm = 100*cmx / cmx.sum(axis=1, keepdims=True)\n",
        "    cmx_zero_diag = cmx_norm.copy()\n",
        " \n",
        "    np.fill_diagonal(cmx_zero_diag, 0)\n",
        " \n",
        "    fig, ax = plt.subplots(ncols=3)\n",
        "    fig.set_size_inches(12, 3)\n",
        "    [a.set_xticks(range(6)) for a in ax]\n",
        "    [a.set_yticks(range(6)) for a in ax]\n",
        " \n",
        "    im1 = ax[0].imshow(cmx, vmax=vmax1)\n",
        "    ax[0].set_title('as is')\n",
        "    im2 = ax[1].imshow(cmx_norm, vmax=vmax2)\n",
        "    ax[1].set_title('%')\n",
        "    im3 = ax[2].imshow(cmx_zero_diag, vmax=vmax3)\n",
        "    ax[2].set_title('% and 0 diagonal')\n",
        " \n",
        "    dividers = [make_axes_locatable(a) for a in ax]\n",
        "    cax1, cax2, cax3 = [divider.append_axes(\"right\", size=\"5%\", pad=0.1)\n",
        "                        for divider in dividers]\n",
        " \n",
        "    fig.colorbar(im1, cax=cax1)\n",
        "    fig.colorbar(im2, cax=cax2)\n",
        "    fig.colorbar(im3, cax=cax3)\n",
        "    fig.tight_layout()\n",
        " \n",
        "plot_confusion_matrix(cmx)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PC4dG4K4MiIQ",
        "colab_type": "text"
      },
      "source": [
        "Now, we have a quite nice score, but several parameters. U\n",
        "Use of pipeline in order to build a *gridsearch* ...\n",
        "XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXx"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DALEufBgVVkh",
        "colab_type": "text"
      },
      "source": [
        "### HOG/Classification - OPTIMIZATION\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5HF3gHSlMmCb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn import svm\n",
        "\n",
        "\"\"\"\n",
        "Definition of a pipeline\n",
        "\"\"\"\n",
        "HOG_pipeline = Pipeline([\n",
        "                         ('hogify', HogTransformer(\n",
        "                             orientations = 9,\n",
        "                             pixels_per_cell = (4,4),\n",
        "                             cells_per_block = (2,2),\n",
        "                             block_norm='L1',\n",
        "                             transform_sqrt=False, \n",
        "                             multichannel = color)\n",
        "                         ),\n",
        "                         ('classify', SGDClassifier(\n",
        "                             random_state = 42, \n",
        "                             max_iter = 1000, \n",
        "                             tol=1e-3)\n",
        "                         )\n",
        "\n",
        "])\n",
        "\n",
        "clf = HOG_pipeline.fit(X_train, y_train)\n",
        "# logging.info(\"Percentage correct: \" + str( 100* np.sum(y_pred == y_test_ab)/len(y_test_ab) ) + \" %\")\n",
        "y_pred = clf.predict(X_test) \n",
        "logging.info(\"Percentage correct: \" + str( 100*np.sum(y_pred == y_test)/len(y_test)) + \" %\")\n",
        "print(y_pred)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fn57FFi6OcQZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        " \n",
        "param_grid = [\n",
        "    {\n",
        "        'hogify__orientations': [9],\n",
        "        'hogify__cells_per_block': [(2, 2),(3, 3)],\n",
        "        'hogify__pixels_per_cell': [(4,4),(8, 8),(12,12), (16, 16)],\n",
        "        'hogify__block_norm': ['L1', 'L2', 'L2-Hys'],\n",
        "        'hogify__transform_sqrt': [False, True],\n",
        "        'hogify__multichannel':[color],\n",
        "        'classify': [\n",
        "            SGDClassifier(random_state=42, max_iter=1000, tol=1e-3),\n",
        "            svm.SVC(kernel='linear'),\n",
        "            svm.SVC(kernel='poly')]}\n",
        "]\n",
        "\n",
        "grid_search = GridSearchCV(HOG_pipeline,\n",
        "                           param_grid, \n",
        "                           cv = 4, \n",
        "                           n_jobs = -1,\n",
        "                           scoring = \"accuracy\",\n",
        "                           verbose = 1, \n",
        "                           return_train_score = True)\n",
        "\n",
        "grid_res = grid_search.fit(X_train, y_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GJjSs_v1RNz4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "grid_res.best_estimator_"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xp2EcopKR_P9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "grid_res.best_score_"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xCo3_Wr5SDs-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "grid_res.best_params_"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n6tBpJriSkUW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "best_prediction = grid_res.predict(X_test)\n",
        "print('Percentage correct: ', 100*np.sum(best_prediction == y_test)/len(y_test))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vhX8e4yJjTyD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "idx = np.where( np.array(y_test - best_prediction) != 0)\n",
        "print(idx)\n",
        "print(y_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V37pPCfnubEQ",
        "colab_type": "text"
      },
      "source": [
        "## PCA Classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LUrEq8kIua7b",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zVjfgPEbFlph",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train = get_matrix_from_set(training_set, color, sq_size, flatten=True)\n",
        "X_test= get_matrix_from_set(test_set, color, sq_size, flatten = True)\n",
        "\n",
        "logging.debug(\"X_train_PCA Shape = \" + str(X_train.shape))\n",
        "logging.debug(\"X_test_PCA Shape = \" + str(X_test.shape))\n",
        "\n",
        "y_train = y_train.copy()\n",
        "y_test = y_test.copy()\n",
        "\n",
        "logging.debug(\"y_train_PCA Shape = \" + str(y_train.shape))\n",
        "logging.debug(\"y_test_PCA Shape = \" + str(y_test.shape))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JQ1nx5PMItFo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pcaify = sklearn_decomposition_PCA(n_components = 25)\n",
        "X_train_PCA = pcaify.fit_transform(X_train)\n",
        "X_train_prepared = X_train_PCA\n",
        "\n",
        "print(X_train_prepared.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XM_prrKMLQhZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sgd_clf = SGDClassifier(random_state=42, max_iter = 1000, tol=1e-4, verbose = 1)\n",
        "sgd_clf.fit(X_train_prepared, y_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cs_pVXy4Linr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_test_PCA = pcaify.transform(X_test)\n",
        "X_test_prepared = X_test_PCA\n",
        "y_pred = sgd_clf.predict(X_test_prepared)\n",
        "\n",
        "logging.info(\"Percentage correct: \" + str(100 * np.sum(y_pred == y_test)/len(y_test)))\n",
        "cmx = confusion_matrix(y_test, y_pred)\n",
        "plot_confusion_matrix(cmx)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aaBbluh6NDUA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "Definition of a PCA pipeline\n",
        "\"\"\"\n",
        "PCA_pipeline = Pipeline([\n",
        "                         ('pcaify', sklearn_decomposition_PCA(\n",
        "                             n_components = 25)\n",
        "                         ),\n",
        "                         ('classify', SGDClassifier(\n",
        "                             random_state = 42, \n",
        "                             max_iter = 1000, \n",
        "                             tol=1e-3)\n",
        "                         )\n",
        "\n",
        "])\n",
        "\n",
        "clf = PCA_pipeline.fit(X_train, y_train)\n",
        "# logging.info(\"Percentage correct: \" + str( 100* np.sum(y_pred == y_test_ab)/len(y_test_ab) ) + \" %\")\n",
        "y_pred = clf.predict(X_test) \n",
        "logging.info(\"Percentage correct: \" + str( 100*np.sum(y_pred == y_test)/len(y_test)) + \" %\")\n",
        "print(y_pred)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nAuE47ZqOGLV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "param_grid = [\n",
        "    {\n",
        "        'pcaify__n_components': range(1,41,2),\n",
        "        'classify': [\n",
        "            SGDClassifier(random_state=42, max_iter=1000, tol=1e-3),\n",
        "            svm.SVC(kernel='linear'),\n",
        "            svm.SVC(kernel='poly')]}\n",
        "]\n",
        "\n",
        "grid_search = GridSearchCV(PCA_pipeline,\n",
        "                           param_grid, \n",
        "                           cv = 3, \n",
        "                           n_jobs = -1,\n",
        "                           scoring = \"accuracy\",\n",
        "                           verbose = 1, \n",
        "                           return_train_score = True)\n",
        "\n",
        "grid_res = grid_search.fit(X_train, y_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Is4LbiYGO1nQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "logging.info(\"Best Score -> \" + str(grid_res.best_score_))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LZbk1lHSPDDV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "logging.info(\"Best Parameters found:\")\n",
        "logging.info(grid_res.best_params_)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fVoct4GYPlrz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "best_prediction = grid_res.predict(X_test)\n",
        "logging.info(\"Percentage correct: \" + str(100*np.sum(best_prediction == y_test)/len(y_test)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-x_REboTQE6L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "idx = np.where( np.array(y_test-best_prediction) != 0)\n",
        "logging.debug(idx)\n",
        "logging.debug(best_prediction)\n",
        "logging.debug(y_test)\n",
        "# cv2_imshow(np.reshape(X_test[22,:], (64,64,3)))\n",
        "# cv2_imshow(np.reshape(X_test[23,:], (64,64,3)))\n",
        "# cv2_imshow(np.reshape(X_test[25,:], (64,64,3)))\n",
        "# cv2_imshow(np.reshape(X_test[26,:], (64,64,3)))\n",
        "# cv2_imshow(np.reshape(X_test[27,:], (64,64,3)))\n",
        "# cv2_imshow(np.reshape(X_test[35,:], (64,64,3)))\n",
        "# cv2_imshow(np.reshape(X_test[39,:], (64,64,3)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z69CgcHOUqcs",
        "colab_type": "text"
      },
      "source": [
        "## Identification\n",
        "In an identification setup the goal is to **compute similarity scores** between pairs of data examples and use them to identify new images. \n",
        "In this exercise you will:\n",
        "- **compute the feature representation of every image** and \n",
        "- **create a data space using your training data**, \n",
        "- **give each of the data points a label based on the person they contain**.\n",
        "\n",
        "Next try to **identify the test images using k-NN**. \n",
        "\n",
        "* What value did you use for k? \n",
        "* Was the identification of person A and B successful for all descriptors? \n",
        "* How did images of person C and D get labeled?\n",
        "\n",
        "\n",
        "===\n",
        "My understanding:\n",
        "> * stepA: compute for, all the training set images, the handcrafted feature representation selected (for instance, HOG)\n",
        "> * stepB: take one particular test image, compute its HOG\n",
        "> * stepC: compute similarity score between HOG_test_image and all other training set images HOG representations computed in stepA\n",
        "> * stepD: label according to k-NN\n",
        "> * stepE: repeat for all other test images (including for personC and personD)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Gnqa7bRgqqP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.metrics.pairwise import cosine_distances\n",
        "from sklearn.metrics.pairwise import euclidean_distances\n",
        "from sklearn.metrics.pairwise import manhattan_distances"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "azrVkyB8PKr-",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "```\n",
        ">>> from numpy.linalg import norm\n",
        ">>> norm(X, axis=1, ord=1)  # L-1 norm\n",
        ">>> norm(X, axis=1, ord=2)  # L-2 norm\n",
        ">>> norm(X, axis=1, ord=np.inf)  # L-∞ norm\n",
        "```\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wMYJIBugmkO9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plot_similarity_matrix(similarity_matrix, show_numbers = False, vmax1 = None, vmax2 = None, norm_only=False, width=16, height = 8, fontsize = 10):\n",
        "    similarity_matrix_norm = 100*similarity_matrix / np.linalg.norm(similarity_matrix, axis = 1, ord=1, keepdims=True)\n",
        "\n",
        "    if norm_only:\n",
        "        raise NotImplementedError    \n",
        "    else:\n",
        "        fig, ax = plt.subplots(ncols=2)\n",
        "        fig.set_size_inches(width, height)\n",
        "    \n",
        "        im1 = ax[0].imshow(similarity_matrix, vmax=vmax1, cmap=\"jet\")\n",
        "        ax[0].set_title('as is')\n",
        "        im2 = ax[1].imshow(similarity_matrix_norm, vmax=vmax2, cmap=\"jet\")\n",
        "        ax[1].set_title('%')\n",
        "        dividers = [make_axes_locatable(a) for a in ax]\n",
        "        cax1, cax2 = [divider.append_axes(\"right\", size=\"5%\", pad=0.1) for divider in dividers]\n",
        "    \n",
        "        fig.colorbar(im1, cax=cax1)\n",
        "        fig.colorbar(im2, cax=cax2)\n",
        "\n",
        "        if show_numbers:\n",
        "            for i in range(similarity_matrix.shape[0]):\n",
        "                for j in range(similarity_matrix.shape[1]):\n",
        "                    ax[0].text(j, i, str(similarity_matrix[i,j]), fontsize=fontsize,va='center', ha='center')\n",
        "                    ax[1].text(j, i, str(round(similarity_matrix_norm[i,j],0)).rstrip('.0'),fontsize=fontsize,va='center', ha='center')\n",
        "    fig.tight_layout()\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JAZ67dFriP-_",
        "colab_type": "text"
      },
      "source": [
        "#### Illustration of distance measures\n",
        "\n",
        "\n",
        "In order to better understand the vizualisation used, a tool example is given hereunder. The input is a matrix handmade. Later, it will be the matrix of shape (#test_image, #train_image) containing the similarity (or reversely, distance) measures pairwise.\n",
        "\n",
        "Regarding the input matrix:\n",
        "- on each line, the ratio between number is kept the same\n",
        "- the five rows contain three different scale of the numbers:    \n",
        "\n",
        "```\n",
        "[[ 100,  20,  30,  40,  50], \n",
        " [   2,  10,   3,   4,   5], \n",
        " [  20,  30, 100,  40,  50],\n",
        " [   2,   3,   4,  10,   5], \n",
        " [ 200, 300, 400, 500,1000]]\n",
        "```\n",
        "\n",
        "\n",
        "The numbers written indicate the value of the cell.\n",
        "\n",
        "**Left side**: The \"As is\" matrix colors the cell as the numbers are set. The colorscale is therefore really large, and it can be useful to observe the disparity of measures across all tests. \n",
        "\n",
        "If the represented matrix is a distance matrix, for instance, one can observe that the 5th test image is *very far* from all the training images, as the 4th row as larger numbers than any other row. \n",
        "The drawback is that if some numbers are much higher than others, we lose in granularity to represent the differences between those numbers. For instance, a distance of \"2\" and a distance of \"20\" are very much alike.\n",
        "\n",
        "**Right side**: The figure shows the same input matrix but normalized by row. That means that the sum of all numbers in a row is equal to 100 %. \n",
        "It is helpful to analyze *locally* the distance/similarity values for 1 specific test image with respect to all training images. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HHDxUAmWI1_H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_dist_mtx = np.array([[100,20,30,40,50], [2,10,3,4,5], [20,30,100,40,50],[2,3,4,10,5], [200,300,400,500,1000]])\n",
        "plot_similarity_matrix(test_dist_mtx, show_numbers=True, width=8, height=4, fontsize=14)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r7YRyCUtu1fh",
        "colab_type": "text"
      },
      "source": [
        "In the next sections of this tutorial, these vizualisations will be used to detail the similarity/distance measures between our different images. \n",
        "The vertical axis correspond to test images, with the following mapping\n",
        "- 0 -> 9: images of PersonA, Emma Stone\n",
        "- 10 -> 19: images of PersonB, Bradley Cooper\n",
        "- 20 -> 29: images of PersonC, Jane Levy\n",
        "- 30 -> 39: images of PersonD, Marc Blucas\n",
        "\n",
        "The horizontal axis corresponds to the training images, with the following mapping:\n",
        "- 0 -> 19: images of PersonA (training set)\n",
        "- 20 -> 39: images of PersonB (training set)\n",
        "\n",
        "If the feature descritions are appropriate, the features distance measurements should lead to\n",
        "- very small distance (= large similarity measure) between training and test images of PersonA; and similarly for personB)\n",
        "- very high distance (= small similarity measure) between training and test images of PersonA; and similarly for personB)\n",
        "\n",
        "==> Persons from the same class would share feature descriptions, and not share other class feature description\n",
        "\n",
        "Intuitively:\n",
        "- feature description of personC should be closer (less distant, more similar) to personA than personB;\n",
        "- feature descriptions of personD should be closer to personB than personA."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CDsTsCnRWqqN",
        "colab_type": "text"
      },
      "source": [
        "### based on HOG feature decriptors\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u9icKavSYH8d",
        "colab_type": "text"
      },
      "source": [
        "#### Pre-process data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RmUs-jTEYBrt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train = get_matrix_from_set(training_set, color, sq_size = sq_size, flatten = False)\n",
        "X_test = get_matrix_from_set(test_set, color, sq_size = sq_size, flatten = False)\n",
        "\n",
        "y_train = np.zeros((40,))\n",
        "y_train[20:40] = 1\n",
        " \n",
        "\"\"\"\n",
        "For now, set up \"0\" for personC; \"1\" for personD !\n",
        "\"\"\"\n",
        "y_test = np.zeros((40,))\n",
        "y_test[10:20] = 1\n",
        "y_test[30:40] = 1\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RA82xnpT6Ui6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "logging.info(\"Training set (horizontal axis):\")\n",
        "logging.info(\"Test set PersonA [0 -> 19]:\")\n",
        "plot_matrix(X_train[0:20,:], color, my_color_map, h=1, w=20, transpose = False)\n",
        "logging.info(\"Test set PersonA [20 -> 39]:\")\n",
        "plot_matrix(X_train[20:40,:], color, my_color_map, h=1, w=20, transpose = False)\n",
        "\n",
        "logging.info(\"Test set (vertical axis):\")\n",
        "logging.info(\"Test set PersonA [0 -> 9]:\")\n",
        "plot_matrix(X_test[0:10, :], color, my_color_map, h=1, w=10, transpose = False)\n",
        "logging.info(\"Test set PersonB [10 -> 19]:\")\n",
        "plot_matrix(X_test[10:20, :], color, my_color_map, h=1, w=10, transpose = False)\n",
        "logging.info(\"Test set PersonC [20 -> 29]:\")\n",
        "plot_matrix(X_test[20:30, :], color, my_color_map, h=1, w=10, transpose = False)\n",
        "logging.info(\"Test set PersonD [30 -> 39]:\")\n",
        "plot_matrix(X_test[30:40, :], color, my_color_map, h=1, w=10, transpose = False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lwnOQwJYGKWZ",
        "colab_type": "text"
      },
      "source": [
        "Creation of Transformer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XNh9vtMhWyiq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "hogify = None\n",
        "hogify = HogTransformer(    orientations = 9, \n",
        "                            pixels_per_cell = (16,16), \n",
        "                            cells_per_block = (2,2),\n",
        "                            block_norm = \"L2-Hys\", \n",
        "                            transform_sqrt = False,\n",
        "                            multichannel = color)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jzGz5DZeGSMY",
        "colab_type": "text"
      },
      "source": [
        "Transformation of Inputs\n",
        " - **X_train**: application of the fit then transform method from the transformer\n",
        " - **X_test**: application of the transform method from the transformer\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c-l2ipZWGRVM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train_hog = hogify.fit_transform(X_train)\n",
        "X_test_hog = hogify.transform(X_test)\n",
        "\n",
        "logging.debug(\"X_train_hog Shape: \" + str(X_train_hog.shape))\n",
        "logging.debug(\"X_test_hog Shape: \" + str(X_test_hog.shape))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QfiKUASmjrIg",
        "colab_type": "text"
      },
      "source": [
        "#### compute similarities pairwise\n",
        "using cosine similarities\n",
        "=> explain why"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2NdCbPXpkBIe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# hog_similarities = cosine_similarity(X_test_hog, X_train_hog)\n",
        "hog_distances_cos = cosine_distances(X_test_hog, X_train_hog)\n",
        "hog_distances_eucl = euclidean_distances(X_test_hog, X_train_hog)\n",
        "# plot_similarity_matrix(hog_similarities)\n",
        "logging.debug(\"DISTANCE BASED ON COSINE\")\n",
        "plot_similarity_matrix((hog_distances_cos))\n",
        "\n",
        "logging.debug(\"DISTANCE BASED ON COSINE - LOG10\")\n",
        "plot_similarity_matrix(np.log10(hog_distances_cos))\n",
        "\n",
        "logging.debug(\"DISTANCE BASED ON EUCL\")\n",
        "plot_similarity_matrix((hog_distances_eucl))\n",
        "\n",
        "logging.debug(\"DISTANCE BASED ON EUCL - LOG10\")\n",
        "plot_similarity_matrix(np.log10(hog_distances_eucl))\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MV_iOmO5re1s",
        "colab_type": "text"
      },
      "source": [
        "kNN\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rtp95kE0reS2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn import metrics\n",
        "\n",
        "k=1\n",
        "knn = KNeighborsClassifier(n_neighbors = k, metric='euclidean')\n",
        "knn.fit(X_train_hog, y_train)\n",
        "y_pred_hog = knn.predict(X_test_hog)\n",
        "logging.info(\"Percentage correct: \" + str(100*np.sum( y_test == y_pred_hog)/len(y_test)))\n",
        "logging.info(\"Percentage correct [SKlearn]: \" + str(metrics.accuracy_score(y_test, y_pred_hog)))\n",
        "logging.debug(\"y_pred => \" + str(y_pred_hog))\n",
        "print(knn.kneighbors(X_test_hog[0,:].reshape(1, -1), n_neighbors=40))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eCFWdMr3ykUD",
        "colab_type": "text"
      },
      "source": [
        "idées : \n",
        "> selectionner les images de tests et montrer leurs nearest neighbours; \n",
        "    - quand ça va bien\n",
        "    - quand ça va pas bien\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lebK99WQWy9H",
        "colab_type": "text"
      },
      "source": [
        "### based on PCA feature descriptors\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AwsBqLUqHXLA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train = get_matrix_from_set(training_set, color, sq_size = sq_size, flatten = True)\n",
        "X_test = get_matrix_from_set(test_set, color, sq_size = sq_size, flatten = True)\n",
        "y_train = np.zeros((40,))\n",
        "y_train[20:40] = 1\n",
        " \n",
        "\"\"\"\n",
        "For now, set up \"0\" for personC; \"1\" for personD !\n",
        "\"\"\"\n",
        "y_test = np.zeros((40,))\n",
        "y_test[10:20] = 1\n",
        "y_test[30:40] = 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zonhIEds0J70",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "logging.info(\"Training set (horizontal axis):\")\n",
        "logging.info(\"Test set PersonA [0 -> 19]:\")\n",
        "plot_matrix(X_train[0:20,:], color, my_color_map, h=1, w=20, transpose = False)\n",
        "logging.info(\"Test set PersonA [20 -> 39]:\")\n",
        "plot_matrix(X_train[20:40,:], color, my_color_map, h=1, w=20, transpose = False)\n",
        "\n",
        "logging.info(\"Test set (vertical axis):\")\n",
        "logging.info(\"Test set PersonA [0 -> 9]:\")\n",
        "plot_matrix(X_test[0:10, :], color, my_color_map, h=1, w=10, transpose = False)\n",
        "logging.info(\"Test set PersonB [10 -> 19]:\")\n",
        "plot_matrix(X_test[10:20, :], color, my_color_map, h=1, w=10, transpose = False)\n",
        "logging.info(\"Test set PersonC [20 -> 29]:\")\n",
        "plot_matrix(X_test[20:30, :], color, my_color_map, h=1, w=10, transpose = False)\n",
        "logging.info(\"Test set PersonD [30 -> 39]:\")\n",
        "plot_matrix(X_test[30:40, :], color, my_color_map, h=1, w=10, transpose = False)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HXqKpzqeFauR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pcaify = None\n",
        "pcaify = sklearn_decomposition_PCA(n_components = 35)\n",
        "\n",
        "X_train_PCA = pcaify.fit_transform(X_train)\n",
        "X_test_PCA = pcaify.transform(X_test)\n",
        "\n",
        "\n",
        "# pca_similarities = cosine_similarity(X_test_PCA, X_train_PCA)\n",
        "pca_distances_cos = cosine_distances(X_test_PCA, X_train_PCA)\n",
        "pca_distances_eucl = euclidean_distances(X_test_PCA, X_train_PCA)\n",
        "# plot_similarity_matrix(pca_similarities)\n",
        "# plot_similarity_matrix(np.log(pca_distances_cos))\n",
        "plot_similarity_matrix((pca_distances_eucl))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p_JWPGPAHt8j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "k=1\n",
        "knn = KNeighborsClassifier(n_neighbors = k, metric='euclidean')\n",
        "knn.fit(X_train_PCA, y_train)\n",
        "y_pred_PCA = knn.predict(X_test_PCA)\n",
        "logging.info(\"Percentage correct: \" + str(100*np.sum( y_test== y_pred_PCA )/len(y_test)))\n",
        "logging.info(\"Percentage correct [SKlearn]: \" + str(metrics.accuracy_score(y_test, y_pred_PCA )))\n",
        "logging.debug(\"Y Pred => \" + str(y_pred_PCA))\n",
        "print(knn.kneighbors(X_test_PCA [0,:].reshape(1, -1), n_neighbors=40))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9qCMI3Tyy0DM",
        "colab_type": "text"
      },
      "source": [
        "## Observations and Comments\n",
        "- PCA distance measures seem more fuzzy than HOG feature descriptors\n"
      ]
    }
  ]
}