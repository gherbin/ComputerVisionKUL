{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CV_Group9_assignment.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [
        "83XvyfDBSLQX",
        "XQ204pRs2P2i"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gherbin/ComputerVisionKUL/blob/master/CV_Group9_assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZsHsvp6wb1n2",
        "colab_type": "text"
      },
      "source": [
        "# Hi there! \n",
        "Welcome to this Colab where we'll dig into some Computer Vision fancy stuff!\n",
        "\n",
        "The goals of this notebook is to perform faces classification and identification. To reach that goals we will first:\n",
        "- retrieve training and test images\n",
        "- build two features\n",
        "    1. handcrafted feature: Histogram of Oriented Gradients\n",
        "    2. Learnt from the data: Principal Component Analysis\n",
        "\n",
        "Then, we will train a model based on each feature and compare the classification and identification results.\n",
        "\n",
        "---\n",
        "\n",
        "Several libraries will be extensively used as optimized for some techniques we need. However, at first, some of the key functionalities will be coded as to provide a better view of what really happens being the calls to library functions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SnkjeOT3GuBy",
        "colab_type": "text"
      },
      "source": [
        "## Import all required packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KH_DPxxjTeoI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import cv2\n",
        "import tarfile\n",
        "import zipfile\n",
        "import shutil\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import random\n",
        "import logging\n",
        "\n",
        "from urllib import request\n",
        "from socket import timeout\n",
        "from urllib.error import HTTPError, URLError\n",
        "\n",
        "from google.colab import drive\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "from distutils.dir_util import copy_tree\n",
        "\n",
        "from skimage.feature import hog as skimage_feature_hog\n",
        "from sklearn.decomposition import PCA as sklearn_decomposition_PCA\n",
        "from skimage import exposure\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "from math import sqrt\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "from matplotlib.gridspec import GridSpec\n",
        "from matplotlib.offsetbox import OffsetImage, AnnotationBbox\n",
        "%matplotlib inline\n",
        "\n",
        "from scipy.interpolate import RectBivariateSpline\n",
        "from scipy.linalg import svd as scipy_linalg_svd\n",
        "\n",
        "logging.basicConfig(level=logging.DEBUG)\n",
        "\n",
        "mpl_logger = logging.getLogger(\"matplotlib\")\n",
        "mpl_logger.setLevel(logging.WARNING)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "83XvyfDBSLQX",
        "colab_type": "text"
      },
      "source": [
        "## Utils functions\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "05JhwSLCSPyI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def pretty_return_dict_size(my_dict):\n",
        "    \"\"\"\n",
        "    returns a string containing the different size of the elements of a dict\n",
        "    \"\"\"\n",
        "    output_list = [\"\\n\"]\n",
        "    for k in my_dict.keys():\n",
        "        output_list.append(str(k))\n",
        "        output_list.append(\":\")\n",
        "        output_list.append(str(len(my_dict[k])))\n",
        "        output_list.append(\"\\n\")\n",
        "    return ''.join(output_list)\n",
        "\n",
        "def show_images_from_dict(my_dict, show_index = False):\n",
        "    \"\"\"\n",
        "    shows the images contained in a dictionary, going through all keys\n",
        "    \"\"\"\n",
        "    for k in my_dict.keys():\n",
        "        logging.debug(\"@------------------- Images of \" + str(k) + \" -------------------@\")\n",
        "        index = 0\n",
        "        for img in my_dict[k]:\n",
        "            if show_index:\n",
        "                logging.debug(\"Image index: \" + str(index))\n",
        "                index+=1\n",
        "            cv2_imshow(img)\n",
        "            logging.debug(\"Shape = \" + str(img.shape))\n",
        "def get_min_size(images_dict):\n",
        "    \"\"\"\n",
        "    returns the minimum size of images contained in the images_dict input\n",
        "    \"\"\"\n",
        "    min_rows, min_cols = float(\"inf\"), float(\"inf\")\n",
        "    max_rows, max_cols = 0, 0\n",
        "    for person in persons:\n",
        "        for src in images_dict[person]:\n",
        "            r, c = src.shape[0], src.shape[1]    \n",
        "            min_rows = min(min_rows, r)\n",
        "            max_rows = max(max_rows, r)\n",
        "            min_cols = min(min_cols, c)\n",
        "            max_cols = max(max_cols, c)\n",
        "    logging.info(\"smallest px numbers (row, cols) = \" + str((min_rows,min_cols)))\n",
        "    return min_rows, min_cols\n",
        "        \n",
        "def my_reshape(image_vector, sq_size, color):\n",
        "    \"\"\"\n",
        "    returns a reshape version of an image represented as an image array, depending of the color parameter.\n",
        "    If color is True, it returns a colored RGB format image of size (sq_size x sq_size) (useable as is by matplotlib)\n",
        "    If color is False, it returns a grayscale image (sq_size x sq_size)\n",
        "    \"\"\"\n",
        "    # cv2_imshow(image_vector)\n",
        "    # logging.debug(\"#############################\")\n",
        "    # print(\"Image_vector shape = \" + str(image_vector.shape))\n",
        "\n",
        "    flattened = image_vector.ndim == 1\n",
        "    # logging.debug(\"Image_vector flattened = \" + str(flattened))\n",
        "    # logging.debug(\"Image_vector color = \" + str(color))\n",
        "\n",
        "    if flattened:\n",
        "        if color:\n",
        "            img_reshaped = (np.reshape(image_vector, (sq_size, sq_size, 3))).astype('uint8')\n",
        "            return cv2.cvtColor(img_reshaped, cv2.COLOR_BGR2RGB)\n",
        "        else:\n",
        "            return np.reshape(image_vector, (sq_size, sq_size))\n",
        "    else:\n",
        "        if color:\n",
        "            img_reshaped = (np.reshape(image_vector, (sq_size, sq_size, 3))).astype('uint8')\n",
        "            return cv2.cvtColor(img_reshaped, cv2.COLOR_BGR2RGB)\n",
        "        else:\n",
        "            return image_vector\n",
        "\n",
        "def get_matrix_from_set(images_set, color, sq_size = 64, flatten = True):\n",
        "    \"\"\"\n",
        "    from images_set (training_set or test_set), create and fill in matrix so that it contains the input data.\n",
        "    if flatten, then the matrix contains images represented in 1D\n",
        "    \"\"\"\n",
        "\n",
        "    # init output\n",
        "    matrix = None\n",
        "    nb_faces = sum([len(images_set[x]) for x in images_set if isinstance(images_set[x], list)])\n",
        "    # depending on mode, select appropriate size items. N\n",
        "    \n",
        "    if color and flatten:\n",
        "        matrix = np.empty((nb_faces, sq_size*sq_size*3)) # *3 => color images\n",
        "    elif color and (not flatten):\n",
        "        matrix = np.empty((nb_faces, sq_size, sq_size, 3))\n",
        "    elif (not color) and flatten:\n",
        "        matrix = np.empty((nb_faces, sq_size*sq_size))\n",
        "    elif (not color) and (not flatten):\n",
        "        matrix = np.empty((nb_faces, sq_size,sq_size ))\n",
        "    else:\n",
        "        raise RuntimeError\n",
        "\n",
        "    i = 0\n",
        "    for person in persons:\n",
        "        for src in images_set[person]:\n",
        "            src_rescaled = cv2.resize(src, (sq_size,sq_size))\n",
        "            if color and flatten:\n",
        "                matrix[i,:] = src_rescaled.flatten()\n",
        "            elif color and (not flatten):\n",
        "                matrix[i,:,:,:] = src_rescaled\n",
        "            elif (not color) and flatten:\n",
        "                matrix[i,:] = cv2.cvtColor(src_rescaled, cv2.COLOR_BGR2GRAY).flatten()\n",
        "            elif (not color) and (not flatten):\n",
        "                matrix[i,:,:] = cv2.cvtColor(src_rescaled, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "            i +=1\n",
        "    return matrix\n",
        "\n",
        "def plot_matrix(images_matrix, color, my_color_map, h=8, w=5, transpose = False):\n",
        "    fig = plt.figure(figsize=(w,h)) \n",
        "    fig.subplots_adjust(left=0, right=1, bottom=0, top=1, hspace=0.05, wspace=0.05) \n",
        "    # plot the faces, each image is 64 by 64 pixels \n",
        "\n",
        "    if transpose:\n",
        "        images_matrix_used = images_matrix.T.copy()\n",
        "    else:\n",
        "        images_matrix_used = images_matrix.copy()\n",
        "\n",
        "    i=0\n",
        "    for img_vector in images_matrix_used: \n",
        "        ax = fig.add_subplot(h, w, i+1, xticks=[], yticks=[]) \n",
        "        ax.imshow(my_reshape(img_vector, sq_size, color), cmap = my_color_map, interpolation='nearest') \n",
        "        i+=1\n",
        "\n",
        "        \n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ebQpRYW50fS2",
        "colab_type": "text"
      },
      "source": [
        "### Inputs\n",
        "\n",
        "The very first input of the system is an archive containing several text file containing each 1000 weblinks to images. This archive is downloaded from [here](http://www.robots.ox.ac.uk/~vgg/data/vgg_face) and extracted locally in `/content/sample_data/CV__Group_assignment` folder.\n",
        "\n",
        "To recognize faces in the images, we download the Haarcascade model.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BGBkRUir9f7g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "base_path = \"/content/sample_data/CV__Group_assignment\"\n",
        "\n",
        "if not os.path.isdir(base_path):\n",
        "  os.makedirs(base_path)\n",
        "\n",
        "vgg_face_dataset_url = \"http://www.robots.ox.ac.uk/~vgg/data/vgg_face/vgg_face_dataset.tar.gz\"\n",
        "\n",
        "with request.urlopen(vgg_face_dataset_url) as r, open(os.path.join(base_path, \"vgg_face_dataset.tar.gz\"), 'wb') as f:\n",
        "  f.write(r.read())\n",
        "\n",
        "with tarfile.open(os.path.join(base_path, \"vgg_face_dataset.tar.gz\")) as f:\n",
        "  f.extractall(os.path.join(base_path))\n",
        "\n",
        "trained_haarcascade_url = \"https://raw.githubusercontent.com/opencv/opencv/master/data/haarcascades/haarcascade_frontalface_default.xml\"\n",
        "\n",
        "with request.urlopen(trained_haarcascade_url) as r, open(os.path.join(base_path, \"haarcascade_frontalface_default.xml\"), 'wb') as f:\n",
        "    f.write(r.read())\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XQ204pRs2P2i",
        "colab_type": "text"
      },
      "source": [
        "## Parameters\n",
        "\n",
        "As all computerized system, several parameters help in defining how the system should react. \n",
        "Those parameters are centralized here."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ueU9Z-iMU63f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "load_from_local_drive = True # allows downloading the source images directly from the archive in github repository (see \"important note\")\n",
        "show = True # similar as global verbose parameter, for images (when custom functions allows it)\n",
        "sq_size = 64 # square size used -> shall be smaller than the output of get_min_size(faces_cropped) # assert sq_size <= min(get_min_size(faces_cropped))\n",
        "\n",
        "color = False\n",
        "if color:\n",
        "    my_color_map = plt.cm.viridis\n",
        "else:\n",
        "    my_color_map = plt.cm.gray"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MOH3DL2jKk3U",
        "colab_type": "text"
      },
      "source": [
        "# Data Retrieval"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lQPukQmEpoRC",
        "colab_type": "text"
      },
      "source": [
        "This tutorial will extensively use images from four different actors. The images are selected (pseudo-)randomly.\n",
        "\n",
        "The movie stars are (chosen quite randomly as well):\n",
        "1.   personA: Emma Stone\n",
        "2.   personB: Bradley Cooper\n",
        "3.   personC: Jane Levy\n",
        "4.   personD: Marc Blucas\n",
        "\n",
        "\n",
        "Process to get datasets images:\n",
        "1. Randomly pick N images (60 for persons A and B, and 30 for persons C and D) images from the list of 1000 images provided in the textfile. \n",
        "2. Select M images randomly out of the N images obtained for each persons:\n",
        "    - M=30 for personA and personB (Training and Test),\n",
        "    - M=10 for personC and personD (Test only)\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "**[IMPORTANT NOTE]**\n",
        "\n",
        "If nothing else is set up, getting the N images require to perform a url request on websites we do not control. This is risky, as for any reason, the target website could be modified, not responding, responding too slowly, have removed the picture of interest, ...\n",
        "To prevent such issue, this tutorial provide the code to do things differently.\n",
        "- the first time (with some parameters below properly set), the source images are downloaded from the website (retrieving errors, skipping too slow website, etc.)\n",
        "- the images, downloaded, are then saved and zipped with a logfile\n",
        "- this zip archive is then uploaded on my personal Github account, as a public file\n",
        "\n",
        "It leads to a controlled database constaining the source images, and ensure reproducibility during the different test run.\n",
        "\n",
        "*Three remarks*\n",
        "1. only the original files are stored in the archive in the ZIP. Those files were selected randomly, using a random number generator.\n",
        "2. the curation of the source files, the face cropping, and selection between training and test sets is still done at every run of this notebook.\n",
        "3. the code dedicated to the archiving and saving part will not be detailed in this notebook, but surely, you are welcome to contact me for more details using geoffroy.herbin@student.kuleuven.be.\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cFh6oMU6phLs",
        "colab_type": "text"
      },
      "source": [
        "Start from clean sheet\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NAMpaW-Ym0oG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "path_datasets = r\"/content/datasets/\"\n",
        "path_discard = r\"/content/discard/\"\n",
        "path_database = r\"/content/DATABASE/\"\n",
        "\n",
        "try:\n",
        "    shutil.rmtree(path_database)\n",
        "    shutil.rmtree(path_datasets)\n",
        "    shutil.rmtree(path_discard)\n",
        "except:\n",
        "    pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I4Rj5OSS3fUx",
        "colab_type": "text"
      },
      "source": [
        "Create required folders"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hQw39iLK36m1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "file_info = path_database+ r\"info_retrieved.txt\"\n",
        "\n",
        "try: \n",
        "    os.mkdir(path_database)\n",
        "    os.mkdir(path_datasets) \n",
        "    os.mkdir(path_discard)\n",
        "except OSError as error: \n",
        "    logging.error(error) \n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JuEfFZE42YeO",
        "colab_type": "text"
      },
      "source": [
        "Instead of randomly download from web, download images from a \"clean\" and controlled repository (in [github](https://raw.githubusercontent.com/gherbin/cv_group9_database_replica/master/DATABASE-20200318T142918Z-001.zip) ), dedicated for this notebook. It ensures reproducibility and accessibility to the 180 input images.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TgaFPa0C2YJH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if load_from_local_drive:\n",
        "    \n",
        "    !wget https://raw.githubusercontent.com/gherbin/cv_group9_database_replica/master/DATABASE-20200318T142918Z-001.zip\n",
        "\n",
        "    with zipfile.ZipFile(\"DATABASE-20200318T142918Z-001.zip\", 'r') as zip_ref:\n",
        "        zip_ref.extractall()\n",
        "    !rm -r \"DATABASE-20200318T142918Z-001.zip\"\n",
        "\n",
        "path_, dirs_, files = next(os.walk(path_database))\n",
        "if len(files) == 180+1:\n",
        "    logging.info(\"Successful database retrieval\")\n",
        "elif load_from_local_drive:\n",
        "    logging.error(\"Most Likely problem with database retrieval, number of files = \" + str(len(files)))\n",
        "else:\n",
        "    logging.info(\"No database images retrieved yet\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M09QQcnf38xN",
        "colab_type": "text"
      },
      "source": [
        "###Definition of several data structures\n",
        "\n",
        "`images_size` : dictionary containing the number of images to first get from the web\n",
        "\n",
        "`persons` : list containing the names of the four persons (the names actually are the name of the text file in original database)\n",
        "\n",
        "`images`: dictionary containing the source images. The keys of the dictionary are the names of the four persons of interest"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UEGMZ43b4iaa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "personA = \"Emma_Stone.txt\"\n",
        "personC = \"Jane_Levy.txt\"\n",
        "personB = \"Bradley_Cooper.txt\"\n",
        "personD = \"Marc_Blucas.txt\"\n",
        "persons = [personA, personB, personC, personD]\n",
        "datasets_dict = {}\n",
        "images_size = {}\n",
        "images_size[personA] = 60\n",
        "images_size[personB] = 60\n",
        "images_size[personC] = 30\n",
        "images_size[personD] = 30\n",
        "\n",
        "total_images_size = sum(images_size.values())\n",
        "\n",
        "# Dictionary containing the ids of the pictures downloaded from internet\n",
        "vgg_ids = {}\n",
        "for p in persons:\n",
        "    vgg_ids[p] = []\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8qdfeFWLAL6C",
        "colab_type": "text"
      },
      "source": [
        "If `confirmation` is `True`, the following code picks randomly (based on a seed being the name of the person) the images from the web.\n",
        "\n",
        "For a normal run, if the user does not want to change the original sourced data, `confirmation` should remain `False` (aka *change at your own risk* ;-) )\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I5VsGBAJ3inK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "confirmation = False\n",
        "if confirmation:\n",
        "    try:\n",
        "        shutil.rmtree(path_database)\n",
        "    except:\n",
        "        pass \n",
        "    try:\n",
        "        os.mkdir(path_database)\n",
        "    except:\n",
        "        pass \n",
        "\n",
        "    fo = open(file_info, \"w+\")\n",
        "\n",
        "    # images = {}\n",
        "    # images_nominal_indices = {}\n",
        "    for person in persons:\n",
        "        logging.debug(\"Taking care of: \" + str(person))\n",
        "        random.seed(person)\n",
        "        # print(hash(person))\n",
        "        images_ = []\n",
        "        # images_nominal_indices_ = []\n",
        "        prev_index = []\n",
        "\n",
        "\n",
        "        with open(os.path.join(base_path, \"vgg_face_dataset\", \"files\", person), 'r') as f:\n",
        "            lines = f.readlines()       \n",
        "        \n",
        "\n",
        "        while len(images_) < images_size[person]:\n",
        "            index = random.randrange(0, 1000)\n",
        "            logging.debug(\"Index = \" + str(index))\n",
        "            if index in prev_index:\n",
        "                logging.debug(\"Index = \" + str(index) + \" => already there\")\n",
        "                continue\n",
        "            else:\n",
        "                prev_index.append(index)\n",
        "                line = lines[index]\n",
        "                # only curated data\n",
        "                if int(line.split(\" \")[8]) == 1:\n",
        "                    url = line[line.find(\"http://\"): line.find(\".jpg\") + 4]\n",
        "                    logging.debug(\"URL > \\\"\" + str(url))\n",
        "                    try:\n",
        "                        res = request.urlopen(url, timeout = 1)\n",
        "                        img = np.asarray(bytearray(res.read()), dtype=\"uint8\")\n",
        "                        img = cv2.imdecode(img, cv2.IMREAD_COLOR)\n",
        "\n",
        "                        h, w = img.shape[:2]\n",
        "                        cv2_imshow(cv2.resize(img, (w//4, h//4)))\n",
        "                        # images_nominal_indices_.append(index)\n",
        "\n",
        "                        filename = path_database +  str(index) + \"_\" + str(person.split(\".\")[0]) + \".jpg\"\n",
        "\n",
        "                        value = cv2.imwrite(filename, img) \n",
        "                        # logging.debug(\"saved in DB: \" + str(filename))\n",
        "                        images_.append(img)\n",
        "                        fo.write(line)\n",
        "                    except ValueError as e:\n",
        "                            logging.error(\"Value Error >\" + str(e))\n",
        "                    except (HTTPError, URLError) as e:\n",
        "                            logging.error('ERROR RETRIEVING URL >' + str(e))\n",
        "                    except timeout:\n",
        "                            logging.error('socket timed out - URL %s', str(url))\n",
        "                    except cv2.error as e: \n",
        "                            logging.error(\"ERROR WRITING FILE IN DB  >\" + str(e))\n",
        "                    except:\n",
        "                        logging.error(\"Weird exception : \" + str(line))\n",
        "                else:\n",
        "                    logging.debug(\"File not curated => rejected (id = \" + str(index) + \" )\")    \n",
        "                \n",
        "                # images[person] = images_\n",
        "                # images_nominal_indices[person] = images_nominal_indices_\n",
        "\n",
        "    fo.close()\n",
        "else:\n",
        "    logging.warning(\"If you really want to erase and renew the database, please change first the \\\"confirmation\\\" boolean variable, at the beginning of this cell\")\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P3eNIMAdCLVu",
        "colab_type": "text"
      },
      "source": [
        "From the logfile in the archive, extract the information and fill the dictionary containing all the images `images`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AdFihucjCO6e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open(file_info, 'r') as f: \n",
        "    lines = f.readlines()\n",
        "\n",
        "assert len(lines)==total_images_size, \"amount of lines in file incompatible\" \n",
        "\n",
        "images = {}\n",
        "\n",
        "for p in persons:\n",
        "    images[p] = []\n",
        "\n",
        "\n",
        "images_index = {}\n",
        "for running_index in range(len(lines)):\n",
        "    if running_index in range(0,images_size[personA]):\n",
        "        p = personA\n",
        "    elif running_index in range(images_size[personA],images_size[personA]+images_size[personB]):\n",
        "        p = personB\n",
        "    elif running_index in range(images_size[personA]+images_size[personB],images_size[personA]+images_size[personB]+images_size[personC]):\n",
        "        p = personC\n",
        "    elif running_index in range(images_size[personA]+images_size[personB]+images_size[personC],total_images_size):\n",
        "        p = personD\n",
        "    ind = str(int(lines[running_index].split(\" \")[0])-1)\n",
        "    vgg_ids[p].append(ind)\n",
        "    filename = ind + \"_\" + str(p.split(\".\")[0]) + \".jpg\"\n",
        "    images[p].append(cv2.imread(path_database+filename, cv2.IMREAD_COLOR))\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9v4tuRVO67u1",
        "colab_type": "text"
      },
      "source": [
        "From the sources files, although the images downloaded were part of a curated data, several images need to be removed to be used in the context of this *educative* tutorial. The main reasons are:\n",
        "\n",
        "*   too different from usual representation (make up, masks, ...)\n",
        "*   really poor image quality\n",
        "*   irrelevance and/or error in dataset\n",
        "*   same image already in dataset\n",
        "*   cropped image\n",
        "\n",
        "Considering the tight selection of images to train our model (20 from personA and 20 from personB), and the relatively large global amount of image candidates (1000 for each person), it is acceptable to reject the images we know won't help.\n",
        "\n",
        "From the initial retrieved images, we then remove the undesired images, that we copy in discard images folder, for tracking purposes. We/you may want to use them later.\n",
        "\n",
        "---\n",
        "`datasets_size`: dictionary of the size required per persons ( keys = person names)\n",
        "\n",
        "`to_remove`: dictionary containing the indices to remove, per persons ( keys = person names)\n",
        "\n",
        "`print_images = False` indicates that the remaining images in `images` dictionary will not be printed. \n",
        "\n",
        "---\n",
        "\n",
        "From the remaining images (after rejection), we can apply randomly select the images that are part of the final sets (training and test sets not split yet)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZPKRTSB5r8pg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Dictionary of the size required (see section 3)\n",
        "datasets_size = {}\n",
        "datasets_size[personA] = 30\n",
        "datasets_size[personB] = 30\n",
        "datasets_size[personC] = 10\n",
        "datasets_size[personD] = 10\n",
        "\n",
        "\n",
        "# manually remove images that are not relevant or considered not good enough to be part of the dataset\n",
        "to_remove = {}\n",
        "to_remove[personA] = [0,1,4,8,12,13,16,23,28,34,36,42,44,47,48,49,54]\n",
        "to_remove[personB] = [4,7,11,12,13,16,21,22,23,24,25,26,27,32,36,39,41,46,49,53,55,58]\n",
        "to_remove[personC] = [0,1,6,7,8,11,14,16,17,19,20,21,24]\n",
        "to_remove[personD] = [0,3,5,6,8,10,12,15,16,17,24]\n",
        "\n",
        "# goal is to sort in descending to remove elements from lists without modifying the indexes\n",
        "for p in persons:\n",
        "    to_remove[p].sort(reverse = True)\n",
        "\n",
        "\n",
        "# retrieve images candidates\n",
        "# --------------------------\n",
        "if len(os.listdir(path_datasets) ) == 0 or True:\n",
        "    logging.debug(\"datasets empty - need to retrieve all !\")\n",
        "    # removing images to discard\n",
        "    for person in persons:\n",
        "        for index in to_remove[person]:\n",
        "            img = images[person].pop(index)\n",
        "            logging.debug(\"Removing item \" + str(index) + \" from list \" + str(person))\n",
        "            try:\n",
        "                filename = path_discard +  str(index) +\"_discarded_\" + str(person.split(\".\")[0]) + \".jpg\"\n",
        "                cv2.imwrite(filename, img) \n",
        "            except:\n",
        "                logging.error(\"Error while writing discarded image \" + str(filename))\n",
        "\n",
        "    # randomly select among remaining images\n",
        "    for person in persons:\n",
        "        # build list of indices from remaining images\n",
        "        logging.debug(\"Phase 2 (part 1) -> random indices selection for \" + str(person))\n",
        "\n",
        "        images_ = []\n",
        "        indices = []\n",
        "        new_ids = []\n",
        "        # prev_index = []\n",
        "        random.seed(person)\n",
        "\n",
        "        while len(indices) < datasets_size[person]:       \n",
        "            index = random.randrange(0, len(images[person]))\n",
        "            if index in indices:\n",
        "                logging.debug(\"Index among remaining = \" + str(index) + \" => already there\")\n",
        "                continue\n",
        "            else:\n",
        "                # prev_index.append(index)\n",
        "                indices.append(index)\n",
        "\n",
        "        logging.debug(\"Phase 2 (part 2) -> image selection based on indices\")\n",
        "\n",
        "        for index in indices:\n",
        "            img = images[person][index]\n",
        "            images_.append(img)\n",
        "            filename = path_datasets +  str(vgg_ids[person][index]) + \"_\" + str(person.split(\".\")[0]) + \".jpg\"\n",
        "            logging.debug(\"saved: \" + str(filename))\n",
        "            cv2.imwrite(filename, img) \n",
        "            new_ids.append(vgg_ids[person][index])\n",
        "        images[person] = images_\n",
        "        vgg_ids[person] = new_ids\n",
        "else:\n",
        "    logging.debug(\"folders not empty => can build directly images dictionnary\")\n",
        "\n",
        "# logging.debug(\"Number of images keys=\" + len(images.keys))\n",
        "# logging.debug(\"Number of images values=\" + len(images.values))\n",
        "\n",
        "logging.info(pretty_return_dict_size(images))\n",
        "\"\"\" \n",
        "print images to get to_remove indices\n",
        "\"\"\"\n",
        "print_images = False\n",
        "if print_images:\n",
        "    for person in persons:\n",
        "        counter = 0\n",
        "        for img in images[person]:\n",
        "            h = 0\n",
        "            w = 0\n",
        "            img_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "            faces = faceCascade.detectMultiScale(\n",
        "                img_gray,\n",
        "                scaleFactor=1.13,\n",
        "                minNeighbors=10,\n",
        "                minSize=(30, 30),\n",
        "                flags=cv2.CASCADE_SCALE_IMAGE\n",
        "            )\n",
        "            for (x,y,w,h) in faces:\n",
        "                # faces_cropped[person].append(img[y:y+h, x:x+w])\n",
        "                cv2.rectangle(img, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
        "            logging.debug(\"------------------------------------------------------\")\n",
        "            logging.debug(\"Photo ID = \" + str(counter))\n",
        "            logging.debug(\"size = \" + str((h,w)))\n",
        "            cv2_imshow(img)\n",
        "            counter += 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jvExNNUtK5AU",
        "colab_type": "text"
      },
      "source": [
        "Mount drive and save images, according to the parameter `to_db_confirmation` value. *Change at your own risk ;-)*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XwJjZ1oaHEV9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Save to drive folders\n",
        "to_db_confirmation = False\n",
        "\n",
        "path_drive_DB = r\"/content/drive/My Drive/ComputerVision/DATABASE\"\n",
        "path_drive_Datasets = r\"/content/drive/My Drive/ComputerVision/DATASETS\"\n",
        "\n",
        "\n",
        "\n",
        "# drive folders should be properly set up\n",
        "\n",
        "if to_db_confirmation:\n",
        "    try:\n",
        "        drive.mount('/content/drive')\n",
        "    except:\n",
        "        pass\n",
        "    try:\n",
        "        shutil.rmtree(path_drive_DB)\n",
        "        shutil.rmtree(path_drive_Datasets)\n",
        "    except:\n",
        "        logging.error(\"Error in rmtree\") \n",
        "\n",
        "\n",
        "    try: \n",
        "        os.mkdir(path_drive_DB) \n",
        "        os.mkdir(path_drive_Datasets)\n",
        "    except OSError as error: \n",
        "        logging.error(error) \n",
        "    \n",
        "    logging.debug(\"Saving database in drive : start\")\n",
        "\n",
        "    fromDirectory = path_database\n",
        "    toDirectory = path_drive_DB\n",
        "    copy_tree(fromDirectory, toDirectory)\n",
        "\n",
        "    logging.debug(\"Saving datasets in drive : start\")\n",
        "\n",
        "    fromDirectory = path_datasets\n",
        "    toDirectory = path_drive_Datasets\n",
        "    copy_tree(fromDirectory, toDirectory)\n",
        "\n",
        "    logging.debug(\"Saving: done !\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9HilePrqbDWC",
        "colab_type": "text"
      },
      "source": [
        "From the raw images saved in the `images` dictionary, the faces are extracted using the *HaarCascade* method.\n",
        "\n",
        "The following code is based on the tutorial: [How to detect faces using Haar Cascade](https://www.digitalocean.com/community/tutorials/how-to-detect-and-extract-faces-from-an-image-with-opencv-and-python)\n",
        "\n",
        "The faces are saved in a new dictionary: `faces_cropped`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G582IgL-fvi1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "faceCascade = cv2.CascadeClassifier(os.path.join(base_path, \"haarcascade_frontalface_default.xml\"))\n",
        "faces_cropped = {}\n",
        "\n",
        "with open(file_info, 'r') as f: \n",
        "    lines = f.readlines()\n",
        "\n",
        "for person in persons:\n",
        "\n",
        "    faces_cropped[person] = []\n",
        "\n",
        "    for img in images[person]:\n",
        "        img_ = img.copy()\n",
        "        img_gray = cv2.cvtColor(img_, cv2.COLOR_BGR2GRAY)\n",
        "        faces = faceCascade.detectMultiScale(\n",
        "            img_gray,\n",
        "            scaleFactor=1.13,\n",
        "            minNeighbors=10,\n",
        "            minSize=(30, 30),\n",
        "            flags=cv2.CASCADE_SCALE_IMAGE\n",
        "        )\n",
        "        for (x,y,w,h) in faces:\n",
        "            faces_cropped[person].append(img[y:y+h, x:x+w])\n",
        "            cv2.rectangle(img_, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
        "\n",
        "        # h, w = img_.shape[:2]\n",
        "        # draw_box(lines, int(vgg_ids[person][running_index])+1, img_, person)\n",
        "        # cv2_imshow(cv2.resize(img_, (w // 2, h // 2)))\n",
        "logging.info(\"Faces extracted and saved in dictionnary faces_cropped\")\n",
        "logging.info(pretty_return_dict_size(faces_cropped))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zVrtCWGgRFKX",
        "colab_type": "text"
      },
      "source": [
        "At this point, `faces_cropped` dictionary contains 30 cropped faces for personA and B, and 10 images for personC and personD.\n",
        "\n",
        "The following code selects randomly (based on a seed) the 20 images part of the training set for personA and personB. The other faces (10 for each person) are then part of the test set.\n",
        "\n",
        "---\n",
        "\n",
        "* `training_set`: dictionary containing faces cropped (original size) part of the training set\n",
        "* `test_set`: dictionary containing faces cropped (original size) part of the test set\n",
        "---\n",
        "\n",
        "At this point, there is not (yet) dedicated validation sets. It is discussed later on.\n",
        "\n",
        "All the training will be done on the training set faces, without any tailoring or dedicated fitting on the test set images. Indeed, metrics on the test set faces indicate how well our model will generalize. It's therefore important to not influence our model with the data of the test set.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ylnkm0G5SCcR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "training_sets_size = {}\n",
        "training_sets_size[personA] = 20\n",
        "training_sets_size[personB] = 20\n",
        "training_sets_size[personC] = 0\n",
        "training_sets_size[personD] = 0\n",
        "\n",
        "test_sets_size = {}\n",
        "test_sets_size[personA] = 10\n",
        "test_sets_size[personB] = 10\n",
        "test_sets_size[personC] = 10\n",
        "test_sets_size[personD] = 10\n",
        "\n",
        "training_set = {}\n",
        "test_set = {}\n",
        "for person in persons:\n",
        "    image_ = faces_cropped[person]\n",
        "    training_set_ = []\n",
        "    random.seed(person)\n",
        "    init_set = set(range(0, len(image_)))\n",
        "\n",
        "    indices_training = random.sample(init_set, training_sets_size[person])\n",
        "    indices_test = list(init_set - set(indices_training))\n",
        "\n",
        "    training_set[person] = [faces_cropped[person][i] for i in indices_training] \n",
        "    test_set[person] = [faces_cropped[person][i] for i in indices_test]\n",
        "\n",
        "logging.info(\"Faces saved in dictionnary training_set: \")\n",
        "logging.info(pretty_return_dict_size(training_set))\n",
        "logging.info(\"Faces saved in dictionnary test_set: \")\n",
        "logging.info(pretty_return_dict_size(test_set))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "pAvlX6WBio8R",
        "colab": {}
      },
      "source": [
        "# show_images_from_dict(training_set)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-pOSPIokbVSV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# show_images_from_dict(test_set)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j73x0LDhZX9k",
        "colab_type": "text"
      },
      "source": [
        "### Faces of the training set\n",
        "- 20 faces of Emma Stone, personA\n",
        "- 20 faces of Bradley Cooper, personB\n",
        "\n",
        "PersonA and PersonB are quite different, A being a female, and B a male. Furthermore, the images within a class are somehow dissimilar as well\n",
        "- different viewpoints (front, left, right)\n",
        "- different lightening conditions\n",
        "- not same hair color\n",
        "- beard/no beard (personB)\n",
        "- not same (limited) background\n",
        "\n",
        "However, a similar characteristic is that both of the actors are most of the time smiling on the faces extracted. Sometimes showing their teeth, sometimes not.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cPlaQzrHZncq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#  Visualisation \n",
        "training_set_matrix = get_matrix_from_set(training_set, color = True, sq_size = sq_size,flatten = True)\n",
        "plot_matrix(training_set_matrix, color = True, my_color_map = plt.cm.viridis, h=4, w=10, transpose = False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T84lBuROhHe3",
        "colab_type": "text"
      },
      "source": [
        "### Faces of the test set\n",
        "Test images are needed for four persons:\n",
        "- 10 faces of Emma Stone, personA\n",
        "- 10 faces of Bradley Cooper, personB\n",
        "- 10 faces of Jane Levy, personC\n",
        "- 10 faces of Marc Blucas, personD\n",
        "\n",
        "(A - C) and (B - D) respectively share some characteristics:\n",
        "* both female / male\n",
        "* same kind of skin tone\n",
        "* visually quite similar (especially for A and C)\n",
        "\n",
        "Within each groups, as for the training set, the faces are taken from different viewpoints, lightening conditions, ...\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kpceMYvAhH_Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " test_set_matrix = get_matrix_from_set(test_set, color = True, sq_size=sq_size, flatten = True)\n",
        "plot_matrix(test_set_matrix, color = True, my_color_map = plt.cm.viridis, h=4, w=10, transpose = False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fq0JMIBgOhih",
        "colab_type": "text"
      },
      "source": [
        "# Feature Representations\n",
        "\n",
        "A feature representation of an object is intuitively a piece of *information*, of a reduced dimension with respect to the object, and that captures the object.\n",
        "It tells what defines the object, and allows differentiating different objects.\n",
        "\n",
        "In the context of an image, a good feature needs to be:\n",
        "- **robust**: the same feature extracted from the same object on an image should be *close*, even if the lightening condition change, the view point change, ...\n",
        "- **discriminative**: different images, representing different object, should lead to different representation in the feature space. As a toy example, the size of an image is not a good feature to detect a person, as several person can be represented in images of the same size.\n",
        "Two types \n",
        "\n",
        "We will look at two features:\n",
        "1. **HOG**: Histogram of oriented gradients. This is a handcrafted feature, extracted using a specific algorithm \n",
        "2. **PCA**: Principal Component Analysis. This is a feature learnt from the data.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yTaT6m7T4lF6",
        "colab_type": "text"
      },
      "source": [
        "## Histogram of Oriented Gradients - HOG\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kFIgA2EaDhSQ",
        "colab_type": "text"
      },
      "source": [
        "As this is a tutorial of Computer Vision, let's look first at what is visually / intuitively the HOG on a real image - one of the Emma Stone (personA) faces. The execution of the following code snippet shows on the left the input face, and on the right, the results.\n",
        "\n",
        "Then, we'll see the details of the algorithm, and its specificities (parameters)\n",
        "\n",
        "This section is extensively inspired by [this course](https://www.learnopencv.com/histogram-of-oriented-gradients/), while the technique has been introduced by [this paper](http://lear.inrialpes.fr/people/triggs/pubs/Dalal-cvpr05.pdf), which I strongly advise to read!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PrC1L60wOadz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "First example - Emma stone first image\n",
        "\"\"\"\n",
        "src = faces_cropped[personA][0]\n",
        "# cv2_imshow(src)\n",
        "\n",
        "#1 resizing\n",
        "resized_img = cv2.resize(src, (64, 64))\n",
        "# resized_img = cv2.cvtColor(resized_img, cv2.COLOR_BGR2GRAY)\n",
        "fd, hog_image = skimage_feature_hog(resized_img, \n",
        "                    orientations=9, \n",
        "                    pixels_per_cell=(8,8), \n",
        "                    cells_per_block=(2, 2), \n",
        "                    block_norm = \"L2\",\n",
        "                    visualize=True, \n",
        "                    transform_sqrt = True,\n",
        "                    multichannel=True)\n",
        "\n",
        "\"\"\"\n",
        "Plotting results\n",
        "\"\"\"\n",
        "\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 8), sharex=True, sharey=True) \n",
        "ax1.imshow(cv2.cvtColor(resized_img, cv2.COLOR_BGR2RGB))\n",
        "ax1.set_title('Input image') \n",
        "\n",
        "# Rescale histogram for better display \n",
        "hog_image_rescaled = exposure.rescale_intensity(hog_image, in_range=(0, 10)) \n",
        "\n",
        "ax2.imshow(hog_image_rescaled, cmap=plt.cm.gray) \n",
        "ax2.set_title('Histogram of Oriented Gradients - rescaled')\n",
        "logging.debug(\"HOG Rescaled: \" + str(hog_image_rescaled.min()) + \" -> \" + str(hog_image_rescaled.max()) )\n",
        "\n",
        "# ax3.imshow(hog_image, cmap=plt.cm.gray) \n",
        "# ax3.set_title('Histogram of Oriented Gradients')\n",
        "# logging.debug(\"HOG: \" + str(hog_image.min()) + \" -> \" + str(hog_image.max()) )\n",
        "plt.show()\n",
        "\n",
        "# logging.debug(hog_image_rescaled.shape)\n",
        "logging.debug(\"fd shape = \" + str(fd.shape))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b_b7lnLnmuDk",
        "colab_type": "text"
      },
      "source": [
        "### HOG - What is that ?\n",
        "\n",
        "HOG is a feature descriptor that extracts information from an image (or more precisely, a patch) based on the gradients in this image. More specifically, it builds a vector representing the weighted distribution of the gradients orientation across the images.\n",
        "\n",
        "#### Why is it interesting ?\n",
        "Let's remember that our goal is to perform image classification and identification. \n",
        "A face can be recognized through the inherent shapes: circular of face, shapes of the eyes, the nose, potentially the glasses, etc. The *edge* information is therefore useful! It is even more useful than the colors... Intuitively, you can think about recognizing someone familiar with only some contours of one face.\n",
        "\n",
        "![Drawing Obama](http://www.drawingskill.com/wp-content/uploads/2/Barack-Obama-Drawing-Pics.jpg)\n",
        "\n",
        "It is easy to recognize the former US President, while no color information is given. Intuitively, HOG gives the same information:\n",
        "- magnitude of gradient is large around the edges and corners\n",
        "- orientation gives the shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "upmrGJDTJfRt",
        "colab_type": "text"
      },
      "source": [
        "#### How to compute the HOG of an image ?\n",
        "\n",
        "In a nutshell:\n",
        "- The gradients are first computed on each pixel. \n",
        "- The gradients orientation and magnitude are used to build an histogram for a cell. The size of a cell is typically 8 x 8 pixels. \n",
        "- Those histogram are normalized \n",
        "- All the histograms computed on the images are then concatenated in a *long* vector, yet much smaller than original image. \n",
        "\n",
        "In the upcoming sections, we will detail the process, as well as the code and parameters required at each step."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kotEMzlaJdvw",
        "colab_type": "text"
      },
      "source": [
        "The following code snippet is a homemade class required to compute the HOG. The results obtained with this code will be compared with the infamous skimage library optimized for the HOG descriptor. \n",
        "\n",
        "You can simply run the snippet and come back later on to see the details of the implementation.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "stIpfAn9FIIz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MyHog():\n",
        "    \n",
        "    def __init__(self, img):\n",
        "        self.img = img # image of the size 64x64; 64x128; 128x128; ... => resized image of the original\n",
        "        self.mag_max, self.orn_max = self.compute_gradients()\n",
        "\n",
        "    def compute_gradients(self):\n",
        "        gx = cv2.Sobel(self.img, cv2.CV_32F, 1, 0, ksize = 1)\n",
        "        gy = cv2.Sobel(self.img, cv2.CV_32F, 0, 1, ksize = 1)\n",
        "\n",
        "        mag, angle = cv2.cartToPolar(gx, gy, angleInDegrees=True)\n",
        "        orn = angle.copy()\n",
        "\n",
        "        logging.debug(\"mag shape :\" + str(mag.shape))\n",
        "        logging.debug(\"orn shape :\" + str(orn.shape))\n",
        "\n",
        "        # constructing matrices of max dimension\n",
        "        mag_max = np.zeros((mag.shape[0], mag.shape[1]))\n",
        "        orn_max = np.zeros((orn.shape[0], orn.shape[1]))\n",
        "        for i in range(mag.shape[0]):\n",
        "            for j in range(mag.shape[1]):\n",
        "                mag_max[i,j] = mag[i,j].max()\n",
        "                idx = np.argmax(mag[i,j])\n",
        "                orn_max[i,j] = orn[i,j,idx] \n",
        "\n",
        "        # mag_max = mag_max.T    \n",
        "        # orn_max = orn_max.T \n",
        "\n",
        "        return mag_max, orn_max\n",
        "\n",
        "    def get_cells_mag_orn(self, y_start, x_start, cell_h, cell_w):\n",
        "        \"\"\"\n",
        "        returns the cell magnitude, orientation and \"clipped\" orientation \n",
        "        ( where 0 -> 360 is mapped into 0 -> 180)\n",
        "        \"\"\"\n",
        "        cell_mag = np.zeros((cell_h,cell_w))\n",
        "        cell_orn = np.zeros((cell_h,cell_w))\n",
        "        for i in range(cell_h):\n",
        "            for j in range(cell_w):\n",
        "                cell_mag[i,j] = self.mag_max[y_start+i, x_start+j]\n",
        "                cell_orn[i,j] = round(self.orn_max[y_start+i,x_start+j])\n",
        "        \n",
        "        cell_orn_clipped = cell_orn.copy() \n",
        "        cell_orn_clipped = ((cell_orn_clipped) + 90 ) % 360\n",
        "        for i in range(cell_h):\n",
        "            for j in range(cell_w):\n",
        "                if 0 <= cell_orn_clipped[i,j] < 180:\n",
        "                    cell_orn_clipped[i,j] = 180 - cell_orn_clipped[i,j]\n",
        "                elif 180 <= cell_orn_clipped[i,j] <=360:\n",
        "                    cell_orn_clipped[i,j] = 180 - cell_orn_clipped[i,j] % 180\n",
        "\n",
        "        return cell_mag.T, cell_orn.T, cell_orn_clipped.T\n",
        "    \n",
        "    def fill_bins_one_pixel(self, mag, orn, bin_list, implementation_type = \"skimage\"):\n",
        "        \"\"\"\n",
        "        # mag: magnitude of the gradient of 1 px\n",
        "        # orn: orientation of the gradient of 1 px\n",
        "        bin_list: reference, list of bins that is incremented\n",
        "        \"\"\"\n",
        "        N_BUCKETS = len(bin_list)\n",
        "        assert N_BUCKETS == 9, \"N_BUCKETS is not 9!!!\"\n",
        "        size_bin = 20.\n",
        "        if implementation_type == \"learnopencv\":\n",
        "            if orn >= 160:\n",
        "                left_bin = 8\n",
        "                right_bin = 9\n",
        "                left_val= mag * (right_bin * 20 - orn) / 20\n",
        "                right_val = mag * (orn - left_bin * 20) / 20\n",
        "                left_bin = 8\n",
        "                right_bin = 0\n",
        "            else:\n",
        "                left_bin = int(orn / size_bin)\n",
        "                right_bin = (int(orn / size_bin) + 1) % N_BUCKETS\n",
        "                left_val= mag * (right_bin * 20 - orn) / 20\n",
        "                right_val = mag * (orn - left_bin * 20) / 20\n",
        "            \n",
        "            assert left_val >= 0, \"leftval = \" + str(left_val) + \", \" + str(\"mag = \") + str(mag) + \" & orn = \" + str(orn)\n",
        "            assert right_val >= 0, \"rightval = \" + str(right_val) + \", \" + str(\"mag = \") + str(mag) + \" & orn = \" + str(orn)\n",
        "\n",
        "            # print(left_val)\n",
        "            # print(right_val)\n",
        "\n",
        "            bin_list[left_bin] += left_val\n",
        "            bin_list[right_bin] += right_val\n",
        "\n",
        "        elif implementation_type == \"skimage\":\n",
        "            # easiest \n",
        "            \"\"\"\n",
        "            this implementation mimics the one from skimage\n",
        "            \"\"\"\n",
        "\n",
        "            if 0 <= orn <= 10:\n",
        "                bin_list[4] += mag\n",
        "            elif 10 < orn <= 30:\n",
        "                bin_list[3] += mag\n",
        "            elif 30 < orn <= 50:\n",
        "                bin_list[2] += mag\n",
        "            elif 50 < orn <= 70:\n",
        "                bin_list[1] += mag\n",
        "            elif 70 < orn <= 90:\n",
        "                bin_list[0] += mag\n",
        "            elif 90 < orn <= 110:\n",
        "                bin_list[8] += mag \n",
        "            elif 110 < orn <= 130:\n",
        "                bin_list[7] += mag\n",
        "            elif 130 < orn <= 150:\n",
        "                bin_list[6] += mag\n",
        "            elif 150 < orn <= 170:\n",
        "                bin_list[5] += mag \n",
        "            elif 170 < orn <= 180:\n",
        "                bin_list[4] += mag \n",
        "            else:\n",
        "                raise RuntimeError(\"Impossible ! > \" + str(orn))\n",
        "        else:\n",
        "            raise NotImplementedError\n",
        "\n",
        "\n",
        "\n",
        "    def compute_hog_bins(self, y_start, x_start, cell_h, cell_w, show_src = True, show_results=True, figsize = (12,4)):\n",
        "        \"\"\"\n",
        "        y_start: y value of the top left pixel\n",
        "        x_start: x value of the top left pixel\n",
        "        cell_h : height of the cell in which HOG is computed\n",
        "        cell_w : width of the cell in which HOG is computed\n",
        "        \"\"\"\n",
        "        cell_img = self.img[y_start:y_start + cell_h, x_start:x_start+cell_w]\n",
        "\n",
        "        if show_src:\n",
        "            tmp = self.img.copy()\n",
        "            cv2.rectangle(tmp, (x_start-1, y_start-1), (x_start+cell_w+1, y_start+cell_h+1), (0,255,0))\n",
        "\n",
        "            fig, ax = plt.subplots(1,1, figsize = (figsize[1],figsize[1]))\n",
        "            ax.imshow(cv2.cvtColor(tmp, cv2.COLOR_BGR2RGB))\n",
        "            ax.set_title(\"Selection of a cell\")\n",
        "\n",
        "            plt.show()\n",
        "\n",
        "        # construction of the magnitude and orn matrices\n",
        "        cell_mag, cell_orn, cell_orn_clipped = self.get_cells_mag_orn(y_start, x_start, cell_h, cell_w)\n",
        "\n",
        "        number_of_bins = 9\n",
        "        bin_list = np.zeros(number_of_bins)\n",
        "        for i in range(cell_h):\n",
        "            for j in range(cell_w):\n",
        "                # m = round(mag_normalized[y_start+j,x_start+i].max())\n",
        "                m = cell_mag[i,j]\n",
        "                d = cell_orn_clipped[i,j]\n",
        "                # print(\"m,d =\" + str((m,d)))\n",
        "                self.fill_bins_one_pixel(m,d,bin_list)\n",
        "        \n",
        "        # logging.debug(\"Bins computed:\" + str(bin_list))\n",
        "        n = np.linalg.norm(bin_list)\n",
        "        bin_norms = bin_list/n\n",
        "        # logging.debug(\"Bins normalized:\" + str(bin_norms))\n",
        "             \n",
        "        if show_results:\n",
        "            fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=figsize, sharex=True, sharey=True) \n",
        "\n",
        "            # ax1 => just to visually represent the arrows\n",
        "            for i in range(cell_h):\n",
        "                for j in range(cell_w):\n",
        "\n",
        "                    radius = cell_mag[i,j] / (cell_mag.max() - cell_mag.min())\n",
        "                    angle_ = cell_orn[i,j]\n",
        "                    \n",
        "                    orn_value_clipped = cell_orn_clipped[i,j]\n",
        "\n",
        "                    mag_value = round(cell_mag[i,j])\n",
        "                    \n",
        "                    ax1.arrow(i, j, radius*np.cos(np.deg2rad(angle_)), radius*np.sin(np.deg2rad(angle_)), head_width=0.15, head_length=0.15, fc='b', ec='b')\n",
        "                    ax2.text(i, j, str(orn_value_clipped.astype(np.int64)), fontsize=10,va='center', ha='center')\n",
        "                    ax3.text(i, j, str(mag_value.astype(np.int64)), fontsize=10,va='center', ha='center')\n",
        "\n",
        "            ax1.imshow(cv2.cvtColor(cell_img, cv2.COLOR_BGR2RGB))\n",
        "            ax1.set_title('Input image') \n",
        "\n",
        "            ax2.matshow(cell_orn_clipped, alpha=0)\n",
        "            ax2.set_title('Orientation values')\n",
        "\n",
        "            ax3.set_title('Magnitude values')\n",
        "            intersection_matrix = np.ones(cell_mag.shape)\n",
        "            ax3.matshow(cell_mag, alpha = 0)\n",
        "           \n",
        "            plt.show()\n",
        "        return bin_list, bin_norms"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oMB6GhIcLi2C",
        "colab_type": "text"
      },
      "source": [
        "###HOG How-to, Step1: Preprocessing the image\n",
        "\n",
        "Usually, the size of an image is not appropriate to perform the HOG computation. The easiest thing is just to resize the image to an appropriate size. In this tutorial, we use a multiple of 8 and a square image. Considering the smallest face cropped, we select 64 x 64 pixels.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o42F21fRN7Fe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "logging.info(\"Toy Example\")\n",
        "\n",
        "img = faces_cropped[personA][0].copy()\n",
        "resized_img = cv2.resize(img, (64,64))\n",
        "logging.info(\"Shape of source  face: \" + str(img.shape))\n",
        "logging.info(\"Shape of resized face: \" + str(resized_img.shape))\n",
        "\n",
        "\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(8, 4), sharex=False, sharey=False) \n",
        "\n",
        "ax1.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
        "ax1.set_title('Input image') \n",
        "\n",
        "ax2.imshow(cv2.cvtColor(resized_img, cv2.COLOR_BGR2RGB))\n",
        "ax2.set_title('Resized image') \n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ufx19W7JePys",
        "colab_type": "text"
      },
      "source": [
        "###HOG How-to, Step2: Computing the gradient for all pixels\n",
        "\n",
        "Computing the gradient in horizontal ($x$) and vertical ($y$) directions can be done using a pass of the Sobel Filter, part of the *openCV* library. \n",
        "This is implemented in the `MyHog.compute_gradients()` function (see implementation of `MyHog` class, above. From these gradients in $x$ and $y$ we can derive the magnitudes and orientations in all pixels using the formulas:\n",
        "> $\n",
        "\\begin{align} \n",
        "mag &= \\sqrt{g_x^2 + g_y^2} \\\\ \n",
        "orn &= atan(\\frac{g_y}{g_x}) \n",
        "\\end{align}\n",
        "$\n",
        "\n",
        "This is implemented using *openCV* library with `cartToPolar`.\n",
        "\n",
        "#####*Grayscale or Colored image*\n",
        "> For grayscale image, every pixel has one value so that this computation is straightforward. For colored image, a pixel has three values (one for Red, one for Green, one for Blue). In the HOG algorithm, the gradients is computed for all three channels, and the final magnitude is the maximum of the three, and the orientation is the one corresponding to the magnitude.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hQiWXisWJItc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "Create an object MyHog, which takes a resized image as input, and compute its \n",
        "gradients.\n",
        "\"\"\"\n",
        "myhog = MyHog(resized_img)\n",
        "\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(8, 4), sharex=True, sharey=True) \n",
        "\n",
        "ax1.imshow(myhog.mag_max, cmap = plt.cm.jet)\n",
        "ax1.set_title('Max of magnitude') \n",
        "\n",
        "ax2.imshow(myhog.orn_max, cmap = plt.cm.jet)\n",
        "ax2.set_title('Max of Orientation') \n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IQxPKrGhizSK",
        "colab_type": "text"
      },
      "source": [
        "As visible on the magnitude and orientation plots above, only essential information regarding the edges is kept. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lSvgxKzkiiWi",
        "colab_type": "text"
      },
      "source": [
        "###HOG How-to, Step3: Compute histograms for cells\n",
        "\n",
        "The histograms are first computed for small cells. It has two major benefits\n",
        "1. the representation is more compact: Suppose we take cells of $8 \\times 8$ pixels. The gradient of each pixel is described using 2 numbers (magnitude and orientation), leading to 128 numbers. Considering an histogram applied on such a cell allows to represent those 128 numbers by a tiny array of typically 9 numbers. In total a colored image of $64 \\times 64$ pixels is represented using $9*8*8$ vector.\n",
        "2. the representation is less sensitive to noise, as applying a cell is equivalent to a low-pass filter. Higher frequency outliers are therefore of less importance.\n",
        "\n",
        "\n",
        "The choice of the cell size is a design choice that can be modified. In a later section, we will modify this parameter to see how it can affect the results. \n",
        "In the paper that first presented the technique, a cell of $8 \\times 8$ pixels was used - we will continue with this(hyper-)parameter. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OziV9UK_rMGp",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "> *You may try yourself to modify the cell size or the x_start or y_start values, to see the influence on the histogram computed for that particular cell*\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bkapi9krhA_l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "Size of a cell\n",
        "\"\"\"\n",
        "cell_h = 8\n",
        "cell_w = 8\n",
        "\n",
        "\"\"\"\n",
        "starting point of the cell on the image\n",
        "\"\"\"\n",
        "y_start = 3 * cell_h - 1\n",
        "x_start = 2 * cell_w - 1\n",
        "\n",
        "hog_val, hog_val_normalized = myhog.compute_hog_bins(y_start, x_start, cell_h, cell_w, show_results=True, figsize=(16,5))\n",
        "\n",
        "fig, ax = plt.subplots(1,1, figsize = (2*5, 5))\n",
        "ax.bar([\"]70-90]\",\"]50-70]\", \"]30-50]\", \"]10-30]\",\"]10-\\n180]\",\"]150-\\n170]\",\"]130-\\n150]\",\"]110-\\n130]\", \"]90-\\n110]\"], hog_val_normalized)\n",
        "ax.set_title(\"Histogram computed with MyHog (homemade)\")\n",
        "plt.show()\n",
        "logging.info(\"MyHog bins   normalized  : {}, {}, {}, {}, {}, {}, {}, {}, {}\".format(*np.round(hog_val_normalized,2)))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2BFakzS91ki5",
        "colab_type": "text"
      },
      "source": [
        "**Legend of the above images**\n",
        "1. source image with *cell* visible in flashy green,\n",
        "2. details of the gradients computations\n",
        "    - *leftmost*: cell enlarged with an arrow indicating the gradient: length of the arrow represent the magnitude, and orientation is the gradient orientation computed on that pixel\n",
        "    - *middle*: matrix (shape == cell) containing the orientations computed (unsigned)\n",
        "    - *rightmost*: matrix (shape == cell) containung the magnitude computed\n",
        "3. histogram computed (`keyword = \"skimage\"`)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NbKkGHW9ru-I",
        "colab_type": "text"
      },
      "source": [
        "####In details\n",
        "\n",
        "The details of building the histogram for a cell is not complicated:\n",
        "- consider N bins. N is a design parameter, and each of the bins represent a range of gradient orientations. I have chosen N = 9, following the [introducing paper](http://lear.inrialpes.fr/people/triggs/pubs/Dalal-cvpr05.pdf), as it induces a granularity fine enough to observe change in the picture.\n",
        "> The HOG is usually applied using **unsigned gradients**. The numbers on the orientation matrix are between 0 and 180 instead of 0 and 360 degrees. Concretely, an angle $\\alpha [deg] $ and $(180 + \\alpha) [deg]$ contribute to the same bin. Empirically, it has been observed that it wasn't decreasing performance in the detection. Of course, nothing forbids to use signed gradients. \n",
        "- for a particular pixel:\n",
        "    - the bin is selected according to the orientation of the gradient; \n",
        "    - the value that goes in the bin is based on the magnitude. Different methods were proposed by the [introducing paper](http://lear.inrialpes.fr/people/triggs/pubs/Dalal-cvpr05.pdf), and are also explained in details for instance in [vidha blog](https://www.analyticsvidhya.com/blog/2019/09/feature-engineering-images-introduction-hog-feature-descriptor/). The class `MyHog` above contains two implementation: either the magnitude is split proportionnaly between two bins (as described in [learnopencv](https://www.learnopencv.com/histogram-of-oriented-gradients/), or -- as done in *openCV* library --, the whole magnitude is assigned to the closest bin. \n",
        "\n",
        "While this step is not difficult, let's realize in image how it's done!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jtmUfkv3XqlQ",
        "colab_type": "text"
      },
      "source": [
        "####Creation of dedicated images\n",
        "\n",
        "The following function allows creating images \"on demand\", in order to better understand the histogram computation. The parameter \"special\" indicate what type of image is required."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4RE2qpxGt9us",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "CREATE_IMAGE returns an image created based on a keyword. \n",
        "\"\"\"\n",
        "def create_image(height, width, special=None):\n",
        "\n",
        "    mat0 = np.ones((height, width), dtype=np.uint8)*255\n",
        "    mat1 = np.ones((height, width), dtype=np.uint8)*255\n",
        "    mat2 = np.ones((height, width), dtype=np.uint8)*255\n",
        "\n",
        "    if special == \"center_black\":\n",
        "        mat0[height//2-1:height//2+1, width//2-1 : width//2+1 ] = 0\n",
        "        mat1[height//2-1:height//2+1, width//2-1 : width//2+1 ] = 0\n",
        "        mat2[height//2-1:height//2+1, width//2-1 : width//2+1 ] = 0\n",
        "    elif special == \"center_gray\":\n",
        "        mat0[height//2-3:height//2+3, width//2-3 : width//2+3 ] = 125\n",
        "        mat1[height//2-3:height//2+3, width//2-3 : width//2+3 ] = 125\n",
        "        mat2[height//2-3:height//2+3, width//2-3 : width//2+3 ] = 125\n",
        "        mat0[height//2-1:height//2+1, width//2-1 : width//2+1 ] = 0\n",
        "        mat1[height//2-1:height//2+1, width//2-1 : width//2+1 ] = 0\n",
        "        mat2[height//2-1:height//2+1, width//2-1 : width//2+1 ] = 0\n",
        "    elif special == \"90\":\n",
        "        mat0[0:height, width//2 : width ] = 0\n",
        "        mat1[0:height, width//2 : width ] = 0\n",
        "        mat2[0:height, width//2 : width ] = 0\n",
        "    elif special == \"180\":\n",
        "        mat0[height//2:height, 0 : width ] = 0\n",
        "        mat1[height//2:height, 0 : width ] = 0\n",
        "        mat2[height//2:height, 0 : width ] = 0\n",
        "    elif special == \"135\":\n",
        "        for i in range(height):\n",
        "            for j in range(i,width):\n",
        "                mat0[i,j] = mat1[i,j] = mat2[i,j] = 0\n",
        "    elif special == \"45\":\n",
        "        for i in range(height):\n",
        "            for j in range(width-i-1,width):\n",
        "                mat0[i,j] = mat1[i,j] = mat2[i,j] = 0\n",
        "    elif special == \"28_34_37\":\n",
        "        mat0[4, 6:8] = 200\n",
        "        mat0[5, 4:8] = 150\n",
        "        mat0[6, 2:8] = 100\n",
        "        mat0[7, 0:8] = 50\n",
        "        mat1 = mat0.copy()\n",
        "        mat2 = mat0.copy()\n",
        "    elif special == \"up_01\":\n",
        "        mat0[4, 4:8] = 250\n",
        "        mat0[5, 0:8] = 50\n",
        "        mat0[6, 0:8] = 50\n",
        "        mat0[7, 0:8] = 50\n",
        "        mat1 = mat0.copy()\n",
        "        mat2 = mat0.copy()\n",
        "    elif special == \"up_10\":\n",
        "        mat0[4, 4:8] = 220\n",
        "        mat0[5, 0:8] = 50\n",
        "        mat0[6, 0:8] = 50\n",
        "        mat0[7, 0:8] = 50\n",
        "        mat1 = mat0.copy()\n",
        "        mat2 = mat0.copy()\n",
        "    elif special == \"up_11\":\n",
        "        mat0[4, 4:8] = 225\n",
        "        mat0[5, 0:8] = 50\n",
        "        mat0[6, 0:8] = 50\n",
        "        mat0[7, 0:8] = 50\n",
        "        mat1 = mat0.copy()\n",
        "        mat2 = mat0.copy()\n",
        "    elif special == \"up_15\":\n",
        "        mat0[4, 4:8] = 200\n",
        "        mat0[5, 0:8] = 50\n",
        "        mat0[6, 0:8] = 50\n",
        "        mat0[7, 0:8] = 50\n",
        "        mat1 = mat0.copy()\n",
        "        mat2 = mat0.copy()\n",
        "    elif special == \"up_27\":\n",
        "        mat0[4, 4:8] = 150\n",
        "        mat0[5, 0:8] = 50\n",
        "        mat0[6, 0:8] = 50\n",
        "        mat0[7, 0:8] = 50\n",
        "        mat1 = mat0.copy()\n",
        "        mat2 = mat0.copy()\n",
        "    elif special == \"up_152\":\n",
        "        mat0 = np.ones((height, width), dtype=np.uint8)*0\n",
        "        mat1 = np.ones((height, width), dtype=np.uint8)*0\n",
        "        mat2 = np.ones((height, width), dtype=np.uint8)*0\n",
        "        mat0[0,6:7] = mat1[0,6:7] = mat2[0,6:7] = 255\n",
        "        mat0[1,7] = mat1[1,7] = mat2[1,7] = 135\n",
        "    elif special == \"down_153\":\n",
        "        mat0 = np.ones((height, width), dtype=np.uint8)*255\n",
        "        mat1 = np.ones((height, width), dtype=np.uint8)*255\n",
        "        mat2 = np.ones((height, width), dtype=np.uint8)*255\n",
        "        mat0[0,6:7] = mat1[0,6:7] = mat2[0,6:7] = 0\n",
        "        mat0[1,7] = mat1[1,7] = mat2[1,7] = 125\n",
        "    elif special == \"up_141\":\n",
        "        mat0 = np.ones((height, width), dtype=np.uint8)*0\n",
        "        mat1 = np.ones((height, width), dtype=np.uint8)*0\n",
        "        mat2 = np.ones((height, width), dtype=np.uint8)*0\n",
        "        mat0[0,6:7] = mat1[0,6:7] = mat2[0,6:7] = 255\n",
        "        mat0[1,7] = mat1[1,7] = mat2[1,7] = 210\n",
        "    elif special == \"up_111\":\n",
        "        mat0 = np.ones((height, width), dtype=np.uint8)*0\n",
        "        mat1 = np.ones((height, width), dtype=np.uint8)*0\n",
        "        mat2 = np.ones((height, width), dtype=np.uint8)*0\n",
        "        mat0[0,0:7] = mat1[0,0:7] = mat2[0,0:7] = 100\n",
        "        mat0[1,7] = mat1[1,7] = mat2[1,7] = 255\n",
        "\n",
        "    image = np.dstack((mat0, mat1, mat2))\n",
        "    return image"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J-N7rgcjwvB7",
        "colab_type": "text"
      },
      "source": [
        "####Examples of histogram computed on created images\n",
        "In order to understand how the bins are fulled, let's look at a few of simple images. Those images are $8 \\times 8$, meaning 1 cell == 1 image\n",
        "\n",
        "* pure 90° gradient\n",
        "* pure 180° gradient\n",
        "* diagonal: 45°\n",
        "* diagonal: 135°\n",
        "\n",
        "For each case, we plot:\n",
        "- 1) the arrows representing the gradients, \n",
        "- 2) the matrices of the magnitude and orientation values, \n",
        "- 3) the resulting histograms\n",
        "\n",
        "**and** we log:\n",
        "\n",
        "- the raw values of the histogram bins\n",
        "- the normalized values of the histogram bins, using *L2-Normalization*:\n",
        "$\\begin{align}\n",
        "bins\\_values &= [x_1, x_2, x_3, x_4, x_5, x_6, x_7, x_8, x_9] \\\\\n",
        "\\|bins\\_values\\| &= \\sqrt{x_1^2 + x_2^2 + x_3^2 + x_4^2 + x_5^2 + x_6^2 + x_7^2 + x_8^2 + x_9^2 } \\\\\n",
        "bins\\_values_{normalized} &=  \\frac{v}{\\|v\\|} \\\\ \n",
        "&= \\Bigg[ \\frac{x_1}{\\|v\\|}, \\frac{x_2}{\\|v\\|}, \\frac{x_3}{\\|v\\|},  \\frac{x_4}{\\|v\\|}, \\frac{x_5}{\\|v\\|}, \\frac{x_6}{\\|v\\|},  \\frac{x_7}{\\|v\\|}, \\frac{x_8}{\\|v\\|}, \\frac{x_9}{\\|v\\|} \\Bigg]\n",
        "\\end{align}$\n",
        "\n",
        "\n",
        "\n",
        "#### Validation of intuition\n",
        "To prove ourselves our implementation and understanding is correct, we will confront the results with the infamous library `skimage`, using `skimage.feature.hog` with the same parameters as the handmade function: 9 bins, an $8\\times 8$ cell, and 1 cell per *block* (we discuss the *blocks* in the next section). Two parameters are still unknown: transform_sqrt and multichannel \n",
        "- `transform_sqrt`: if True, then the sqrt operator is applied to the global image first. It can give better results. We can safely leave it to False for the purpose of this exercices with the HOG bins...\n",
        "- `multichannel`: simply indicates if the image is grayscale (`multichannel = False`) or in color (`multichannel = True`) \n",
        "\n",
        "<!--Note: we briefly discussed the `block_norm` parameter, but more to come in the next step.-->\n",
        "\n",
        "####Finally...\n",
        "Coming back to the very first example of the HOG, we saw the Emma Stone picture with weird white-ish pictograms describing her face... Well, thanks to the `skimage` library, it's very easy to get this image, and we show it for the toy example we are studying now. It allows grabbing the full overview of how, eventually, the complete histogram and visualization is computed.\n",
        "\n",
        "> *Of course, don't hesitate to change yourself the list of images that are analyzed, considering the list implemented. You find the keywords accepted in the special list*\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zzqs9qmavSyr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "Creation of the images\n",
        "\"\"\"\n",
        "special = [\"up_01\",\"45\",\"90\", \"135\",\"180\",\"up_10\",\"up_11\",\"up_15\", \"up_27\", \"28_34_37\", \"up_111\", \"up_141\", \"up_152\", \"down_153\",\"center_black\", \"center_gray\"]\n",
        "list_as_example = [\"90\", \"180\", \"45\", \"135\", \"28_34_37\"]\n",
        "\n",
        "# created_img = create_image(cell_h,cell_w, \"45\")\n",
        "\n",
        "\"\"\"\n",
        "Homemade implementation of the histogram\n",
        "and\n",
        "Validation with an optimized library\n",
        "\"\"\"\n",
        "for keyword in list_as_example:\n",
        "    logging.info(\"Considering image: \" + str(keyword))\n",
        "\n",
        "    # creation of the simple image\n",
        "    created_img = create_image(cell_h, cell_w, keyword)\n",
        "\n",
        "    # creation of MyHog object\n",
        "    toyhog=MyHog(created_img)\n",
        "\n",
        "    # compute bins using MyHog\n",
        "    y_start_loc = 0\n",
        "    x_start_loc = 0\n",
        "    bins, bins_normalized = toyhog.compute_hog_bins(y_start_loc, x_start_loc, cell_h, cell_w, show_src=False, show_results=True, figsize = (14,4))\n",
        "\n",
        "    # compute bins using Skimage \n",
        "    fd, hog_image = skimage_feature_hog(created_img, \n",
        "                orientations=9, \n",
        "                pixels_per_cell=(8,8), \n",
        "                cells_per_block=(1, 1), \n",
        "                block_norm = \"L2\",\n",
        "                visualize=True, \n",
        "                transform_sqrt = False,\n",
        "                multichannel=True)\n",
        "\n",
        "    # plot results from Skimage\n",
        "    plt.figure()\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 4), sharex=False, sharey=False) \n",
        "\n",
        "    # Rescale hog for better display \n",
        "    hog_image_rescaled = exposure.rescale_intensity(hog_image, in_range=(0, 10)) \n",
        "\n",
        "\n",
        "    xlabels = [\"]70-90]\",\"]50-70]\", \"]30-50]\", \"]10-30]\",\"]10-\\n180]\",\"]150-\\n170]\",\"]130-\\n150]\",\"]110-\\n130]\", \"]90-\\n110]\"]\n",
        "    x = np.arange(9)\n",
        "    w = 0.2\n",
        "    ax1.bar( x-w, bins_normalized,  width=2*w, align='center',color=\"b\")\n",
        "    ax1.bar( x+w, fd, width=2*w, align='center',color=\"r\")\n",
        "   \n",
        "    ax1.set_title(\"Histogram computed by Skimage.feature.hog\")\n",
        "    ax1.legend([\"MyHog (homemade)\", \"Skimage.feature.hog\"])\n",
        "    # start, end = ax.get_xlim()\n",
        "    # ax.xaxis.set_ticks(np.arange(start, end, 1))\n",
        "    # ax1.set_xticklabels(xlabels)\n",
        "    ax1.set_xticks(x)\n",
        "    ax1.set_xticklabels(xlabels)\n",
        "\n",
        "    ax2.imshow(hog_image_rescaled, cmap=plt.cm.gray) \n",
        "    ax2.set_title('Histogram of Oriented Gradients - visual')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "\n",
        "    logging.info(\"MyHog bins     computed  : {}, {}, {}, {}, {}, {}, {}, {}, {}\".format(*np.round(bins,2)))\n",
        "    logging.info(\"MyHog bins   normalized  : {}, {}, {}, {}, {}, {}, {}, {}, {}\".format(*np.round(bins_normalized,2)))\n",
        "    logging.info(\"Skimage bins normalized  : {}, {}, {}, {}, {}, {}, {}, {}, {}\".format(*np.round(fd,2)))\n",
        "    logging.info(\"***\"*30)\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "co65Rf5VLdZ8",
        "colab_type": "text"
      },
      "source": [
        "> Notice: as a reminder, the purpose of the `MyHog` class (or any of the other class from this tutorial) is definitely not to reproduce exactly the behavior of a well-known and optimized library, but solely to break the magic behind using a library function without understanding the algorithm behind. As a result, the HOG computed may differ in several ways."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B5u575_gKGKy",
        "colab_type": "text"
      },
      "source": [
        "####Coming back to our initial cell...\n",
        "Emma Stone cell defined above can now be shown in terms of HOG, helped by the `skimage` library.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HH2aQX3NRd4P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "Retrieving the cell defined above\n",
        "\"\"\"\n",
        "cell_img=resized_img[y_start:y_start + cell_h, x_start:x_start+cell_w]\n",
        "\n",
        "\"\"\"\n",
        "computing HOG of the cell using same parameters\n",
        "\"\"\"\n",
        "fd, hog_image = skimage_feature_hog(cell_img, \n",
        "                    orientations=9, \n",
        "                    pixels_per_cell=(8,8), \n",
        "                    cells_per_block=(1, 1), \n",
        "                    block_norm = \"L2\",\n",
        "                    visualize=True, \n",
        "                    transform_sqrt = False,\n",
        "                    multichannel=True)\n",
        "\n",
        "\n",
        "\n",
        "plt.figure()\n",
        "\n",
        "fig, (ax0, ax1, ax2) = plt.subplots(1, 3, figsize=(12, 4), sharex=False, sharey=False) \n",
        "# hog_cropped = hog_image[y_start:y_start + cell_h, x_start:x_start+cell_w]\n",
        "ax0.imshow(cv2.cvtColor(resized_img, cv2.COLOR_BGR2RGB))\n",
        "ax0.set_title('Input image') \n",
        "\n",
        "ax1.imshow(cv2.cvtColor(cell_img, cv2.COLOR_BGR2RGB))\n",
        "ax1.set_title('Extracted cell') \n",
        "\n",
        "# Rescale histogram for better display \n",
        "hog_image_rescaled = exposure.rescale_intensity(hog_image, in_range=(0, 10)) \n",
        "ax2.imshow(hog_image_rescaled, cmap=plt.cm.gray) \n",
        "ax2.set_title('Histogram of Oriented Gradients - rescaled')\n",
        "# print(hog_image_rescaled)\n",
        "plt.show()\n",
        "\n",
        "logging.info(\"Skimage bins normalized  : {}, {}, {}, {}, {}, {}, {}, {}, {}\".format(*np.round(fd,2)))\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jiAjftDx2Nxm",
        "colab_type": "text"
      },
      "source": [
        "This is the end of the Step3: the computation of the histogram for one cell! A careful eye will have seen the parameters `cells-per-block` and `block-norm` of the library method. This is linked to Step4: Block Normalization!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OXLInz6551Ak",
        "colab_type": "text"
      },
      "source": [
        "###HOG How-to, Step4: Block normalization\n",
        "\n",
        "In the Step3, we have extensively seen how to compute the histogram of gradients for a cell. We are almost at the end of the feature representation build up, but there are yet one normalization step. \n",
        "> In the previous step, we actually already normalized the histogram values using *L2-Normalization*. This is a simple case of the Block normalization where there is 1 cell per block. In general, we can define to perform Block normalization on more than 1 block. A common value is 4, as discussed in the [introducing paper](http://lear.inrialpes.fr/people/triggs/pubs/Dalal-cvpr05.pdf). \n",
        "\n",
        "####<u>Why do we need normalization ?</u>\n",
        " \n",
        "When normalized, a histogram becomes independant from the lighting variation. \n",
        "Indeed, illumination has the impact of increasing/decreasing the values of the pixels in a cell. \n",
        "\n",
        "Using normalization, a change on the pixel value has no impact if all the pixels in a cell are subject to the same change. \n",
        "> let's say a low illumination make the pixel values divided by two. Having a normalized histogram on the cell will not be affected by such a change, as in the end, the absolute value is not important: only the relative value of one pixel to others matter. This is the very essence of the normalization.\n",
        "\n",
        "As a result, normalizing the histogram makes it quite independant of the lighting condition (provided that in a cell, all the pixels have the same lighting condition, which seems a sensible assumption).\n",
        "\n",
        "####<u>Normalizing by block</u> \n",
        "A nice visualization of the normalization by block of multiple cells is given in [learnopencv](https://www.learnopencv.com/wp-content/uploads/2016/12/hog-16x16-block-normalization.gif) where we see in green the different cells, and in blue a block of 4 cells. \n",
        "Using a block normalization - so, normalizing multiple cells at ones, and slide the normalization window across the image - is introduced in the [introducing paper](http://lear.inrialpes.fr/people/triggs/pubs/Dalal-cvpr05.pdf) which shows some benefits in terms of missing rate. Typically, 4 cells per blocks is often used. In a later section (see Classification), a grid search tends to try out other block sizes.\n",
        "\n",
        "####<u>What normalization ?</u>\n",
        "Several normalization can be considered: *L1*, *L2*, *L2-Hys*, ...\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jc4AH7fJp63w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "Definition of the block size (in number of px)\n",
        "1 cell = 8 x 8\n",
        "1 block => 16 x 16 => 4 cells\n",
        "\"\"\"\n",
        "block_w = 16\n",
        "block_h = 16\n",
        "\n",
        "x_cells = np.arange(0,64,8)\n",
        "y_cells = np.arange(0,64,8)\n",
        "\n",
        "# credit: https://stackoverflow.com/questions/44816682/drawing-grid-lines-across-the-image-uisng-opencv-python\n",
        "def draw_grid(img, line_color=(0, 255, 0), thickness=1, type_=8, pxstep=8):\n",
        "    '''(ndarray, 3-tuple, int, int) -> void\n",
        "    draw gridlines on img\n",
        "    line_color:\n",
        "        BGR representation of colour\n",
        "    thickness:\n",
        "        line thickness\n",
        "    type:\n",
        "        8, 4 or cv2.LINE_AA\n",
        "    pxstep:\n",
        "        grid line frequency in pixels\n",
        "    '''\n",
        "    x = pxstep\n",
        "    y = pxstep\n",
        "    while x < img.shape[1]:\n",
        "        cv2.line(img, (x, 0), (x, img.shape[0]), color=line_color, lineType=type_, thickness=thickness)\n",
        "        x += pxstep\n",
        "\n",
        "    while y < img.shape[0]:\n",
        "        cv2.line(img, (0, y), (img.shape[1], y), color=line_color, lineType=type_, thickness=thickness)\n",
        "        y += pxstep\n",
        "\n",
        "def draw_one_block(img, origin=(0,0), line_color=(255,0, 0), block_size = 16, thickness=1, type_=8):\n",
        "    cv2.rectangle(img, origin, (origin[0]+block_size, origin[1]+block_size), line_color, thickness=thickness, lineType =type_)\n",
        "\n",
        "def draw_all_blocks(img, thickness):\n",
        "    color_list = [(255,0,0), (255,255,0), (255,0,255)]\n",
        "    x = 0\n",
        "    y = 0\n",
        "    counter = 0\n",
        "    while x < img.shape[1]-8:\n",
        "        # cv2.line(img, (x, 0), (x, img.shape[0]), color=line_color, lineType=type_, thickness=thickness)\n",
        "        # draw_one_block(img, (x,y))\n",
        "        \n",
        "        \n",
        "        while y < img.shape[0]-8:\n",
        "            # cv2.line(img, (0, y), (img.shape[1], y), color=line_color, lineType=type_, thickness=thickness)\n",
        "            lc = color_list[counter%3]\n",
        "            draw_one_block(img, (x,y),line_color=lc, thickness=thickness)\n",
        "            counter += 1\n",
        "            y += 8\n",
        "        y=0\n",
        "        x += 8\n",
        "    return counter\n",
        "\n",
        "\"\"\"\n",
        "creating a green grid covering the resized image\n",
        "\"\"\"\n",
        "\n",
        "grid_cells_img = resized_img.copy()\n",
        "draw_grid(grid_cells_img, type_=8)\n",
        "\n",
        "\"\"\"\n",
        "Creating the three first blocks\n",
        "\"\"\"\n",
        "\n",
        "first_block_img = grid_cells_img.copy()\n",
        "second_block_img = grid_cells_img.copy()\n",
        "third_block_img = grid_cells_img.copy()\n",
        "\n",
        "draw_one_block(first_block_img, origin=(0,0), line_color=(255,0,0), thickness=2)\n",
        "draw_one_block(second_block_img, origin=(8,0), line_color=(255,255,0),thickness=2)\n",
        "draw_one_block(third_block_img, origin=(16,0), line_color=(255,0,255),thickness=2)\n",
        "\n",
        "\"\"\"\n",
        "Creating all the blocks on top of the cells\n",
        "\"\"\"\n",
        "\n",
        "blocks_img = grid_cells_img.copy()\n",
        "counter = draw_all_blocks(blocks_img, thickness=1)\n",
        "\n",
        "\"\"\"\n",
        "Vizualization\n",
        "\"\"\"\n",
        "\n",
        "fig, (ax0, ax1, ax2, ax3) = plt.subplots(1, 4, figsize=(16, 4), sharex=False, sharey=False) \n",
        "ax0.imshow(cv2.cvtColor(grid_cells_img, cv2.COLOR_BGR2RGB))\n",
        "ax0.set_title('Cells') \n",
        "\n",
        "ax1.imshow(cv2.cvtColor(first_block_img, cv2.COLOR_BGR2RGB))\n",
        "ax1.set_title('first block') \n",
        "\n",
        "ax2.imshow(cv2.cvtColor(second_block_img, cv2.COLOR_BGR2RGB))\n",
        "ax2.set_title('second block') \n",
        "\n",
        "ax3.imshow(cv2.cvtColor(third_block_img, cv2.COLOR_BGR2RGB))\n",
        "ax3.set_title('third block') \n",
        "\n",
        "fig, ax = plt.subplots(1,1,figsize=(4,4))\n",
        "ax.imshow(cv2.cvtColor(blocks_img, cv2.COLOR_BGR2RGB))\n",
        "ax.set_title(\"All Blocks\")\n",
        "\n",
        "plt.show()\n",
        "\n",
        "logging.info(\"In total, there are: \" + str(counter) + \" blocks possible in the picture\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1LybYTHG0fxi",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "As shown in the previous example, on the image chosen, there are $49$ blocks possible of size $(16 \\times 16)$ pixels.\n",
        "\n",
        "##HOG How-to, Step5: concatenation\n",
        "\n",
        "After the normalization, the only step remaining is the concatenation of the computed vectors into a larger one, that represent the input image. This will be the feature representation of the image, based on the *oriented* gradients in that image.\n",
        "\n",
        "###What size is this feature representation ?\n",
        "- one cell is represented by $9$ numbers (histogram)\n",
        "- four histograms are normalized together, leading to a $(36,1)$ vector\n",
        "- there are $49$ such vectors representing the image.\n",
        "    If the image as a width of size $(w*8)$ pixels, and a height of $(h*8)$ pixels, the image dimension is $(8*h \\times  8*w)$. In such an image, they are :\n",
        "    *   h cells vertically, and w cells horizontally,\n",
        "    *  (h-1) blocks vertically and (w-1) blocks horizontally.\n",
        "\n",
        "The length of the final vector is then $36 \\cdot 49$ numbers, or a $(1764,1)$ vector.\n",
        "\n",
        "Of course, this is still large... But much more compact that our initial $(64,64,3)$ array of $12288$ numbers\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eXffH7oz8GEJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fd, hog_image = skimage_feature_hog(resized_img, \n",
        "                    orientations=9, \n",
        "                    pixels_per_cell=(8,8), \n",
        "                    cells_per_block=(2, 2), \n",
        "                    block_norm = \"L2\",\n",
        "                    visualize=True, \n",
        "                    transform_sqrt = True,\n",
        "                    multichannel=True)\n",
        "\n",
        "\"\"\"\n",
        "Plotting results\n",
        "\"\"\"\n",
        "\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 8), sharex=True, sharey=True) \n",
        "ax1.imshow(cv2.cvtColor(resized_img, cv2.COLOR_BGR2RGB))\n",
        "ax1.set_title('Input image') \n",
        "\n",
        "# Rescale histogram for better display \n",
        "hog_image_rescaled = exposure.rescale_intensity(hog_image, in_range=(0, 10)) \n",
        "ax2.imshow(hog_image_rescaled, cmap=plt.cm.gray) \n",
        "ax2.set_title('Histogram of Oriented Gradients - rescaled')\n",
        "logging.debug(\"HOG Rescaled: \" + str(hog_image_rescaled.min()) + \" -> \" + str(hog_image_rescaled.max()) )\n",
        "\n",
        "plt.show()\n",
        "\n",
        "# logging.debug(hog_image_rescaled.shape)\n",
        "logging.info(\"Shape of the HOG feature : \" + str(fd.shape))\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GuWXw_8xuTWb",
        "colab_type": "text"
      },
      "source": [
        "<!--histogram construction is based on the gradient computed - both magnitude and orientation (as defined)\n",
        "\n",
        "- the bin is selected according to the orientation (direction) of the gradient; \n",
        "- the value that goes in the bin is based on the magnitude\n",
        "\n",
        "For instance on the toy example:\n",
        "first pixel has:\n",
        "* mag = 6; orn = 45°. So, the vote of this pixel goes for 75% in the bin of 40°, and 25% in the bin of 60°, as closer to 40°. As a result, we add 4.5 to bin nb 3, and 1.5 to bin nb 4. -->"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "erQwBkq35RQ-",
        "colab_type": "text"
      },
      "source": [
        "##HOG: Exec summary\n",
        "\n",
        "* one cell (typ 8 x 8) of an image is represented by a histogram \n",
        "    * the orientation and magnitude of the gradient are computed on each pixel\n",
        "    * orientation of the gradient indicate a bin\n",
        "    * magnitude indicate the amount to place into the bin\n",
        "* the histogram is a vector of size 9 (as 9 bins)\n",
        "* one block (16 x 16) histogram is the concatenation of the 4 histograms, each representing one cell of the block, hence represented by a (36 x 1) vector.\n",
        "* the final HOG feature vector is based on the concatenation of all blocks.\n",
        "\n",
        "\n",
        "For an image of 64 x 64, we then have:\n",
        "* w = 64/8 = 8\n",
        "* h = 64/8 = 8\n",
        "* #cells = 8x8\n",
        "* #Blocks = 7 * 7 = 49 blocks\n",
        "\n",
        "each block has a representative vector of dimension $(36 \\times 1)$, and the resulting vector has dimension $(49*36 \\times 1)$, or $(1764,)$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tfMDFGqsZvzC",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NtYBaU-bzkio",
        "colab_type": "text"
      },
      "source": [
        "Still to do : \n",
        " These [hog] parameters were obtained by experimentation and examining the accuracy of the classifier — you should expect to do this as well whenever you use the HOG descriptor. Running experiments and tuning the HOG parameters based on these parameters is a critical component in obtaining an accurate classifier."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "geHORWA5mMdM",
        "colab_type": "text"
      },
      "source": [
        "2ND - detecting an object of interest in a new image \n",
        "\n",
        "idea: find cropped face in the global idea"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rodK9Dor7oeR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tVlqSL-Y2Y3F",
        "colab_type": "text"
      },
      "source": [
        "Compute local descriptor for all faces_cropped of the training dataset => hog_training{}:\n",
        "\n",
        "=> `hog_training[person_i] = [(fd_0, hog_image_0), ..., (fd_n, hog_image_n)]`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "guoG2jWm4Dv3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "hog_training = {}\n",
        "\n",
        "for person in persons:\n",
        "    hog_training[person] = []\n",
        "    for src_img in training_set[person]:\n",
        "        resized_img = cv2.resize(src_img, (64,64))\n",
        "        fd, hog_image = skimage_feature_hog(resized_img, \n",
        "                                            orientations=9, \n",
        "                                            pixels_per_cell=(8,8), \n",
        "                                            cells_per_block=(2, 2), \n",
        "                                            block_norm = \"L2\",\n",
        "                                            visualize=True, \n",
        "                                            transform_sqrt = True,\n",
        "                                            multichannel=True)\n",
        "        hog_training[person].append((fd, hog_image, resized_img))\n",
        "\n",
        "logging.debug(pretty_return_dict_size(hog_training))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "55hZpBQoGLdD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_local_crop(src_img, center_pixel, crop_shape, show = False):\n",
        "    \"\"\"\n",
        "    src_img:\n",
        "    center_pixel:\n",
        "    crop_shape:\n",
        "    \"\"\"\n",
        "    crop_height =  crop_shape[0]\n",
        "    crop_width = crop_shape[1]\n",
        "    top_= center_pixel[0] - crop_height // 2\n",
        "    bottom_ = center_pixel[0] + crop_height // 2\n",
        "    left_ = center_pixel[1] - crop_width // 2  \n",
        "    right_ = center_pixel[1] + crop_width // 2\n",
        "    if len(src_img.shape)> 2:\n",
        "        crop = src_img[ top_ : bottom_, left_:right_, :]\n",
        "    else:\n",
        "        crop = src_img[ top_ : bottom_, left_:right_]\n",
        "\n",
        "    if show:\n",
        "        cv2_imshow(crop)\n",
        "    return crop\n",
        "\n",
        "def match_hog(src_img, hog_desc, original_face_shape, step = 16, show = False):\n",
        "    \"\"\"\n",
        "    src_img : image to analyse\n",
        "    hog_desc: (fd, hog_image) of the corresponding faces_cropped\n",
        "    original_face_shape: shape of the face used for hog_desc computation\n",
        "    \"\"\"\n",
        "    height = src_img.shape[0] # height of the image to analyze\n",
        "    width  = src_img.shape[1] # width of the image to analyze\n",
        "    \n",
        "    height_face = original_face_shape[0]\n",
        "    width_face = original_face_shape[1]\n",
        "\n",
        "    res_shape = (src_img.shape[0], src_img.shape[1])\n",
        "    res = np.ones(res_shape)*-1\n",
        "\n",
        "    if show:\n",
        "        tmp_image = src_img.copy()\n",
        "        cv2.rectangle(tmp_image, (width_face//2, height_face//2), (width - width_face//2, height - height_face//2), (0, 255, 0))\n",
        "        cv2_imshow(tmp_image)\n",
        "\n",
        "    running_h_idx = range(height_face //2, height - height_face//2+1, step)\n",
        "    running_w_idx = range(width_face //2, width - width_face//2+1, step)\n",
        "\n",
        "    for h_idx in running_h_idx:\n",
        "        for w_idx in running_w_idx:\n",
        "            local_crop = get_local_crop(src_img, (h_idx, w_idx), original_face_shape, False)\n",
        "            local_resized_img = cv2.resize(local_crop, (64,64))\n",
        "            local_fd = skimage_feature_hog(local_resized_img, \n",
        "                                            orientations=9, \n",
        "                                            pixels_per_cell=(8,8), \n",
        "                                            cells_per_block=(2, 2), \n",
        "                                            block_norm = \"L2\",\n",
        "                                            visualize=False, \n",
        "                                            transform_sqrt = True,\n",
        "                                            multichannel=True)\n",
        "            res[h_idx,w_idx]= np.linalg.norm(local_fd-hog_desc[0])\n",
        "            # logging.debug(res[h_idx, w_idx])\n",
        "            if show:\n",
        "                cv2_imshow(local_resized_img)\n",
        "    \n",
        "\n",
        "    # find along height the indexes that are not empty\n",
        "    # find along width the indexes that are not empty\n",
        "    # call function RectBivariateSpline(y, x, Z), from scipy.interpolate.RectBivariateSpline(x, y, z, bbox=[None, None, None, None], kx=3, ky=3, s=0)[source]\n",
        "    return res\n",
        "\n",
        "def fill_matrix_min_neighbours(matrx, size_to_consider = 16, margin = 0):\n",
        "    \"\"\"\n",
        "    fill the gaps in the  computation by taking the min values from closest neighbours\n",
        "    that were computed.\n",
        "    \"\"\"\n",
        "    l = np.argwhere(matrx != -1)\n",
        "    res = matrx.copy()\n",
        "\n",
        "    if len(l)<2:\n",
        "        idx_to_change = res == -1\n",
        "        logging.warning(\"len < 2 --> len(idx_to_change) = \" + str(len(idx_to_change)))\n",
        "        res[idx_to_change] = res.max() + margin\n",
        "        return res\n",
        "\n",
        "    top_left_corner = l[0]\n",
        "    bottom_right_corner = l[-1]\n",
        "\n",
        "    for i in range(top_left_corner[0], bottom_right_corner[0]+size_to_consider//2):\n",
        "        for j in range(top_left_corner[1],bottom_right_corner[1]+size_to_consider//2):\n",
        "            if matrx[i,j] == -1:\n",
        "                local_roi = get_local_crop(matrx, (i,j),(size_to_consider,size_to_consider))\n",
        "                cand = local_roi[ local_roi != -1 ]\n",
        "                res[i,j] = cand.min()\n",
        "    idx_to_change = res == -1\n",
        "    res[idx_to_change] = res.max() + margin\n",
        "    return res"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KUtgffa1HZey",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "idx_of_interest = 5\n",
        "\"\"\"\n",
        " regarding the hog template \n",
        "\"\"\"\n",
        "object_of_interest = training_set[personA][idx_of_interest]\n",
        "image_of_interest = training_set[personA][idx_of_interest]\n",
        "template_hog = hog_training[personA][idx_of_interest]\n",
        "original_face_shape = training_set[personA][idx_of_interest].shape\n",
        "\n",
        "local_crop = get_local_crop(image_of_interest, (original_face_shape[0]//2, original_face_shape[1]//2), original_face_shape, False)\n",
        "local_crop_resized = cv2.resize(local_crop, (64,64))\n",
        "\n",
        "\n",
        "\n",
        "logging.debug(\"object to find: \" +  str(object_of_interest.shape))\n",
        "cv2_imshow(object_of_interest)\n",
        "logging.debug(\"based image to analyze: \" +  str(image_of_interest.shape))\n",
        "cv2_imshow(image_of_interest)\n",
        "logging.debug(\"local_crop to analyze: \" +  str(local_crop.shape))\n",
        "cv2_imshow(local_crop)\n",
        "logging.debug(\"local_crop_resized: \" +  str(local_crop_resized.shape))\n",
        "cv2_imshow(local_crop_resized)\n",
        "\n",
        "#####################################################################################\n",
        "\n",
        "local_fd = skimage_feature_hog(local_crop_resized, \n",
        "                               orientations=9, \n",
        "                               pixels_per_cell=(8,8), \n",
        "                               cells_per_block=(2, 2), \n",
        "                               block_norm = \"L2\",\n",
        "                               visualize=False, \n",
        "                               transform_sqrt = True,\n",
        "                               multichannel=True)\n",
        "            \n",
        "logging.debug(\"Template hog: \" + str(template_hog[0]))\n",
        "logging.debug(\"Computed hog: \" + str(local_fd))\n",
        "\n",
        "res = np.linalg.norm(local_fd-template_hog[0])\n",
        "\n",
        "logging.debug(\"Results: \" + str(res))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qPLRtjqEFjdy",
        "colab_type": "text"
      },
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L06xUrOubbu_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "idx_of_interest = 5\n",
        "\"\"\"\n",
        " regarding the hog template \n",
        "\"\"\"\n",
        "object_of_interest  = training_set[personA][idx_of_interest]\n",
        "template_hog        = hog_training[personA][idx_of_interest]\n",
        "original_face_shape = training_set[personA][idx_of_interest].shape\n",
        "logging.debug(\"object to find:\")\n",
        "cv2_imshow(object_of_interest)\n",
        "\n",
        "counter = 0\n",
        "upper_bound = 1 # len(images[personA])\n",
        "for img in images[personA]:\n",
        "    if counter >= upper_bound:\n",
        "        break;\n",
        "    else:\n",
        "        counter+=1\n",
        "\n",
        "    image_of_interest = img\n",
        "    # image_of_interest = faces_cropped[personA][idx_of_interest]\n",
        "\n",
        "    # logging.debug(\"Object to find - shape = \" + str(original_face_shape)) \n",
        "\n",
        "    # logging.debug(\"Image of interest:\")\n",
        "    # cv2_imshow(image_of_interest)\n",
        "\n",
        "    step=16\n",
        "    res = match_hog(image_of_interest, hog_training[personA][idx_of_interest], original_face_shape, step)\n",
        "    new_res = fill_matrix_min_neighbours(res, step)\n",
        "\n",
        "    logging.debug(\"worst match hog results: \" + str(new_res.max()))\n",
        "    logging.debug(\"best match hog results: \" + str(new_res.min()))\n",
        "\n",
        "    b = new_res.copy()\n",
        "    bmax, bmin = b.max(), b.min()\n",
        "    if bmax == bmin and bmax == 0:\n",
        "        logging.info(\"Perfect match!\")\n",
        "    # b = (b - bmin)/(bmax - bmin)\n",
        "    b = (b - bmin)/(bmax)\n",
        "\n",
        "    logging.debug(\"worst match hog results normalized [expexted 1]: \" + str(b.max()))\n",
        "    logging.debug(\"best match hog results [expected 0]: \" + str(b.min()))\n",
        "\n",
        "\n",
        "\n",
        "    fig, (ax1, ax2,ax3) = plt.subplots(1,3 , figsize=(16, 8), sharex=True, sharey=True) \n",
        "\n",
        "    ax1.imshow(cv2.cvtColor(image_of_interest,cv2.COLOR_BGR2RGB))\n",
        "    ax1.set_title(\"Image where to find base face\")\n",
        "\n",
        "    ax2.imshow(new_res)\n",
        "    ax2.set_title(\"Results gaps filled with min neighbour\")\n",
        "\n",
        "    ax3.imshow(b)\n",
        "    ax3.set_title(\"Normalized results of Matching\")\n",
        "    plt.show()\n",
        "    logging.debug(\"=====================================================\")\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SpA0M6qxccoE",
        "colab_type": "text"
      },
      "source": [
        "### HOG - [XXX]\n",
        "In the previous section, still missing:\n",
        "* organization in different steps using\n",
        "   \n",
        "* normalization and pre-processing (exactly)\n",
        "* block-normalization\n",
        "* validation set in order to find the parameters of the HOG\n",
        "* explain several method to compute the gradients\n",
        "* set up the visualization of the inputs (dataset)\n",
        "* properly right a toy example for each step"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FMBCO6xyhskK",
        "colab_type": "text"
      },
      "source": [
        "##  Principal Component Analysis - **PCA**\n",
        "\n",
        "- How to convert my image (you can work in color if you like) dataset to a 2D matrix?\n",
        "- Can you exploit the dimensionality of this data matrix to make your computations more effective?\n",
        "- Mean subtraction or not?\n",
        "- Shall we use eigenvalue or singular value decomposition?\n",
        "- How many non-zero eigenvalues/singular values should we have?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zIFK9vVWiyuf",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "### Step1: Pre-processing\n",
        "\n",
        "#### Get the data\n",
        "\n",
        "#### Sizing and Scaling\n",
        "\n",
        "#### Grayscale"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C293QFgzgezY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "Build useable training set from (hyper-)parameters\n",
        "\"\"\"\n",
        "\n",
        "# column 0 = first image\n",
        "# column 1 = second image\n",
        "# ...\n",
        "m_src = get_matrix_from_set(training_set, color, sq_size = sq_size, flatten = True)\n",
        "\n",
        "logging.debug(\" src_mat_training: original matrix\")\n",
        "logging.debug(\" src_mat_training: shape = \" + str(m_src.shape))\n",
        "\n",
        "plot_matrix(m_src, color, my_color_map, h=4, w=10, transpose = False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "foS_IqQljwTu",
        "colab_type": "text"
      },
      "source": [
        "#### Process the test sets as a matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_tDYQ-ODjvAV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "Build useable test set from (hyper-)parameters\n",
        "\"\"\"\n",
        "m_test_src = get_matrix_from_set(test_set, color, sq_size = sq_size, flatten = True)\n",
        "\n",
        "\n",
        "logging.debug(\" src_mat_test: original matrix\")\n",
        "logging.debug(\" src_mat_test: shape = \" + str(m_test_src.shape))\n",
        "\n",
        "\n",
        "plot_matrix(m_test_src, color, my_color_map,h = 4, w=10,transpose = False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JlzOen9a4Nq_",
        "colab_type": "text"
      },
      "source": [
        "###Step2: Centering the Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8uVi6DotjZF9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = m_src.copy()\n",
        "X_mean = np.mean(X, axis = 0)\n",
        "Xc = X - X_mean\n",
        "\n",
        "fig = plt.figure(figsize=(18, 5))\n",
        "\n",
        "Xs = np.arange(0, X.shape[0])\n",
        "Ys = np.arange(0, X.shape[1])\n",
        "Xs, Ys = np.meshgrid(Xs, Ys)\n",
        "  \n",
        "ax1 = fig.add_subplot(131,projection='3d')\n",
        "surf1 = ax1.plot_surface(Xs, Ys, X.T, cmap=plt.cm.jet, antialiased=True)\n",
        "ax1.set_xlabel('x1 label')\n",
        "ax1.set_ylabel('y1 label')\n",
        "ax1.set_zlabel('z1 label')\n",
        "\n",
        "ax2 = fig.add_subplot(132)\n",
        "ax2.imshow(my_reshape(X_mean, sq_size, color), cmap = my_color_map, interpolation='nearest') \n",
        "\n",
        "# ax2.imshow(X_mean.reshape(sq_size,sq_size), cmap=plt.cm.gray)\n",
        "\n",
        "# ax2.plot(np.arange(0, m_src.shape[0]), m_src_means, 'bo')\n",
        "# ax2.plot(np.arange(0, m_src.shape[1]), m_src_max, 'go')\n",
        "# ax2.plot(np.arange(0, m_src.shape[1]), m_src_min, 'ro')\n",
        "\n",
        "ax3 = fig.add_subplot(133,projection='3d')\n",
        "surf3 = ax3.plot_surface(Xs, Ys, Xc.T, cmap=plt.cm.jet, antialiased=True)\n",
        "ax3.set_xlabel('x3 label')\n",
        "ax3.set_ylabel('y3 label')\n",
        "ax3.set_zlabel('z3 label')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vc6kwotfxtH_",
        "colab_type": "text"
      },
      "source": [
        "### *Canonical* PCA\n",
        "\n",
        "#### Data \n",
        "Let's take X, a $(n \\times p)$ matrix of data where n is the number of images, and p is the number of variables. In this case, the number of variables is the number of pixels of one image.\n",
        "\n",
        "First, we need to center the data, hence substracting the mean image.\n",
        "\n",
        "#### Covariance Matrix \n",
        "\n",
        "Then, we can compute the covariance matrix, that indicates how a variable (= pixel intensity) varies with respect to other pixels. The Covariance matrix indicates how the variables evolve with respect to each others. $C = \\frac{X^T \\cdot X}{n-1}$ and has dimension $(p \\times p)$. This is therefore a pretty large matrix already.\n",
        "\n",
        "#### Eigenvalues and EigenVectors\n",
        "Having computed C, we can compute its eigenvectors and eigenvalues, indicating the main directions (and their strength) of how the data evolve. $C = V * L * V^T$ where\n",
        "- $L$ is a diagonal matrix of eigenvalues\n",
        "- $V$ is the $(p \\times p)$ matrix of eigenvectors\n",
        "\n",
        "##### Mathematical Trick: Exploit the dimensionality of the matrix\n",
        "Computing the eigenvalues and eigenvectors of C can be pretty cumbersome, as C is a large matrix $(p \\times p)$. \n",
        "Recalling our algebra skills, one can however note that given the dimension of $X$, only a limited amount of eigenvalues are of interest (non zero). There is only $(n-1)$ non zero eigenvalues. There is actually no need to compute the $p$ eigenvalues and related $(p \\times p)$ eigenvectors matrix as the information is contained in only $(n-1)$ eigenvalues.\n",
        "\n",
        "\n",
        "As a result, to speed up the computation and take advantage of this property, instead of computing the eigenvalues and eigenvectors of $C = \\frac{X^T \\cdot X}{n-1}$ of size $(p \\times p)$, let's rather compute the $n$ eigenvalues and corresponding eigenvectors of the matrix $D = \\frac{X \\cdot X^T}{n-1}$ of size $(n \\times n)$, such that $D = W * L * W^T$\n",
        "- the eigenvalues computed are the same of $C$\n",
        "- the corresponding eigenvectors of C, in matrix $V$, are related such that $V = X^T \\cdot W$\n",
        "\n",
        "This way, it takes advantage of the dimension of the problem.\n",
        "\n",
        "#### Principal Components\n",
        "\n",
        "The principal components are defined as the eigenvectors V. \n",
        "##### Scores\n",
        "By projecting the original data $X$ on the new directions, hence the eigenvectors, we get *new coordinates* that yet fully describe the original data. \n",
        "\n",
        "Furthermore, the number of eigenvectors on which we project the data is reduced with respect to original problem dimensionality.   \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4HE5XZ2h1kmX",
        "colab_type": "text"
      },
      "source": [
        "## Singular Value Decomposition\n",
        "\n",
        "### Idea\n",
        "The idea is different.\n",
        "From a matrix X, centered, (as in previous section), one can compute its decomposition $X = U \\cdot S \\cdot V^T$ \n",
        "where $U$ is a unitary matrix, $S$ is diagonal, containing what's called the singular values $s_i$. \n",
        "One can see that $V$, right singular vectors, are related to eigenvectors of the covariance matrix from previous section.\n",
        "\n",
        "Indeed, computing this covariance matrix gives:\n",
        "\n",
        "\\begin{align}\n",
        "Cov &= \\frac{X^T \\cdot X}{n-1} \\\\\n",
        "    &= \\frac{V \\cdot S \\cdot U^T \\cdot U \\cdot S \\cdot V}{n-1} \\\\\n",
        "    &= V \\cdot \\frac{S^2}{n-1} V^T\n",
        "\\end{align}\n",
        "\n",
        "There is therefore a link between the singular values ($s_i$) and the eigenvalues ($\\lambda_i$):\n",
        "$$\\lambda_i = \\frac{s_i^2}{n-1}$$ and the right singular vectors are the eigenvectors $V$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Rr9R4fwgtK_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MyPCA:\n",
        "    \"\"\"\n",
        "    homemade class to perform PCA using several methods and compare\n",
        "    \"\"\"\n",
        "    def __init__(self, method = \"svd\"):\n",
        "        self.method = method\n",
        "        self.eigenvalues = None\n",
        "        self.eigenvectors = None\n",
        "        self.X_mean = None\n",
        "\n",
        "    def fit(self, data):\n",
        "        X = data.copy()\n",
        "        n, m = X.shape\n",
        "        assert n < m, \"n is not smaller than m -> you most likely need \" + \\\n",
        "                    \"to transpose your input data\"\n",
        "        self.X_mean = np.mean(X, axis = 0)\n",
        "        X -= self.X_mean\n",
        "\n",
        "        self.eigenvalues = None\n",
        "        self.eigenvectors = None\n",
        "\n",
        "        if self.method == \"svd\":\n",
        "            # singular value decomposition\n",
        "            U, S, Vt = np.linalg.svd(X)\n",
        "            self.eigenvalues = S**2 / (n - 1)\n",
        "            self.eigenvectors = Vt.T[:,0:n]\n",
        "\n",
        "        elif self.method == \"eigen_fast\":\n",
        "\n",
        "            # compute small covariance matrix\n",
        "            D = np.dot(X, X.T) / (n - 1)\n",
        "\n",
        "            # eigen decomposition\n",
        "            LD, W = np.linalg.eig(D)\n",
        "\n",
        "            order_D = np.argsort(LD)[::-1]\n",
        "            LD_sorted = LD[order_D]\n",
        "            W_sorted = W[:,order_D]\n",
        "            \n",
        "            eigenVectors_sorted_tmp = np.dot(X.T, W_sorted)\n",
        "            eigenVectors_sorted = np.empty(eigenVectors_sorted_tmp.shape)\n",
        "            for i in range(n):\n",
        "                v = eigenVectors_sorted_tmp[:,i]\n",
        "                eigenVectors_sorted[:,i] = v / np.linalg.norm(v)\n",
        "            \n",
        "            self.eigenvalues = LD_sorted\n",
        "            self.eigenvectors = eigenVectors_sorted\n",
        "                \n",
        "        elif self.method ==\"eigen\":\n",
        "            # compute covariance matrix\n",
        "\n",
        "            Cov = np.dot(X.T, X) / (n - 1)\n",
        "\n",
        "            # eigen decomposition\n",
        "            LC, V =np.linalg.eig(Cov)\n",
        "\n",
        "            # sort in appropriate order and keep only relevant component\n",
        "            order = np.argsort(LC)[::-1]\n",
        "            LC_sorted = LC[order][0:n]\n",
        "            V_sorted = V[:,order][:,0:n]\n",
        "\n",
        "            self.eigenvalues = LC_sorted\n",
        "            self.eigenvectors = V_sorted.real\n",
        "        else:\n",
        "            raise RuntimeError(\"method value unknown\")\n",
        "        \n",
        "    def projectPC(self, X, k):\n",
        "        Vk = self.eigenvectors[:, 0:k]\n",
        "        # logging.debug(\"Reduce X \" + str(X.shape) + \"using k = \" + str(k) + \" components\")\n",
        "        # logging.debug(\"Vk (4096 x k)= \" + str(Vk.shape))\n",
        "        X_reduced = np.dot(X, Vk)\n",
        "        # logging.debug(\"X_reduced (n x k) = \" + str(X_reduced.shape))\n",
        "\n",
        "        return X_reduced\n",
        "    \n",
        "    def reconstruct(self, X_reduced, k, show = False):\n",
        "        if len(X_reduced.shape) > 1:\n",
        "            X_reduced = X_reduced[:,0:k]\n",
        "        else:\n",
        "            X_reduced = X_reduced[0:k]\n",
        "        \n",
        "        Vkt = self.eigenvectors[:, 0:k].T\n",
        "        # logging.debug(\"Reduce using X_reduced = \" + str(X_reduced.shape) )\n",
        "        # logging.debug(\"Reconstruct using k = \" + str(k) + \" components\")\n",
        "        # logging.debug(\"Vkt (k x 4096)= \" + str(Vkt.shape))\n",
        "        X_hat_centered = np.dot(X_reduced, Vkt)\n",
        "        # logging.debug(\"X_hat_centered.shape (20,4096): \" + str(X_hat_centered.shape))\n",
        "        if show:\n",
        "            self.show_data(X_hat_centered, add_mean = True)\n",
        "        return X_hat_centered \n",
        "\n",
        "    def compute_error(self, X, X_hat):\n",
        "        return sqrt(mean_squared_error(X, X_hat))\n",
        "\n",
        "    def show_principal_components(self,k):\n",
        "        fig = plt.figure(figsize=(5,8)) \n",
        "        fig.subplots_adjust(left=0, right=1, bottom=0, top=1, hspace=0.05, wspace=0.05) \n",
        "\n",
        "        logging.debug(\"self.eigenvectors.shape = \" + str(self.eigenvectors.shape) )\n",
        "        for i in range(k):\n",
        "            pc = self.eigenvectors[:,i]\n",
        "            assert pc.shape[0]==(sq_size**2)*1 or pc.shape[0]==(sq_size**2)*3, \"Not proper shape (expected (sq_size**2) (*3)) \" + str(pc.shape)\n",
        "            \n",
        "            ax = fig.add_subplot(8, 5, i+1, xticks=[], yticks=[]) \n",
        "            if color:\n",
        "                pc_img = (np.reshape(pc.real, (sq_size, sq_size, 3))*255).astype(\"uint8\") \n",
        "                ax.imshow(pc_img, interpolation='nearest') \n",
        "            else:\n",
        "                pc_img = np.reshape(pc.real, (sq_size, sq_size))\n",
        "                ax.imshow(pc_img  , cmap=plt.cm.gray, interpolation='nearest')\n",
        "\n",
        "        plt.show()\n",
        "\n",
        "    def compute_explained_variance(self, show=True):\n",
        "        sum_all_eigenValues = sum(self.eigenvalues)\n",
        "        logging.debug(\"\\nSum of all eigenValues: \" + str(sum_all_eigenValues))\n",
        "\n",
        "        explained_variance      = [(value / sum_all_eigenValues)*100 for value in self.eigenvalues]\n",
        "        cum_explained_variance  = np.cumsum(explained_variance)\n",
        "        logging.debug(\"Cum explained variance : \\n\" + str(cum_explained_variance))\n",
        "        if show:\n",
        "            fig = plt.figure(figsize=(12, 6))\n",
        "            ax1 = fig.add_subplot(121)\n",
        "            ax1.bar(range(len(self.eigenvalues)), self.eigenvalues)\n",
        "            ax1.set_xlabel('eigenvalues')\n",
        "            ax1.set_ylabel('values')\n",
        "\n",
        "            ax2 = fig.add_subplot(122)\n",
        "            ax2.bar(range(len(explained_variance)), explained_variance)\n",
        "            ax2.plot(range(len(cum_explained_variance)), cum_explained_variance, color='green', linestyle='dashed', marker='o', markersize=5)\n",
        "\n",
        "            ax2.set_xlabel('eigenvalues')\n",
        "            ax2.set_ylabel('% information ')\n",
        "            ax2.legend( labels = [\"Cumulative Expl. Var.\", \"Explained Variance\"])\n",
        "            ax2.grid()\n",
        "            plt.show()\n",
        "        return explained_variance, cum_explained_variance\n",
        "\n",
        "    def show_data(self, X, add_mean = False):\n",
        "        \n",
        "        # copy so that adding the mean does not modify the original centered\n",
        "        # data X\n",
        "        X_ = X.copy()\n",
        "\n",
        "        if len(X.shape) > 1:\n",
        "            self._show_data(X_, add_mean)\n",
        "        else:\n",
        "            fig = plt.figure(figsize=(3,3))\n",
        "            if add_mean:\n",
        "                    X_ += self.X_mean\n",
        "            # img = np.reshape(X_, (sq_size, sq_size))\n",
        "            img = my_reshape(X_, sq_size, color)\n",
        "\n",
        "            ax = fig.add_subplot(1, 1, 1, xticks=[], yticks=[]) \n",
        "            ax.imshow(img, cmap = my_color_map, interpolation='nearest') \n",
        "\n",
        "            plt.show()\n",
        "\n",
        "\n",
        "    def _show_data(self, X, add_mean = False):\n",
        "        fig = plt.figure(figsize=(8,8)) \n",
        "        fig.subplots_adjust(left=0, right=1, bottom=0, top=1, hspace=0.05, wspace=0.05) \n",
        "        i=0\n",
        "        for x in X:\n",
        "            if add_mean:\n",
        "                    x += self.X_mean\n",
        "            # assert x.shape[0]==(sq_size**2)*3, \"Not proper shape (expected (sq_size**2)*3) \" + str(x.shape)\n",
        "            ax = fig.add_subplot(8, 5, i+1, xticks=[], yticks=[]) \n",
        "            img = my_reshape(x, sq_size, color) \n",
        "            ax.imshow(img, cmap = my_color_map, interpolation='nearest') \n",
        "\n",
        "            i+=1\n",
        "        plt.show()\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T1OfmaOV4tkL",
        "colab_type": "text"
      },
      "source": [
        "## use of the code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ZZagj0-bU7a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "my_pca = None\n",
        "nb_training_faces = sum(training_sets_size.values())\n",
        "X = m_src.copy()\n",
        "my_pca = MyPCA(\"svd\")\n",
        "my_pca.fit(X)\n",
        "my_pca.show_principal_components(k=nb_training_faces)\n",
        "\n",
        "X = m_src.copy()[9,:]\n",
        "logging.debug(\" Shape X = \" + str((X.shape)))\n",
        "Xc = X - np.mean(m_src.copy(), axis=0)\n",
        "\n",
        "\n",
        "# my_pca_svd.show_data(X)\n",
        "\n",
        "# reduce the image to the principal components (all first)\n",
        "X_reduced = my_pca.projectPC(Xc, k=nb_training_faces)\n",
        "logging.debug(\"shape of X_reduced = \" + str(X_reduced.shape))\n",
        "\n",
        "# reconstruct original data based on k first components only\n",
        "for k in [1,2,3,5,8,10,12,15,20,25,30,40]:\n",
        "    logging.debug(\"==\"*30)\n",
        "    X_hat_centered = my_pca.reconstruct(X_reduced, k, show=True)\n",
        "    logging.debug(\"PC used = \" + str(k) + \"; Reconstruction error = \" + str(my_pca.compute_error(Xc, X_hat_centered)))\n",
        "\n",
        "\n",
        "my_pca.show_data(X)\n",
        "expl_var, cum_expl_var = my_pca.compute_explained_variance(show = True)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S2Kkdwd456FK",
        "colab_type": "text"
      },
      "source": [
        "Plot 40 faces projected on the two first principal components."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QCv5xuzT0VTx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "my_pca = None\n",
        "\n",
        "X = m_src.copy()\n",
        "my_pca = MyPCA(\"svd\")\n",
        "my_pca.fit(X)\n",
        "# my_pca.show_principal_components(k=nb_training_faces)\n",
        "\n",
        "data = m_src.copy()\n",
        "data_mean = np.mean(m_src.copy(), axis = 0)\n",
        "\n",
        "data_centered = data - data_mean\n",
        "\n",
        "# my_pca_svd.show_data(X)\n",
        "\n",
        "# reduce the image to the principal components (all first)\n",
        "data_projected = my_pca.projectPC(data_centered, k=2)\n",
        "logging.debug(\"shape of data_projected = \" + str(data_projected.shape))\n",
        "\n",
        "\n",
        "plt.figure() \n",
        "fig, ax = plt.subplots(1, 1, figsize=(16, 16), sharex=True, sharey=True)\n",
        "\n",
        "eig1 = data_projected[:,0]\n",
        "eig2 = data_projected[:,1]\n",
        "ax.plot(eig1, eig2, 'bo')\n",
        "\n",
        "print(len(data))\n",
        "\n",
        "for (x_, y_), img_vector_ in zip(data_projected, data):\n",
        "    img = my_reshape(img_vector_, sq_size, color)\n",
        "    ab = AnnotationBbox(OffsetImage(img, cmap = my_color_map), (x_, y_), frameon=False)\n",
        "    ax.add_artist(ab)\n",
        "\n",
        "ax.grid()\n",
        "plt.show()\n",
        "\n",
        "plt.figure() \n",
        "fig, ax = plt.subplots(1, 1, figsize=(8, 8), sharex=True, sharey=True)\n",
        "\n",
        "ax.plot(eig1[0:20], eig2[0:20], 'ro')\n",
        "ax.plot(eig1[20:40], eig2[20:40], 'go')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DTta4jFe8oN8",
        "colab_type": "text"
      },
      "source": [
        "Interesting to observe the scale of the data => for every image, the mins and the maxs are at similar values. Extra scale of the data to have all of them on the same scale is therefore for required."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5tAzt5CpMqTf",
        "colab_type": "text"
      },
      "source": [
        "Debrief about the information contained in each eigen values - principal dimension"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L6P-anlyeL3Z",
        "colab_type": "text"
      },
      "source": [
        "##Demo scikit learn\n",
        "\n",
        "Of course, everything that has been done so far regarding PCA can be achieved using dedicated  - and optimized - libraries. For that purpose, we can use the `sklearn.decomposition` package that, among other things, implement the PCA using the *SVD* decomposition that we've looked at. \n",
        "\n",
        "A difference to note is the use, internally, of the `svd_flip(u, vt)`, a function that ensures the vectors to be deterministic, hence solving the sign ambiguity inherent to matrix decomposition.\n",
        "\n",
        "The following part performs the same operation as we've implemented before. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lhJkmJSQBtYk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "images_faces_tests = []\n",
        "for person in persons:\n",
        "    for src in training_set[person]:\n",
        "        src_rescaled = cv2.resize(src, (sq_size,sq_size))\n",
        "        if color:\n",
        "            src_gray = src_rescaled\n",
        "        else:\n",
        "            src_gray = cv2.cvtColor(src_rescaled, cv2.COLOR_BGR2GRAY)\n",
        "        images_faces_tests.append(src_gray)\n",
        "    \n",
        "fig = plt.figure(figsize=(10,4)) \n",
        "fig.subplots_adjust(left=0, right=1, bottom=0, top=1, hspace=0.05, wspace=0.05) \n",
        "# plot the faces, each image is 64 by 64 pixels \n",
        "for i in range(40): \n",
        "    ax = fig.add_subplot(4, 10, i+1, xticks=[], yticks=[]) \n",
        "    img_ = my_reshape(images_faces_tests[i], sq_size, color)\n",
        "    ax.imshow(img_, my_color_map, interpolation='nearest') \n",
        "\n",
        "plt.show() \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2l2rE-lgCT29",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = m_src.copy()\n",
        "n_components = 40\n",
        "print(\"Shape input data: \", X.shape)\n",
        "pca_ = sklearn_decomposition_PCA(n_components=n_components) \n",
        "pca_.fit(X)\n",
        "\n",
        "eigenfaces = pca_.components_\n",
        "logging.debug(\"eigenfaces shape = \" + str(eigenfaces.shape))\n",
        "i=0\n",
        "fig = plt.figure(figsize=(5,8)) \n",
        "fig.subplots_adjust(left=0, right=1, bottom=0, top=1, hspace=0.05, wspace=0.05) \n",
        "for face in eigenfaces:\n",
        "    # plot the faces, each image is 64 by 64 pixels \n",
        "    ax = fig.add_subplot(8, 5, i+1, xticks=[], yticks=[]) \n",
        "    if color:\n",
        "        face_ = (np.reshape(face, (sq_size, sq_size, 3))*255).astype(\"uint8\")\n",
        "    else:\n",
        "        face_ = np.reshape(face, (sq_size, sq_size))\n",
        "    # ax.imshow(np.reshape(face, (sq_size, sq_size)), cmap=plt.cm.gray, interpolation='nearest') \n",
        "    ax.imshow(face_, cmap = my_color_map, interpolation='nearest') \n",
        "    i+=1\n",
        "plt.show() \n",
        "\n",
        "\n",
        "\n",
        "# X_proj = pca_oliv.fit_transform(m_src) \n",
        "# print(X_proj.shape)\n",
        "# print(\"Explained Variances:\\n \", pca_.explained_variance_)\n",
        "# print(\"Singular Values:\\n \", pca_.singular_values_)\n",
        "# print(\"Cum explained variance : \\n \", np.cumsum(pca_.explained_variance_ratio_))\n",
        "\n",
        "# fig = plt.figure(figsize=(8, 4))\n",
        "# ax1 = fig.add_subplot(121)\n",
        "# ax1.bar(range(len(pca_.singular_values_)),pca_.singular_values_)\n",
        "# ax1.set_xlabel('x1 label')\n",
        "# ax1.set_ylabel('y1 label')\n",
        "\n",
        "# ax2 = fig.add_subplot(122)\n",
        "# ax2.plot(range(len(np.cumsum(pca_.explained_variance_ratio_))), np.cumsum(pca_.explained_variance_ratio_), color='green', linestyle='dashed', marker='o', markersize=5)\n",
        "# ax2.bar(range(len(pca_.singular_values_)),pca_.singular_values_/sum(pca_.singular_values_))\n",
        "\n",
        "# ax2.set_xlabel('x2 label')\n",
        "# ax2.set_ylabel('y2 label')\n",
        "# ax2.grid()\n",
        "# plt.show()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "logging.debug(\"Projecting the input data on the eigenfaces orthonormal basis\")\n",
        "# X_reduced_pca= pca_.transform(X[4,:].reshape(1, -1))\n",
        "X_reduced_pca= pca_.transform(X)\n",
        "logging.debug(\"X_reduced_pca shape = \" + str(X_reduced_pca.shape))\n",
        "\n",
        "X_hat = pca_.inverse_transform(X_reduced_pca)\n",
        "logging.debug(\"X_projected = \" + str(X_hat.shape))\n",
        "\n",
        "fig = plt.figure(figsize=(5,8)) \n",
        "fig.subplots_adjust(left=0, right=1, bottom=0, top=1, hspace=0.05, wspace=0.05) \n",
        "i=0\n",
        "for face in X_hat:\n",
        "    # plot the faces, each image is 64 by 64 pixels \n",
        "    ax = fig.add_subplot(8, 5, i+1, xticks=[], yticks=[]) \n",
        "    face_ = my_reshape(face, sq_size, color)\n",
        "    ax.imshow(face_, my_color_map, interpolation='nearest') \n",
        "    i+=1\n",
        "plt.show() \n",
        "logging.debug(\"Root Mean Squared Error: \" + str(sqrt(mean_squared_error(X, X_hat))))\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "reproducing the eigenface 1 and 2 projections and drawings\n",
        "\"\"\"\n",
        "data = m_src.copy()\n",
        "X_reduced_pca_2eigenfaces= pca_.transform(data)[:, 0:2]\n",
        "print(X_reduced_pca_2eigenfaces)\n",
        "\n",
        "plt.figure() \n",
        "fig, ax = plt.subplots(1, 1, figsize=(16, 16), sharex=True, sharey=True)\n",
        "eig1 = X_reduced_pca_2eigenfaces[:,0]\n",
        "eig2 = X_reduced_pca_2eigenfaces[:,1]\n",
        "ax.plot(eig1, eig2, 'bo')\n",
        "\n",
        "for (x_, y_), img_vector_ in zip(X_reduced_pca_2eigenfaces, data):\n",
        "    img = my_reshape(img_vector_, sq_size, color)\n",
        "    ab = AnnotationBbox(OffsetImage(img, cmap = my_color_map), (x_, y_), frameon=False)\n",
        "    ax.add_artist(ab)\n",
        "\n",
        "ax.grid()\n",
        "plt.show()\n",
        "plt.figure() \n",
        "fig, ax = plt.subplots(1, 1, figsize=(8, 8), sharex=True, sharey=True)\n",
        "\n",
        "ax.plot(eig1[0:20], eig2[0:20], 'ro')\n",
        "ax.plot(eig1[20:40], eig2[20:40], 'go')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TqITs-xBjIIE",
        "colab_type": "text"
      },
      "source": [
        "## projection of test faces (from the 4 persons) on the first two principal components\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qneLYLvEoAeD",
        "colab_type": "text"
      },
      "source": [
        "We need to apply the sames steps that were applied to the training set, to the test set. that is:\n",
        "\n",
        "\n",
        "1.   center the data: substracting the mean **of the training set**\n",
        "2.   projecting the resulting centered data on the two first principal components computed by applying PCA on the training data. We don't apply PCA here, we just project the test data using the already-found principal components\n",
        "3.   \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1uNKTkyvi5Sb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_test = m_test_src.copy()\n",
        "\n",
        "# data_test_mean = np.mean(m_test_src.T.copy(), axis = 0)\n",
        "\n",
        "data_test_centered = data_test - np.mean(m_src.copy(), axis = 0)\n",
        "\n",
        "# my_pca_svd.show_data(X)\n",
        "\n",
        "# reduce the image to the principal components (all first)\n",
        "data_test_projected = my_pca.projectPC(data_test_centered, k=3)\n",
        "logging.debug(\"shape of data_projected = \" + str(data_test_projected.shape))\n",
        "\n",
        "\n",
        "# plt.figure() \n",
        "# fig, ax = plt.subplots(1, 1, figsize=(16, 16), sharex=True, sharey=True)\n",
        "\n",
        "# eig1 = data_test_projected[:,0]\n",
        "# eig2 = data_test_projected[:,1]\n",
        "# eig3 = data_test_projected[:,2]\n",
        "# ax.scatter(eig1, eig2, eig3, 'b')\n",
        "\n",
        "# # for point in data_projected:\n",
        "# #     print(point)\n",
        "\n",
        "# # img = np.reshape(x, (sq_size, sq_size))\n",
        "\n",
        "# # ax.imshow(img  , cmap=plt.cm.gray, interpolation='nearest')\n",
        "# # i+=1\n",
        "# print(len(data_test))\n",
        "\n",
        "# for (x_, y_, z_), img_vector_ in zip(data_test_projected, data_test):\n",
        "#     img = np.reshape(img_vector_, (sq_size, sq_size))\n",
        "#     ab = AnnotationBbox(OffsetImage(img, cmap=plt.cm.gray), (x_, y_, z_), frameon=False)\n",
        "#     ax.add_artist(ab)\n",
        "\n",
        "# ax.grid()\n",
        "# plt.show()\n",
        "\n",
        "fig = plt.figure(figsize=(8,8))\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "eig1 = data_test_projected[:,0]\n",
        "eig2 = data_test_projected[:,1]\n",
        "eig3 = data_test_projected[:,2]\n",
        "ax.scatter(eig1[0:10], eig2[0:10], eig3[0:10], 'b')\n",
        "ax.scatter(eig1[10:20], eig2[10:20], eig3[10:20], 'r')\n",
        "ax.scatter(eig1[20:30], eig2[20:30], eig3[20:30], 'g')\n",
        "ax.scatter(eig1[30:40], eig2[30:40], eig3[30:40], 'y')\n",
        "plt.show()\n",
        "\n",
        "# fig = plt.figure(figsize=(24,8))\n",
        "\n",
        "# eig1 = data_test_projected[:,0]\n",
        "# eig2 = data_test_projected[:,1]\n",
        "# eig3 = data_test_projected[:,2]\n",
        "\n",
        "\n",
        "# ax1 = fig.add_subplot(1,3,1)\n",
        "# eig1 = data_test_projected[:,0]\n",
        "# eig2 = data_test_projected[:,1]\n",
        "# eig3 = data_test_projected[:,2]\n",
        "# ax1.plot(eig1[0:10], eig2[0:10], 'bo')\n",
        "# ax1.plot(eig1[10:20], eig2[10:20], 'ro')\n",
        "# ax1.plot(eig1[20:30], eig2[20:30], 'go')\n",
        "# ax1.plot(eig1[30:40], eig2[30:40], 'yo')\n",
        "\n",
        "# ax2 = fig.add_subplot(1,3,2)\n",
        "# ax2.plot(eig1[0:10], eig3[0:10], 'bo')\n",
        "# ax2.plot(eig1[10:20], eig3[10:20], 'ro')\n",
        "# ax2.plot(eig1[20:30], eig3[20:30], 'go')\n",
        "# ax2.plot(eig1[30:40], eig3[30:40], 'yo')\n",
        "\n",
        "# ax3 = fig.add_subplot(1,3,3)\n",
        "# ax3.plot(eig2[0:10], eig3[0:10], 'bo')\n",
        "# ax3.plot(eig2[10:20], eig3[10:20], 'ro')\n",
        "# ax3.plot(eig2[20:30], eig3[20:30], 'go')\n",
        "# ax3.plot(eig2[30:40], eig3[30:40], 'yo')\n",
        "\n",
        "# plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6MNLtaC9of-J",
        "colab_type": "text"
      },
      "source": [
        "Determining the optimal \"p\" components based on the average reconstruction error. \n",
        "\n",
        "Idea:\n",
        "1. get the 40 training data\n",
        "2. center the data by substracting the mean face\n",
        "3. compute the PCA on the training data\n",
        "4. reconstruct the image. For every reconstruction, compute the RMSE, and plot the RMSE vs PC used curve.\n",
        "5. ?????????????????????????????????????????????????????????? \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nlLn4VNMbzzu",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7jHGWbSAi5MF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train = m_src.copy()\n",
        "X_train_mean = np.mean(X_train, axis = 0)\n",
        "X_train_centered = X_train - X_train_mean\n",
        "\n",
        "my_pca = None\n",
        "my_pca = MyPCA(\"svd\")\n",
        "my_pca.fit(X_train)\n",
        "\n",
        "n = X_train.shape[0]\n",
        "\"\"\"\n",
        "np array containing all the rmse computed\n",
        "\n",
        "if n = 40, max number of components, then rmse has a size 40x40 (0 -> 39)\n",
        "> a row matches the rmse of one image wrt the dimension reconstructed. Last column should be 0 (or close, ~e-14)\n",
        "\"\"\"\n",
        "rmse = np.empty((n,n))\n",
        "index_image = 0\n",
        "for img_center_vector in X_train_centered:\n",
        "    # logging.debug(\"projecting on \" + str(n) + \" principal components\")\n",
        "    X_centered_reduced = my_pca.projectPC(img_center_vector, n)\n",
        "    for k in range(n):\n",
        "        # from 1 to n, included\n",
        "        p = k+1\n",
        "        # logging.debug(\"reconstructing using \" + str(p) + \" principal components\")\n",
        "        X_hat_centered = my_pca.reconstruct(X_centered_reduced, p)\n",
        "        rmse[index_image,k] = my_pca.compute_error(img_center_vector, X_hat_centered)\n",
        "    \n",
        "    index_image += 1\n",
        "\n",
        "fig = plt.figure(figsize = (16,8))\n",
        "ax1 = fig.add_subplot(1,2,1)\n",
        "\n",
        "for idx in range(n):\n",
        "    rmse_ = rmse[idx,:]\n",
        "    ax1.plot([i for i in range(1,n+1)],rmse_, 'o-')\n",
        "ax1.set_title(\"reconstruction errors (RMSE) for all images\")\n",
        "ax1.set_xlabel(\"reconstruction dimension(s) \\'p\\' \")\n",
        "ax1.set_ylabel(\"RMSE\")\n",
        "\n",
        "rmse_mean = np.mean(rmse, axis = 0)\n",
        "ax2 = fig.add_subplot(1,2,2)\n",
        "ax2.plot([i for i in range(1, n+1)], rmse_mean, \"ro-\")\n",
        "ax2.set_title(\"Mean of RMSE for all images\")\n",
        "ax2.set_xlabel(\"reconstruction dimension(s) \\'p\\' \")\n",
        "ax2.set_ylabel(\"mean of RMSEs\")\n",
        "\n",
        "ax1.set_ylim((0,80))\n",
        "ax2.set_ylim((0,80))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sc0HWxeKGwtc",
        "colab_type": "text"
      },
      "source": [
        "From the test above [XXXXXXXXXX] and the cumulative variance explained that we've seen one could decide to go for a p = 25, covering "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s5TqYKddlAip",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_test = m_test_src.copy()[0:9, :]\n",
        "p = 30\n",
        "# data_test_mean = np.mean(m_test_src.T.copy(), axis = 0)\n",
        "data_train_mean = np.mean(m_src.copy(), axis = 0)\n",
        "data_test_centered = data_test - data_train_mean\n",
        "\n",
        "# my_pca_svd.show_data(X)\n",
        "\n",
        "# reduce the image to the principal components (all first)\n",
        "data_test_projected = my_pca.projectPC(data_test_centered, k=40)\n",
        "logging.debug(\"shape of data_projected = \" + str(data_test_projected.shape))\n",
        "\n",
        "eig1 = data_test_projected[:,0]\n",
        "eig2 = data_test_projected[:,1]\n",
        "\n",
        "rmse_test = np.empty(len(range(n)))\n",
        "\n",
        "for ptemp in range(n):\n",
        "    x_test_hat = my_pca.reconstruct(data_test_projected, ptemp+1, show=False)\n",
        "    rmse_test[ptemp] = my_pca.compute_error(data_test_centered, x_test_hat)\n",
        "\n",
        "logging.debug(\"RMSE = \" + str(rmse_test))\n",
        "fig = plt.figure()\n",
        "ax1 = fig.add_subplot(1,1,1)\n",
        "ax1.plot([i for i in range(1,n+1)], rmse_test, \"ro-\")\n",
        "\n",
        "\n",
        "data_test_reconstructed = my_pca.reconstruct(data_test_projected, p, show=False)\n",
        "\n",
        "\n",
        "\n",
        "plt.figure() \n",
        "fig, ax = plt.subplots(1, 1, figsize=(16, 16), sharex=True, sharey=True)\n",
        "ax.grid()\n",
        "ax.plot(eig1, eig2, 'bo')\n",
        "for x_, y_, img_vector_ in zip(eig1, eig2, data_test):\n",
        "    img = my_reshape(img_vector_, sq_size, color)\n",
        "    ab = AnnotationBbox(OffsetImage(img, cmap=my_color_map), (x_, y_), frameon=False)\n",
        "    ax.add_artist(ab)\n",
        "\n",
        "\n",
        "plt.figure() \n",
        "fig, ax = plt.subplots(1, 1, figsize=(8, 8), sharex=True, sharey=True)\n",
        "eig1 = data_test_projected[:,0]\n",
        "eig2 = data_test_projected[:,1]\n",
        "ax.plot(eig1[0:10], eig2[0:10], 'bo')\n",
        "ax.plot(eig1[10:20], eig2[10:20], 'ro')\n",
        "ax.plot(eig1[20:30], eig2[20:30], 'go')\n",
        "ax.plot(eig1[30:40], eig2[30:40], 'yo')\n",
        "plt.show()\n",
        "\n",
        "plt.figure() \n",
        "fig, ax = plt.subplots(1, 1, figsize=(16, 16), sharex=True, sharey=True)\n",
        "ax.grid()\n",
        "ax.plot(eig1, eig2, 'bo')\n",
        "for x_, y_, img_vector_ in zip(eig1, eig2, data_test_reconstructed):\n",
        "    img = my_reshape(img_vector_ + data_train_mean, sq_size, color)\n",
        "    ab = AnnotationBbox(OffsetImage(img, cmap = my_color_map), (x_, y_), frameon=False)\n",
        "    ax.add_artist(ab)\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Djz9YO4Guhr",
        "colab_type": "text"
      },
      "source": [
        "# Exploit Feature Representations\n",
        "- use of feature representations that we have constructed before\n",
        "- evaluate each feature representation for classification and identification\n",
        "- discuss each method + why it is appropriate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GO2I8rHUHRuC",
        "colab_type": "text"
      },
      "source": [
        "## Classification - howto\n",
        "\n",
        "- pre-process data\n",
        "    1. Resizing to appropriate size => selected parameter\n",
        "    2. color or grayscale => selected parameter\n",
        "    \n",
        "- compute feature reprensentation \n",
        "    1. HOG\n",
        "    2. PCA\n",
        "- train the corresponding classifier using training sets\n",
        "    1. training using HOG feature\n",
        "    2. training using PCA feature\n",
        "- apply the classifier on unseen images (test)\n",
        "    * apply same preprocessing as for training (exact same)\n",
        "    * train\n",
        "    * compute metrics\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yLF5zzo4IFBZ",
        "colab_type": "text"
      },
      "source": [
        "### Preprocess the data\n",
        "\n",
        "- get raw training data: faces cropped data for personA and personB\n",
        "- resize the raw images to a common image size\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nk6XzxXHH1q3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "logging.info(min(get_min_size(training_set), get_min_size(test_set)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gqBussWgOwAd",
        "colab_type": "text"
      },
      "source": [
        "The smallest face crop that we have in training set and test set is (70,70), so we can without issue rescale all our face crops to (64,64) as part of the data pre-processing."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_kb7kzbG3cTB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train = get_matrix_from_set(training_set, color, sq_size = sq_size, flatten = False)\n",
        "X_test = get_matrix_from_set(test_set, color, sq_size = sq_size, flatten = False)\n",
        "y_train = np.zeros((40,))\n",
        "y_train[20:40] = 1\n",
        " \n",
        "\"\"\"\n",
        "For now, set up \"0\" for personC; \"1\" for personD !\n",
        "\"\"\"\n",
        "y_test = np.zeros((40,))\n",
        "y_test[10:20] = 1\n",
        "y_test[30:40] = 1\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hr-i3JSN7nYn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "logging.info(\"Training set (horizontal axis):\")\n",
        "logging.info(\"Test set PersonA [0 -> 19]:\")\n",
        "plot_matrix(X_train[0:20,:], color, my_color_map, h=1, w=20, transpose = False)\n",
        "logging.info(\"Test set PersonA [20 -> 39]:\")\n",
        "plot_matrix(X_train[20:40,:], color, my_color_map, h=1, w=20, transpose = False)\n",
        "\n",
        "logging.info(\"Test set (vertical axis):\")\n",
        "logging.info(\"Test set PersonA [0 -> 9]:\")\n",
        "plot_matrix(X_test[0:10, :], color, my_color_map, h=1, w=10, transpose = False)\n",
        "logging.info(\"Test set PersonB [10 -> 19]:\")\n",
        "plot_matrix(X_test[10:20, :], color, my_color_map, h=1, w=10, transpose = False)\n",
        "logging.info(\"Test set PersonC [20 -> 29]:\")\n",
        "plot_matrix(X_test[20:30, :], color, my_color_map, h=1, w=10, transpose = False)\n",
        "logging.info(\"Test set PersonD [30 -> 39]:\")\n",
        "plot_matrix(X_test[30:40, :], color, my_color_map, h=1, w=10, transpose = False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K2w_3OUNVmOx",
        "colab_type": "text"
      },
      "source": [
        "## HOG Classification\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ztVlTGrVE0M",
        "colab_type": "text"
      },
      "source": [
        "### Construction of Transformers\n",
        "followed method of https://kapernikov.com/tutorial-image-classification-with-scikit-learn/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tddO9f8APPda",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "\n",
        "class HogTransformer(BaseEstimator, TransformerMixin):\n",
        "    \"\"\"\n",
        "    Expects an array of 2D arrays (1 channel images)\n",
        "    Calculates hog features for each image\n",
        "    \"\"\"\n",
        "    def __init__(self, y=None, \n",
        "                 orientations = 9, \n",
        "                 pixels_per_cell = (8,8), \n",
        "                 cells_per_block = (2,2),\n",
        "                 block_norm = \"L2-Hys\", \n",
        "                 transform_sqrt = False,\n",
        "                 multichannel = False):\n",
        "        self.y = y\n",
        "        self.orientations = orientations\n",
        "        self.pixels_per_cell = pixels_per_cell\n",
        "        self.cells_per_block = cells_per_block\n",
        "        self.block_norm = block_norm\n",
        "        self.transform_sqrt = transform_sqrt\n",
        "        self.multichannel = multichannel # default is grayscale\n",
        "    \n",
        "    def fit(self, X, y=None):\n",
        "        logging.debug(\"[HOGTransformer.fit] X.Shape \" + str(X.shape))\n",
        "        return self\n",
        "    \n",
        "    def transform(self, X, y=None):\n",
        "        logging.debug(\"[HOGTransformer.transform] X.Shape \" + str(X.shape))\n",
        "\n",
        "        def local_hog(X):\n",
        "            if self.multichannel:\n",
        "                X_ = X.copy().T\n",
        "                logging.debug(\"TO CHECK IF TRANSPOSE STILL NEEDED ?\")\n",
        "                logging.debug(\"[HOGTransformer.transform] (1) X.Shape \" + str(X.shape))\n",
        "                logging.debug(\"[HOGTransformer.transform] (2) X_.Shape \" + str(X_.shape))\n",
        "            else:\n",
        "                X_ = X.copy()\n",
        "            # logging.debug(\"[HOGTransformer.transform.local_HOG]\" )\n",
        "            # cv2_imshow(X_)\n",
        "            return skimage_feature_hog(X_,\n",
        "                                       orientations = self.orientations, \n",
        "                                       pixels_per_cell = self.pixels_per_cell,\n",
        "                                       cells_per_block = self.cells_per_block,\n",
        "                                       block_norm = self.block_norm,\n",
        "                                       visualize = False, \n",
        "                                       transform_sqrt = self.transform_sqrt, \n",
        "                                       feature_vector = True, \n",
        "                                       multichannel = self.multichannel)\n",
        "\n",
        "        try: \n",
        "            # tmp = [str(image.shape) for image in X]\n",
        "            # logging.debug( str(tmp) )\n",
        "            return np.array([local_hog(image) for image in X])\n",
        "        except ValueError as ve:\n",
        "            logging.error(str(ve))\n",
        "        except NameError as ne:\n",
        "            logging.error(str(ne))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ajQ4PN5asVjc",
        "colab_type": "text"
      },
      "source": [
        "HOG feature\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "mef1tejVsYN6",
        "colab": {}
      },
      "source": [
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.model_selection import cross_val_predict\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# resizify = ResizeTransformer(sq_size=sq_size)\n",
        "# grayify = RGB2GrayTransformer(color=color)\n",
        "\n",
        "# scalify = StandardScaler()\n",
        "hogify = HogTransformer(    orientations = 9, \n",
        "                            pixels_per_cell = (16,16), \n",
        "                            cells_per_block = (2,2),\n",
        "                            block_norm = \"L2-Hys\", \n",
        "                            transform_sqrt = False,\n",
        "                            multichannel = color)\n",
        "\n",
        "\n",
        "\n",
        "# X_train_gray = grayify.fit_transform(X_train)\n",
        "X_train_hog = hogify.fit_transform(X_train)\n",
        "# X_train_prepared = scalify.fit_transform(X_train_hog)\n",
        "X_train_prepared = X_train_hog"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gGSuC0Lr1up7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(X_train_prepared.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BSnOwwFICgPZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sgd_clf = SGDClassifier(random_state=42, max_iter=1000, tol=1e-6, verbose = 1)\n",
        "sgd_clf.fit(X_train_prepared, y_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "183bDFIGELk1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "X_test_ab = X_test[0:20,:]\n",
        "y_test_ab = y_test[0:20]\n",
        "print(\"X_test shape -> \" + str(X_test.shape))\n",
        "print(\"X_test_ab shape -> \" + str(X_test_ab.shape))\n",
        "print(\"ytest [10]      -> \" + str(sum(y_test_ab)))\n",
        "X_test_hog = hogify.transform(X_test_ab)\n",
        "X_test_prepared = X_test_hog\n",
        "\n",
        "y_pred = sgd_clf.predict(X_test_prepared)\n",
        "logging.info(\"Percentage correct: \" + str( 100* np.sum(y_pred == y_test_ab)/len(y_test_ab) ))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mkYFiPjIKHan",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "label_names = [\"Emma Stone\", \"Bradly Cooper\"]\n",
        "\n",
        "cmx = confusion_matrix(y_test_ab, y_pred)\n",
        "cmx\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r1vzcfScKwJZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
        " \n",
        "def plot_confusion_matrix(cmx, vmax1=None, vmax2=None, vmax3=None):\n",
        "    cmx_norm = 100*cmx / cmx.sum(axis=1, keepdims=True)\n",
        "    cmx_zero_diag = cmx_norm.copy()\n",
        " \n",
        "    np.fill_diagonal(cmx_zero_diag, 0)\n",
        " \n",
        "    fig, ax = plt.subplots(ncols=3)\n",
        "    fig.set_size_inches(12, 3)\n",
        "    [a.set_xticks(range(6)) for a in ax]\n",
        "    [a.set_yticks(range(6)) for a in ax]\n",
        " \n",
        "    im1 = ax[0].imshow(cmx, vmax=vmax1)\n",
        "    ax[0].set_title('as is')\n",
        "    im2 = ax[1].imshow(cmx_norm, vmax=vmax2)\n",
        "    ax[1].set_title('%')\n",
        "    im3 = ax[2].imshow(cmx_zero_diag, vmax=vmax3)\n",
        "    ax[2].set_title('% and 0 diagonal')\n",
        " \n",
        "    dividers = [make_axes_locatable(a) for a in ax]\n",
        "    cax1, cax2, cax3 = [divider.append_axes(\"right\", size=\"5%\", pad=0.1)\n",
        "                        for divider in dividers]\n",
        " \n",
        "    fig.colorbar(im1, cax=cax1)\n",
        "    fig.colorbar(im2, cax=cax2)\n",
        "    fig.colorbar(im3, cax=cax3)\n",
        "    fig.tight_layout()\n",
        " \n",
        "plot_confusion_matrix(cmx)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PC4dG4K4MiIQ",
        "colab_type": "text"
      },
      "source": [
        "Now, we have a quite nice score, but several parameters. U\n",
        "Use of pipeline in order to build a *gridsearch* ...\n",
        "XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXx"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DALEufBgVVkh",
        "colab_type": "text"
      },
      "source": [
        "### HOG/Classification - OPTIMIZATION\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5HF3gHSlMmCb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn import svm\n",
        "\n",
        "\"\"\"\n",
        "Definition of a pipeline\n",
        "\"\"\"\n",
        "HOG_pipeline = Pipeline([\n",
        "                         ('hogify', HogTransformer(\n",
        "                             orientations = 9,\n",
        "                             pixels_per_cell = (4,4),\n",
        "                             cells_per_block = (2,2),\n",
        "                             block_norm='L1',\n",
        "                             transform_sqrt=False, \n",
        "                             multichannel = color)\n",
        "                         ),\n",
        "                         ('classify', SGDClassifier(\n",
        "                             random_state = 42, \n",
        "                             max_iter = 1000, \n",
        "                             tol=1e-3)\n",
        "                         )\n",
        "\n",
        "])\n",
        "\n",
        "clf = HOG_pipeline.fit(X_train, y_train)\n",
        "# logging.info(\"Percentage correct: \" + str( 100* np.sum(y_pred == y_test_ab)/len(y_test_ab) ) + \" %\")\n",
        "y_pred = clf.predict(X_test) \n",
        "logging.info(\"Percentage correct: \" + str( 100*np.sum(y_pred == y_test)/len(y_test)) + \" %\")\n",
        "print(y_pred)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fn57FFi6OcQZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        " \n",
        "param_grid = [\n",
        "    {\n",
        "        'hogify__orientations': [9],\n",
        "        'hogify__cells_per_block': [(2, 2),(3, 3)],\n",
        "        'hogify__pixels_per_cell': [(4,4),(8, 8),(12,12), (16, 16)],\n",
        "        'hogify__block_norm': ['L1', 'L2', 'L2-Hys'],\n",
        "        'hogify__transform_sqrt': [False, True],\n",
        "        'hogify__multichannel':[color],\n",
        "        'classify': [\n",
        "            SGDClassifier(random_state=42, max_iter=1000, tol=1e-3),\n",
        "            svm.SVC(kernel='linear'),\n",
        "            svm.SVC(kernel='poly')]}\n",
        "]\n",
        "\n",
        "grid_search = GridSearchCV(HOG_pipeline,\n",
        "                           param_grid, \n",
        "                           cv = 4, \n",
        "                           n_jobs = -1,\n",
        "                           scoring = \"accuracy\",\n",
        "                           verbose = 1, \n",
        "                           return_train_score = True)\n",
        "\n",
        "grid_res = grid_search.fit(X_train, y_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GJjSs_v1RNz4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "grid_res.best_estimator_"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xp2EcopKR_P9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "grid_res.best_score_"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xCo3_Wr5SDs-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "grid_res.best_params_"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n6tBpJriSkUW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "best_prediction = grid_res.predict(X_test)\n",
        "print('Percentage correct: ', 100*np.sum(best_prediction == y_test)/len(y_test))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vhX8e4yJjTyD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "idx = np.where( np.array(y_test - best_prediction) != 0)\n",
        "print(idx)\n",
        "print(y_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V37pPCfnubEQ",
        "colab_type": "text"
      },
      "source": [
        "## PCA Classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LUrEq8kIua7b",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zVjfgPEbFlph",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train = get_matrix_from_set(training_set, color, sq_size, flatten=True)\n",
        "X_test= get_matrix_from_set(test_set, color, sq_size, flatten = True)\n",
        "\n",
        "logging.debug(\"X_train_PCA Shape = \" + str(X_train.shape))\n",
        "logging.debug(\"X_test_PCA Shape = \" + str(X_test.shape))\n",
        "\n",
        "y_train = y_train.copy()\n",
        "y_test = y_test.copy()\n",
        "\n",
        "logging.debug(\"y_train_PCA Shape = \" + str(y_train.shape))\n",
        "logging.debug(\"y_test_PCA Shape = \" + str(y_test.shape))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JQ1nx5PMItFo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pcaify = sklearn_decomposition_PCA(n_components = 25)\n",
        "X_train_PCA = pcaify.fit_transform(X_train)\n",
        "X_train_prepared = X_train_PCA\n",
        "\n",
        "print(X_train_prepared.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XM_prrKMLQhZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sgd_clf = SGDClassifier(random_state=42, max_iter = 1000, tol=1e-4, verbose = 1)\n",
        "sgd_clf.fit(X_train_prepared, y_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cs_pVXy4Linr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_test_PCA = pcaify.transform(X_test)\n",
        "X_test_prepared = X_test_PCA\n",
        "y_pred = sgd_clf.predict(X_test_prepared)\n",
        "\n",
        "logging.info(\"Percentage correct: \" + str(100 * np.sum(y_pred == y_test)/len(y_test)))\n",
        "cmx = confusion_matrix(y_test, y_pred)\n",
        "plot_confusion_matrix(cmx)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aaBbluh6NDUA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "Definition of a PCA pipeline\n",
        "\"\"\"\n",
        "PCA_pipeline = Pipeline([\n",
        "                         ('pcaify', sklearn_decomposition_PCA(\n",
        "                             n_components = 25)\n",
        "                         ),\n",
        "                         ('classify', SGDClassifier(\n",
        "                             random_state = 42, \n",
        "                             max_iter = 1000, \n",
        "                             tol=1e-3)\n",
        "                         )\n",
        "\n",
        "])\n",
        "\n",
        "clf = PCA_pipeline.fit(X_train, y_train)\n",
        "# logging.info(\"Percentage correct: \" + str( 100* np.sum(y_pred == y_test_ab)/len(y_test_ab) ) + \" %\")\n",
        "y_pred = clf.predict(X_test) \n",
        "logging.info(\"Percentage correct: \" + str( 100*np.sum(y_pred == y_test)/len(y_test)) + \" %\")\n",
        "print(y_pred)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nAuE47ZqOGLV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "param_grid = [\n",
        "    {\n",
        "        'pcaify__n_components': range(1,41,2),\n",
        "        'classify': [\n",
        "            SGDClassifier(random_state=42, max_iter=1000, tol=1e-3),\n",
        "            svm.SVC(kernel='linear'),\n",
        "            svm.SVC(kernel='poly')]}\n",
        "]\n",
        "\n",
        "grid_search = GridSearchCV(PCA_pipeline,\n",
        "                           param_grid, \n",
        "                           cv = 3, \n",
        "                           n_jobs = -1,\n",
        "                           scoring = \"accuracy\",\n",
        "                           verbose = 1, \n",
        "                           return_train_score = True)\n",
        "\n",
        "grid_res = grid_search.fit(X_train, y_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Is4LbiYGO1nQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "logging.info(\"Best Score -> \" + str(grid_res.best_score_))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LZbk1lHSPDDV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "logging.info(\"Best Parameters found:\")\n",
        "logging.info(grid_res.best_params_)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fVoct4GYPlrz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "best_prediction = grid_res.predict(X_test)\n",
        "logging.info(\"Percentage correct: \" + str(100*np.sum(best_prediction == y_test)/len(y_test)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-x_REboTQE6L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "idx = np.where( np.array(y_test-best_prediction) != 0)\n",
        "logging.debug(idx)\n",
        "logging.debug(best_prediction)\n",
        "logging.debug(y_test)\n",
        "# cv2_imshow(np.reshape(X_test[22,:], (64,64,3)))\n",
        "# cv2_imshow(np.reshape(X_test[23,:], (64,64,3)))\n",
        "# cv2_imshow(np.reshape(X_test[25,:], (64,64,3)))\n",
        "# cv2_imshow(np.reshape(X_test[26,:], (64,64,3)))\n",
        "# cv2_imshow(np.reshape(X_test[27,:], (64,64,3)))\n",
        "# cv2_imshow(np.reshape(X_test[35,:], (64,64,3)))\n",
        "# cv2_imshow(np.reshape(X_test[39,:], (64,64,3)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z69CgcHOUqcs",
        "colab_type": "text"
      },
      "source": [
        "## Identification\n",
        "In an identification setup the goal is to **compute similarity scores** between pairs of data examples and use them to identify new images. \n",
        "In this exercise you will:\n",
        "- **compute the feature representation of every image** and \n",
        "- **create a data space using your training data**, \n",
        "- **give each of the data points a label based on the person they contain**.\n",
        "\n",
        "Next try to **identify the test images using k-NN**. \n",
        "\n",
        "* What value did you use for k? \n",
        "* Was the identification of person A and B successful for all descriptors? \n",
        "* How did images of person C and D get labeled?\n",
        "\n",
        "\n",
        "===\n",
        "My understanding:\n",
        "> * stepA: compute for, all the training set images, the handcrafted feature representation selected (for instance, HOG)\n",
        "> * stepB: take one particular test image, compute its HOG\n",
        "> * stepC: compute similarity score between HOG_test_image and all other training set images HOG representations computed in stepA\n",
        "> * stepD: label according to k-NN\n",
        "> * stepE: repeat for all other test images (including for personC and personD)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Gnqa7bRgqqP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.metrics.pairwise import cosine_distances\n",
        "from sklearn.metrics.pairwise import euclidean_distances\n",
        "from sklearn.metrics.pairwise import manhattan_distances"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "azrVkyB8PKr-",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "```\n",
        ">>> from numpy.linalg import norm\n",
        ">>> norm(X, axis=1, ord=1)  # L-1 norm\n",
        ">>> norm(X, axis=1, ord=2)  # L-2 norm\n",
        ">>> norm(X, axis=1, ord=np.inf)  # L-∞ norm\n",
        "```\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wMYJIBugmkO9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plot_similarity_matrix(similarity_matrix, show_numbers = False, vmax1 = None, vmax2 = None, norm_only=False, width=16, height = 8, fontsize = 10):\n",
        "    similarity_matrix_norm = 100*similarity_matrix / np.linalg.norm(similarity_matrix, axis = 1, ord=1, keepdims=True)\n",
        "\n",
        "    if norm_only:\n",
        "        raise NotImplementedError    \n",
        "    else:\n",
        "        fig, ax = plt.subplots(ncols=2)\n",
        "        fig.set_size_inches(width, height)\n",
        "    \n",
        "        im1 = ax[0].imshow(similarity_matrix, vmax=vmax1, cmap=\"jet\")\n",
        "        ax[0].set_title('as is')\n",
        "        im2 = ax[1].imshow(similarity_matrix_norm, vmax=vmax2, cmap=\"jet\")\n",
        "        ax[1].set_title('%')\n",
        "        dividers = [make_axes_locatable(a) for a in ax]\n",
        "        cax1, cax2 = [divider.append_axes(\"right\", size=\"5%\", pad=0.1) for divider in dividers]\n",
        "    \n",
        "        fig.colorbar(im1, cax=cax1)\n",
        "        fig.colorbar(im2, cax=cax2)\n",
        "\n",
        "        if show_numbers:\n",
        "            for i in range(similarity_matrix.shape[0]):\n",
        "                for j in range(similarity_matrix.shape[1]):\n",
        "                    ax[0].text(j, i, str(similarity_matrix[i,j]), fontsize=fontsize,va='center', ha='center')\n",
        "                    ax[1].text(j, i, str(round(similarity_matrix_norm[i,j],0)).rstrip('.0'),fontsize=fontsize,va='center', ha='center')\n",
        "    fig.tight_layout()\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JAZ67dFriP-_",
        "colab_type": "text"
      },
      "source": [
        "#### Illustration of distance measures\n",
        "\n",
        "\n",
        "In order to better understand the vizualisation used, a tool example is given hereunder. The input is a matrix handmade. Later, it will be the matrix of shape (#test_image, #train_image) containing the similarity (or reversely, distance) measures pairwise.\n",
        "\n",
        "Regarding the input matrix:\n",
        "- on each line, the ratio between number is kept the same\n",
        "- the five rows contain three different scale of the numbers:    \n",
        "\n",
        "```\n",
        "[[ 100,  20,  30,  40,  50], \n",
        " [   2,  10,   3,   4,   5], \n",
        " [  20,  30, 100,  40,  50],\n",
        " [   2,   3,   4,  10,   5], \n",
        " [ 200, 300, 400, 500,1000]]\n",
        "```\n",
        "\n",
        "\n",
        "The numbers written indicate the value of the cell.\n",
        "\n",
        "**Left side**: The \"As is\" matrix colors the cell as the numbers are set. The colorscale is therefore really large, and it can be useful to observe the disparity of measures across all tests. \n",
        "\n",
        "If the represented matrix is a distance matrix, for instance, one can observe that the 5th test image is *very far* from all the training images, as the 4th row as larger numbers than any other row. \n",
        "The drawback is that if some numbers are much higher than others, we lose in granularity to represent the differences between those numbers. For instance, a distance of \"2\" and a distance of \"20\" are very much alike.\n",
        "\n",
        "**Right side**: The figure shows the same input matrix but normalized by row. That means that the sum of all numbers in a row is equal to 100 %. \n",
        "It is helpful to analyze *locally* the distance/similarity values for 1 specific test image with respect to all training images. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HHDxUAmWI1_H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_dist_mtx = np.array([[100,20,30,40,50], [2,10,3,4,5], [20,30,100,40,50],[2,3,4,10,5], [200,300,400,500,1000]])\n",
        "plot_similarity_matrix(test_dist_mtx, show_numbers=True, width=8, height=4, fontsize=14)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r7YRyCUtu1fh",
        "colab_type": "text"
      },
      "source": [
        "In the next sections of this tutorial, these vizualisations will be used to detail the similarity/distance measures between our different images. \n",
        "The vertical axis correspond to test images, with the following mapping\n",
        "- 0 -> 9: images of PersonA, Emma Stone\n",
        "- 10 -> 19: images of PersonB, Bradley Cooper\n",
        "- 20 -> 29: images of PersonC, Jane Levy\n",
        "- 30 -> 39: images of PersonD, Marc Blucas\n",
        "\n",
        "The horizontal axis corresponds to the training images, with the following mapping:\n",
        "- 0 -> 19: images of PersonA (training set)\n",
        "- 20 -> 39: images of PersonB (training set)\n",
        "\n",
        "If the feature descritions are appropriate, the features distance measurements should lead to\n",
        "- very small distance (= large similarity measure) between training and test images of PersonA; and similarly for personB)\n",
        "- very high distance (= small similarity measure) between training and test images of PersonA; and similarly for personB)\n",
        "\n",
        "==> Persons from the same class would share feature descriptions, and not share other class feature description\n",
        "\n",
        "Intuitively:\n",
        "- feature description of personC should be closer (less distant, more similar) to personA than personB;\n",
        "- feature descriptions of personD should be closer to personB than personA."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CDsTsCnRWqqN",
        "colab_type": "text"
      },
      "source": [
        "### based on HOG feature decriptors\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u9icKavSYH8d",
        "colab_type": "text"
      },
      "source": [
        "#### Pre-process data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RmUs-jTEYBrt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train = get_matrix_from_set(training_set, color, sq_size = sq_size, flatten = False)\n",
        "X_test = get_matrix_from_set(test_set, color, sq_size = sq_size, flatten = False)\n",
        "\n",
        "y_train = np.zeros((40,))\n",
        "y_train[20:40] = 1\n",
        " \n",
        "\"\"\"\n",
        "For now, set up \"0\" for personC; \"1\" for personD !\n",
        "\"\"\"\n",
        "y_test = np.zeros((40,))\n",
        "y_test[10:20] = 1\n",
        "y_test[30:40] = 1\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RA82xnpT6Ui6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "logging.info(\"Training set (horizontal axis):\")\n",
        "logging.info(\"Test set PersonA [0 -> 19]:\")\n",
        "plot_matrix(X_train[0:20,:], color, my_color_map, h=1, w=20, transpose = False)\n",
        "logging.info(\"Test set PersonA [20 -> 39]:\")\n",
        "plot_matrix(X_train[20:40,:], color, my_color_map, h=1, w=20, transpose = False)\n",
        "\n",
        "logging.info(\"Test set (vertical axis):\")\n",
        "logging.info(\"Test set PersonA [0 -> 9]:\")\n",
        "plot_matrix(X_test[0:10, :], color, my_color_map, h=1, w=10, transpose = False)\n",
        "logging.info(\"Test set PersonB [10 -> 19]:\")\n",
        "plot_matrix(X_test[10:20, :], color, my_color_map, h=1, w=10, transpose = False)\n",
        "logging.info(\"Test set PersonC [20 -> 29]:\")\n",
        "plot_matrix(X_test[20:30, :], color, my_color_map, h=1, w=10, transpose = False)\n",
        "logging.info(\"Test set PersonD [30 -> 39]:\")\n",
        "plot_matrix(X_test[30:40, :], color, my_color_map, h=1, w=10, transpose = False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lwnOQwJYGKWZ",
        "colab_type": "text"
      },
      "source": [
        "Creation of Transformer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XNh9vtMhWyiq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "hogify = None\n",
        "hogify = HogTransformer(    orientations = 9, \n",
        "                            pixels_per_cell = (16,16), \n",
        "                            cells_per_block = (2,2),\n",
        "                            block_norm = \"L2-Hys\", \n",
        "                            transform_sqrt = False,\n",
        "                            multichannel = color)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jzGz5DZeGSMY",
        "colab_type": "text"
      },
      "source": [
        "Transformation of Inputs\n",
        " - **X_train**: application of the fit then transform method from the transformer\n",
        " - **X_test**: application of the transform method from the transformer\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c-l2ipZWGRVM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train_hog = hogify.fit_transform(X_train)\n",
        "X_test_hog = hogify.transform(X_test)\n",
        "\n",
        "logging.debug(\"X_train_hog Shape: \" + str(X_train_hog.shape))\n",
        "logging.debug(\"X_test_hog Shape: \" + str(X_test_hog.shape))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QfiKUASmjrIg",
        "colab_type": "text"
      },
      "source": [
        "#### compute similarities pairwise\n",
        "using cosine similarities\n",
        "=> explain why"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2NdCbPXpkBIe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# hog_similarities = cosine_similarity(X_test_hog, X_train_hog)\n",
        "hog_distances_cos = cosine_distances(X_test_hog, X_train_hog)\n",
        "hog_distances_eucl = euclidean_distances(X_test_hog, X_train_hog)\n",
        "# plot_similarity_matrix(hog_similarities)\n",
        "logging.debug(\"DISTANCE BASED ON COSINE\")\n",
        "plot_similarity_matrix((hog_distances_cos))\n",
        "\n",
        "logging.debug(\"DISTANCE BASED ON COSINE - LOG10\")\n",
        "plot_similarity_matrix(np.log10(hog_distances_cos))\n",
        "\n",
        "logging.debug(\"DISTANCE BASED ON EUCL\")\n",
        "plot_similarity_matrix((hog_distances_eucl))\n",
        "\n",
        "logging.debug(\"DISTANCE BASED ON EUCL - LOG10\")\n",
        "plot_similarity_matrix(np.log10(hog_distances_eucl))\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MV_iOmO5re1s",
        "colab_type": "text"
      },
      "source": [
        "kNN\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rtp95kE0reS2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn import metrics\n",
        "\n",
        "k=1\n",
        "knn = KNeighborsClassifier(n_neighbors = k, metric='euclidean')\n",
        "knn.fit(X_train_hog, y_train)\n",
        "y_pred_hog = knn.predict(X_test_hog)\n",
        "logging.info(\"Percentage correct: \" + str(100*np.sum( y_test == y_pred_hog)/len(y_test)))\n",
        "logging.info(\"Percentage correct [SKlearn]: \" + str(metrics.accuracy_score(y_test, y_pred_hog)))\n",
        "logging.debug(\"y_pred => \" + str(y_pred_hog))\n",
        "print(knn.kneighbors(X_test_hog[0,:].reshape(1, -1), n_neighbors=40))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eCFWdMr3ykUD",
        "colab_type": "text"
      },
      "source": [
        "idées : \n",
        "> selectionner les images de tests et montrer leurs nearest neighbours; \n",
        "    - quand ça va bien\n",
        "    - quand ça va pas bien\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lebK99WQWy9H",
        "colab_type": "text"
      },
      "source": [
        "### based on PCA feature descriptors\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AwsBqLUqHXLA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train = get_matrix_from_set(training_set, color, sq_size = sq_size, flatten = True)\n",
        "X_test = get_matrix_from_set(test_set, color, sq_size = sq_size, flatten = True)\n",
        "y_train = np.zeros((40,))\n",
        "y_train[20:40] = 1\n",
        " \n",
        "\"\"\"\n",
        "For now, set up \"0\" for personC; \"1\" for personD !\n",
        "\"\"\"\n",
        "y_test = np.zeros((40,))\n",
        "y_test[10:20] = 1\n",
        "y_test[30:40] = 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zonhIEds0J70",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "logging.info(\"Training set (horizontal axis):\")\n",
        "logging.info(\"Test set PersonA [0 -> 19]:\")\n",
        "plot_matrix(X_train[0:20,:], color, my_color_map, h=1, w=20, transpose = False)\n",
        "logging.info(\"Test set PersonA [20 -> 39]:\")\n",
        "plot_matrix(X_train[20:40,:], color, my_color_map, h=1, w=20, transpose = False)\n",
        "\n",
        "logging.info(\"Test set (vertical axis):\")\n",
        "logging.info(\"Test set PersonA [0 -> 9]:\")\n",
        "plot_matrix(X_test[0:10, :], color, my_color_map, h=1, w=10, transpose = False)\n",
        "logging.info(\"Test set PersonB [10 -> 19]:\")\n",
        "plot_matrix(X_test[10:20, :], color, my_color_map, h=1, w=10, transpose = False)\n",
        "logging.info(\"Test set PersonC [20 -> 29]:\")\n",
        "plot_matrix(X_test[20:30, :], color, my_color_map, h=1, w=10, transpose = False)\n",
        "logging.info(\"Test set PersonD [30 -> 39]:\")\n",
        "plot_matrix(X_test[30:40, :], color, my_color_map, h=1, w=10, transpose = False)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HXqKpzqeFauR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pcaify = None\n",
        "pcaify = sklearn_decomposition_PCA(n_components = 35)\n",
        "\n",
        "X_train_PCA = pcaify.fit_transform(X_train)\n",
        "X_test_PCA = pcaify.transform(X_test)\n",
        "\n",
        "\n",
        "# pca_similarities = cosine_similarity(X_test_PCA, X_train_PCA)\n",
        "pca_distances_cos = cosine_distances(X_test_PCA, X_train_PCA)\n",
        "pca_distances_eucl = euclidean_distances(X_test_PCA, X_train_PCA)\n",
        "# plot_similarity_matrix(pca_similarities)\n",
        "# plot_similarity_matrix(np.log(pca_distances_cos))\n",
        "plot_similarity_matrix((pca_distances_eucl))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p_JWPGPAHt8j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "k=1\n",
        "knn = KNeighborsClassifier(n_neighbors = k, metric='euclidean')\n",
        "knn.fit(X_train_PCA, y_train)\n",
        "y_pred_PCA = knn.predict(X_test_PCA)\n",
        "logging.info(\"Percentage correct: \" + str(100*np.sum( y_test== y_pred_PCA )/len(y_test)))\n",
        "logging.info(\"Percentage correct [SKlearn]: \" + str(metrics.accuracy_score(y_test, y_pred_PCA )))\n",
        "logging.debug(\"Y Pred => \" + str(y_pred_PCA))\n",
        "print(knn.kneighbors(X_test_PCA [0,:].reshape(1, -1), n_neighbors=40))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9qCMI3Tyy0DM",
        "colab_type": "text"
      },
      "source": [
        "## Observations and Comments\n",
        "- PCA distance measures seem more fuzzy than HOG feature descriptors\n"
      ]
    }
  ]
}